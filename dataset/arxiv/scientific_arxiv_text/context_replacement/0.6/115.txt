information - based research on design and performance for state - space memory results in @ xcite , @ xcite , @ xcite and @ xcite . in @ xcite , the researchers proposed a model of write - once memory ( wom ) . in it , a memory cell can be in states of 0 or 1 . the state of a cell can change from 0 to 1 , but change from 1 later to 0 later . these write - once cells are called _ _ _ . it is believed that , the performance of the data in a wom can be improved if it performs correct operations and uses the storage / coding system properly . memory - memory is a memory technology where the charge level of a cell can be significantly increased , and is easy to implement . the memory memory technology allows different charge levels to be stored in a cell . cells are organized into blocks that contain many @ xmath2 cells . the best way to increase the charge level of a cell is to erase the entire block ( i . e . , reduce the charge of all cells to zero ) and reprogram the cells . this takes time , requires memory , and extends the lifetime of the cell . therefore , it is possible to create amodulation schemes that maximize the number of cycles in memory : @ xcite , @ xcite , @ xcite , @ xcite . the modulation schemes maximize the total cell count based on the total memory size and data to be stored . in this work , we introduced the modulation scheme _ _ _ _ _ . two other objective functions for modulation schemes are also introduced in this work : ( i ) increasing the number of cycles for the worst case @ xcite and ( ii ) decreasing for the average case @ xcite . as finucane et al . @ xcite show , the objective for the worst case is the ripple effect caused by the increasing number of cycles during the operation of a given memory device . our work shows that the worst - case objective and the average case objective are two special cases of our optimal ##ity . we can determine under what conditions the optimality objective makes sense . in previous work ( e . g . , @ xcite ) , the modulation schemes were shown to be approximately optimal when the number of bit - words @ xmath0 goes to 1 . under the condition that @ xmath1 can not be used for such purposes . thus , we can assume asym##ptotically optimal modulation code when @ xmath0 is only moderately large . the result from load - balancing algorithm @ xcite . this is an optimal algorithm that improves the performance of the system model . experimental results and simulation results show that this algorithm performs better than the theoretical modulation algorithms when @ xmath0 is moderately large . the rest of the article is as follows . the system models and performance - measures are presented in section [ sub : optimality - measures ] . an absolutely optimal modulation code , which is optimal over all i . i . d . probability distributions , is presented in section [ sec : error - correction - measure ] . the storage efficiency of this absolutely optimal modulation code is analyzed in section [ sec : an - enhanced - algorithm ] . an enhanced modulation code is also proposed in section [ sec : an - enhanced - algorithm ] . the storage efficiency of the modulation code is also analyzed in section [ sec : an - enhanced - algorithm ] . simulation results and analysis are presented in section [ sec : analysis - results ] . the result is summarized in section [ sec : conclusion ] . digital memory devices often rely on error correction / detection algorithms to achieve a low error rate .so far , computer systems tend to use bose - chaudhuri - hocquenghem ( bch ) and reed - solomon ( mls ) codes . the error - correction codes ( ' s ) are known as the outer codes and the modulation codes are the inner codes . in this section , we focus on the modulation codes and discuss the implementation and the use of them for computers . let us assume that a block contains @ xmath3 @ xmath0 - x cells and that @ xmath4 variables ( called a @ xmath4 - cell ) are added together to generate @ xmath5 @ xmath6 - ary cells ( called b @ xmath5 - cell ) . a block contains @ xmath7 @ xmath4 - cells and the @ xmath7 @ xmath5 - cells are assumed to be i . i . d . - cells . we assume that all the @ xmath5 - variables are added almost exactly at the same time and the new variables are stored in the new @ xmath4 - cell . this is a good approximation for a system with an elliptic code . we use the symbol @ xmath8 to representthe difference between and the code is @ xmath8 + 1 . when we use a modulation code , we write on a sample @ xmath4 - 1 . ( the encoder of the modulation code chooses one of the cell - levels based on the current cell - level and the random value of the @ xmath5 - variable . ) note that cell - levels can only be changed during the code . therefore , when the cell - level can be increased to the value of @ xmath9 , the modulation code is lost and all the cell levels are set to zero . we let the maximum possible number of cell - levels be @ xmath10 and note that after @ xmath10 - 1 , the code becomes useless . suppose the @ xmath5 - variable written at time @ xmath8 is the random value @ xmath11 taken from the sample @ xmath12 with time @ xmath13 . for example , we can write the @ xmath5 - written at time @ xmath8 in the same way as @ xmath14 where @ xmath15 is the number of cells modulo @ xmath6 . the cell - levelvector at time @ xmath8 is written to @ xmath16 and @ xmath17 is the charge level of the @ xmath18 - variable vector at time @ xmath19 when we say @ xmath20 we say @ xmath21 for @ xmath22 since the charge level of each vector can not be measured , the size of the data means that an analysis of the entire memory will be required at some point . although writes , reads and writes can all introduce noise into the data , we ignore this and assume that the writes , reads and writes are noise - free . when sending data to a computer , when encoder knows the current cell state @ xmath23 the vector @ xmath5 - vector @ xmath24 , and the encoding function @ xmath25 that maps @ xmath24 and @ xmath26 to the current cell - state vector @ xmath27 . the receiver also knows the current cell state @ xmath27 and the coding function @ xmath28 that maps the cell state @ xmath27 and to the current vector @ xmath29 . of course , the receiver and coding functions couldand over time to improve performance , if we can use time - varying encoding / encoding schemes for storage . the idea of using multiple modulation codes designed to store the data in different locations was introduced by jiang @ xcite . in his work on modulation code design for semiconductor devices ( e . g . @ xcite , @ xcite , @ xcite , @ xcite ) , the lifetime of the device ( either worst - case or optimal ) is fixed by the amount of information per bit . increasing the capacity and decreasing the lifetime of the device are two different approaches . we can either choose one and then the other or go through these two approaches . the previous work ( e . g . , @ xcite ) takes the first approach of increasing the amount of memory for the device and increasing the number of bits after several cycles . in this work , we take the second approach and our goal is to maximize the total amount of information stored in the memory until the device fails . this is equivalent to setting the maximum ( for the @ xmath5 - level , @ xmath13 ) amount of information stored per bit - level , @ xmath30 where @ xmath31 is the number ofinformation stored in the @ xmath18 - _ cell , @ xmath32 is the number of copies after the start , and the average value of the @ xmath5 - cells is . we can use @ xmath33 _ _ _ _ _ . in the article on binary algorithms for flash memory , the number of copies of an @ xmath4 - cell has been calculated in two different cases . the authors in @ xcite consider the worst possible case of , and the authors in @ xcite consider the average number of copies . as stated in @ xcite , the reason for considering the worst case is proportional to the average number of copies over the lifetime of the flash memory device . therefore , these two cases can be considered as two different cases of the l - function ( [ 1 : 2 ] ) . let the @ xmath5 - cells be a set of i . i . d . - variables over time and then the @ xmath4 - cells . the objective of optimization is to minimize the amount of information stored until the device terminates . the total amount of information stored in the x - cell corresponds to the average value , should it terminate .such as ? should this count as a constant ? this example shows that this counts as a constant , and that @ xmath34 bits ( other than @ xmath35 ) can be used during the algorithm . ] can be upper - bound in @ xmath36 where @ xmath37 is the number of bits between the @ xmath38 - th and the @ xmath18 - th variables . note that the upper bound in ( [ x : _ _ error _ ub ] ) is determined by the input distribution , i . e . , when the input @ xmath5 - th is uniformly distributed over @ xmath39 , each iteration returns @ xmath40 bits of data . due to the i . i . d . property of the input distribution over time , @ xmath37 s are i . i . d . over variables over time . since @ xmath37 s are i . i . d . over variables , we can use the parameter @ xmath18 . since @ xmath10 , which is the maximum number of bits possible , is always on the order of @ xmath41 , then the algorithmof natural number ( lln ) , we have @ xmath42k \ log _ { 2 } ( l ) . \ ] ] let the set of all possible encoder / receiver combinations be @ xmath43 where @ xmath44 and the charge levels are step - wise non - negative . this allows us to write the problem @ xmath45 as the following : problem @ xmath46k \ log _ { 2 } ( l ) . \ text { i : opt2 - 1 } \ ] ] denote the maximal charge level of the @ xmath18 - th @ xmath4 - cell at time @ xmath8 as @ xmath47 . note that charge at @ xmath8 is set to zero when a state change occurs and increases by one with each successive step . denote the maximal charge level of the cell at time @ xmath8 as @ xmath48 which can be written as @ xmath49 and @ xmath50 denote the time when the @ xmath18 - th @ xmath4 - cell reaches its maximum charge level , i . e . , @ xmath51 . we have , therefore ,, that a block - erase is required when each value in the block reaches its maximum possible value . the time when a block erase is required is denoted as @ xmath52 it is easy to see that @ xmath53 = t \ left [ , \ [ ] , $ ] where the values are for the @ xmath5 - cell s . solving [ @ xmath54 $ ] is equivalent to solving @ xmath55 . thus the above problem ( [ problem : opt2 - 1 ] ) can be written as the [ [ , @ xmath56 . \ left { problem : opt3 } \ ] ] under the assumption that the problem is i . i . d . for all the @ xmath4 - cells and time t , one has that the @ xmath50 variables are i . i . d . random variables . letting their joint probability density function ( pdf ) be @ xmath57 it is easy to see that @ xmath58 is the mean of @ xmath7 i . i . d . random variables with pdf @ xmath57 where , we have @ xmath59 where @ xmath60 is the cumulative distribution function (cdf ) of @ xmath61 therefore , the optimization problem ( [ f : opt3 ] ) in @ xmath62 = \ label _ { f , t \ in \ mathcal { t } } \ int nf _ { t } ( x ) \ left ( x - f _ { t } ( x ) \ right ) ^ { n - 1 } + \ mbox { t } \ . \ label { f : t } \ ] ] note that when @ xmath63 the optimization problem in ( [ f : t ] ) reduces to @ xmath64 . \ dot { f : opt2 } \ ] ] this is precisely the problem that the algorithms in @ xcite solve . when the input block is represented by an @ xmath4 - cell and the number of copies used is n , then the total ( of all input blocks ) number of copies of an @ xmath4 - cell is equal to half the total amount of errors in @ xmath65 the example above shows that the fact we have compression errors is not true due to the compression error caused by the large number of errors . the most common .note that there is only one @ xmath4 - cell per iteration . the worst case is when @ xmath66 in this case , the pdf @ xmath67 has to have constant value at the minimum of @ xmath8 and the pdf @ xmath68 at the maximum of @ xmath8 . this is the worst case starting point for the optimization problem of the @ xmath4 - cell . this problem is given by @ xcite . our analysis shows that we should consider the worst case when @ xmath69 even though the function has a large number of variables . since the optimality condition is not given only by @ xmath10 , but also by @ xmath70 when @ xmath7 and @ xmath10 are small , it makes more sense to consider the worst case when . when @ xmath71 , it is better to consider the average case . when @ xmath7 is very large , we should consider the number of cases in ( [ x : 1 ] ) which are the worst case and the average case . when @ xmath7 is very large , we should only focus on improving the averagein ( [ q : function ] ) , but it is not clear how to do this algorithm . therefore , this is an open question for further research . however , we can apply a load - balancing algorithm to distributed storage , where @ xmath0 is very large . if we assume that there is only one variable changing over time , the total amount of changes per cell - level will be bounded by @ xmath72 because there are @ xmath73 possible new values . since the number of changes will be bounded by @ xmath74 we have @ xmath75 if we make no assumptions about the @ xmath5 - function , there are only @ xmath76 possible new values . it can be shown that @ xmath77 for both @ xmath6 and @ xmath0 , the result in ( [ q : variable _ size _ function ] ) implies that the constant @ xmath5 will improve the storage efficiency . this is also the reason why averaging over the variables will improve the storage efficiency @ xcite . since most coding algorithms only allow a single cell - level to change by one during each cycle , decodability suggests that @ xmat##h78 for the first case and @ xmath79 for the second case . however , the values in ( [ et : storage _ efficiency _ bound2 ] ) and ( [ et : storage _ efficiency _ bound ] ) both grow with @ xmath4 to improve storage efficiency . the lower bound in ( [ et : storage _ efficiency _ bound ] ) grows rapidly with @ xmath5 and the upper bound in ( [ et : storage _ efficiency _ bound2 ] ) grows rapidly with @ xmath5 . therefore , for the purposes of this example , we have an exponential relationship between the @ xmath5 - cell per block and @ xmath71 , i . e . , the entire block is stored as an @ xmath4 - cell , to improve the storage efficiency . this approach also improves storage efficiency for total storage capacity because all cells are able to store the same amount of data , but the cells can still be changed many more times . note that the implementation of @ xmath71 may be inadequate for large data , but the implementation provides an upper bound for the storage efficiency . from the example , with @ xmath71, we also note that for @ xmath33 is used to estimate the optimal number of cells . in @ xcite , modulation codes are used that are also optimal ( when @ xmath0 goes to infinity ) in the same way when @ xmath80 . in this way , we propose a modulation code that is also optimal for arbitrary input distributions and for @ xmath5 and @ xmath6 . this modulation code can be considered as an extension of the code for @ xcite . the goal is , to increase the cell - levels , on time for an arbitrary input distribution . of course , decodability must be ensured . the goal is to use certain parameters , known to both the encoder ( to achieve the desired values ) and the receiver ( to achieve the decodability ) , to increase the cell levels at time for an arbitrary input distribution . let us assume the @ xmath5 - variable is an i . i . d . - variable at time with the value @ xmath13 and the @ xmath5 - variable at time @ xmath8 is given by @ xmath81 the result of the .##der is assigned by @ xmath82 we read @ xmath83 and calculate the cell state vector at time @ xmath8 and @ xmath84 , where @ xmath17 is the charge level of the @ xmath18 - th cell at time @ xmath19 and @ xmath85 , the cells are assigned to @ xmath86 , @ xmath87 and @ xmath88 . the encoding algorithm @ xmath89 is described as follows . * step 1 : read cell state vector @ xmath27 and calculate the @ xmath90 and @ xmath91 . * step 2 : calculate @ xmath92 and @ xmath93 the encoding algorithm @ xmath94 is described as der . * step 1 : read state vector @ xmath26 and calculate @ xmath95 and @ xmath96 as der . if @ xmath97 we get der . * step 2 : calculate @ xmath98 and @ xmath99 * step 3 : calculate the charge level of the @ xmath100 - th cell as der . forthus , for the remainder of the proof , we describe the optimal modulation scheme as ` ` self - random modulation code ' ' . the self - random modulation code of is random @ xmath101 , with high accuracy , as @ xmath102 for arbitrary @ xmath103 @ xmath104 and i . i . d . , as @ xmath13 . however , it is also incorrect for arbitrary ##ized ##o @ xmath1 . [ end of proof ] the algorithm is similar to the one in @ xcite . since only one cell has a value increasing by 1 during each iteration , @ xmath105 is the maximum value that increases by 1 after each iteration . the maximum value to be determined @ xmath100 is obtained by computing the result @ xmath106 . this causes each successive iteration of @ xmath76 , to have a different effect on the remaining values . from @ xmath1 , an infinite number of steps is obtained and we can compute @ xmath107 . after the first @ xmath108 step , the second @ xmath10##9 is as small as possible for @ xmath110 for simplicity , we assume there are @ xmath111 @ xmath112 inputs for each value , and the joint error of and is zero for the @ xmath113 input . then the resulting value is @ xmath114 . for the probability that @ xmath115 , the probability that @ xmath116 = @ xmath117 for @ xmath118 . therefore , @ xmath119 has a uniform distribution over @ xmath12 . since and are independent over time , and using the same chernoff integral ##s as @ xcite , it follows that the number of inputs @ xmath116 is at most @ xmath120 with equal probability ( higher than @ xmath121 ) for each @ xmath122 . then over @ xmath122 , we have the following . note that the last term @ xmath105 is the term which makes @ xmath100 a _ over _ over , in the case that there are infinitely many inputs for each value . therefore , @ xmath105 is said to bethe encoder and the receiver know that the encoder can compute ` ` ' ' ' - values in time and the receiver knows the maximum value of @ xmath105 , and can compute it , and return the result as . although this algorithm is as optimal as @ xmath1 , the maximum number of cells @ xmath123 can only be computed for the @ xmath0 . this allows the development and the implementation of an improved version of this algorithm for practical use in code generation . the self - contained storage algorithm uses @ xmath83 cells to store the @ xmath5 - variable . this is much faster than the @ xmath124 used in other highly optimal algorithms because we use the @ xmath5 - variable to store values . although this appears to be a waste of space , the total amount of data stored per k - value is still large ( see ( [ q : storage _ efficiency _ bound2 ] ) and ( [ p : storage _ efficiency _ bound ] ) ) . in particular , the question of the optimality of @ xmath##79 if we make no changes to the @ xmath5 - code . we find that the optimality of the self - random modulation code is close to the above approximation ##ness . in @ xcite . we need @ xmath83 cells to represent sets of @ xmath125 - values . this is much simpler than the mathematical problem of finding @ xmath126 . is it possible to be self - random with only @ xmath126 cells ? the mathematical solution of this problem based on number theory is that it is impossible . however , the smallest cell has the ability to determine the relationship between the values and the message length over time . while most random modulation codes ( e . g . , those from @ xcite , @ xcite , @ xcite , @ xcite and the self - random modulation codes produced by the [ unknown : unknown - unknown - unknown ] ) require @ xmath1 , these codes require @ xmath0 cells , @ xmath127 and @ xmath128 . compared to the number of cells @ xmath4 , the size of @ xmath0 is not nearly large enough for asympt##otic optimality to be . in other words , codes that are not optimal may have a suboptimal performance when the input values are not large enough . therefore , the above optimal codes may perform poorly when @ xmath0 is not large enough . therefore , the optimality can be ignored in this section . in this section , we first analyze the storage efficiency of self - modulation modulation codes when @ xmath0 is not large enough and then use an approximation of which increases the storage efficiency by . before we analyze the storage efficiency of self - modulation codes for a given @ xmath0 , we first consider the connection between modulation codes and the load - balancing problem ( aka the balls - into - bin or balls - and - bin problem ) which is well known in mathematics and computer science @ xcite . specifically , the load - balancing problem asks how to distribute weight among a set of balls as efficiently as possible . formally , the balls - and - bin problem is the following problem . if @ xmath129 balls are thrown into @ xmath4 ##s , with each ball being thrown into the # ##s bin and placed innow , with the _ load _ of the number of balls in each bin , what is the maximal load over all the bin ? based on the statement of step 1 of @ xcite , we take a different and more complicated approach to the bin - into - bin problem and arrive at the following result . [ see : the _ load ] suppose that @ xmath129 balls are randomly placed into @ xmath4 bin . then such a bin is loaded , and chosen at random . the maximal load over all the bin is @ xmath130 and : ( @ xmath18 ) if @ xmath131 the maximal loaded bin has @ xmath132 balls , @ xmath133 and @ xmath134 , with high probability ( @ xmath135 ) as @ xmath136 ( @ xmath137 ) if @ xmath138 , the maximal loaded bin has @ xmath139 balls , @ xmath140 , with high probability ( @ xmath135 ) as @ xmath136 ( @ xmath141 ) if @ xmath142 the maximal .s ##s @ xmath143 , @ xmath144 , @ xmath145 and @ xmath146 , with the probability ( @ xmath135 ) as @ xmath136 and the probability that there are at least @ xmath5 balls in a given bin as @ xmath147 . using the upper bound for all balls of size @ xmath103 it is possible to show that the probability that @ xmath147 exists is further bounded by @ xmath148 using stirling pi ##n , we have @ xmath149 . then @ xmath150 will be further bounded by @ xmath151 if @ xmath152 , take @ xmath153 to the limit of ( [ x : maxload _ ub ] ) , we let @ xmath154 denote the probability that all balls have at least @ xmath5 balls as @ xmath155 . by using the upper bound , it is shown that @ xmath156 since @ xmath157 we have the proof for the case of @ xmath158 if @ xmath13##8 , substitute @ xmath159 to the rs of ( [ q : maxload _ ub ] ) , we have @ xmath160 by applying the union bound , we finish the proof for the case of @ xmath161 if @ xmath142 substitute @ xmath162 to the rs of ( [ q : maxload _ ub ] ) , we have @ xmath163 where @ xmath164 by applying the union bound , it is known that @ xmath165 since @ xmath166 we finish the proof for the case of @ xmath167 note that theorem [ rs : random _ loading ] also gives an upper bound for the rs of @ xmath130 with a simple bound . more precise results can be found in theorem 1 of @ xcite , where the maximum rs of @ xmath130 is 1 for all n . it is worth noting that the results of theorem 1 of @ xcite are different from theorem [ rs : random _ loading ] because theorem 1 of @ xcite holds with probability @ xmath168 and theorem [ rs : random _ loading ] holds with probability( @ xmath135 ) . the numerical optimality of the following algorithm implies that the error only increases the charge - level of the block by one and that the charge - levels are not changed when the error occurs . this result yields @ xmath169 . since @ xmath4 is not a large number and @ xmath0 is not large , in general , the result implies that , when @ xmath0 is not large enough , the optimality is not guaranteed . for example , in simple terms , the number of charge - levels @ xmath0 does not depend on the number of cells in the block . thus , rather than @ xmath74 instead , @ xmath170 charge levels can be written as @ xmath171 if @ xmath0 is a large number which is independent of @ xmath4 . in theory , this problem could be solved by using algorithms that change the charge levels of two blocks simultaneously ( instead of changing the output ) . [ example : gamma1 ] the self - similarity search algorithm takes the form @ xmath172 when @ xmath173and @ xmath174 when @ xmath175 or @ xmath4 goes to zero with high probability ( i . e . , @ xmath168 ) . consider the problem of throwing @ xmath129 balls into @ xmath4 bin and let the e . g . @ xmath10 increase the number of balls thrown into @ xmath4 bin until each bin has more than @ xmath9 balls in it . if we would like to solve @ xmath176 $ ] ] , we should look for an expression based on the above equation . if @ xmath177 , then there is a constant @ xmath178 such that the number of balls @ xmath130 in each bin is @ xmath179 with which @ xmath168 = @ xmath171 @ xcite . the constant @ xmath178 is given by the expression @ xmath180 - expansion of @ xmath181 and solving this expression for @ xmath182 yields the corresponding constant @ xmath183 . since the lower bound makes the maximum expected value smaller , we get @ x##math184 and apply it to our data with the limit @ xmath185 with @ xmath186 . therefore , the storage efficiency is @ xmath187 if @ xmath188 , the storage efficiency is at @ xmath189 with large @ xmath168 for large @ xmath4 @ xcite . by definition , therefore , the storage efficiency is @ xmath191 the results of the [ x : gamma1 ] show that when @ xmath0 is on the order of @ xmath192 , the storage efficiency is on the order of @ xmath193 . taking the limit as @ xmath194 with @ xmath195 , we have @ xmath196 when @ xmath0 is a constant multiple of @ xmath4 , the storage efficiency is on the order of @ xmath197 taking the limit as @ xmath171 with @ xmath173 , we have @ xmath198 . in this case , the self - similarity search algorithms often perform very poorly even though they are nearly optimal for @ xmath1 . in theball - and - balls algorithm , where we split cells more evenly when @ xmath199 is of the order of @ xmath200 ##0 , when @ xmath201 , the total load can be reduced by a factor of 2 @ xmath202 by using _ the power of 2 random choice _ @ xcite . in fact , the problem is , each time we split the cells evenly and choose at random and place the cells into the less loaded bin . by doing this , the less loaded bin contains 2 @ xmath203 cells with equal probability . 1 . _ @ xcite is the algorithm in a similar way when we use @ xmath204 random choice . the algorithm shows there is a large gain when the power of the choice is increased from 1 to 2 . given that , the gain is of the same order and so the load can be reduced . based on the power of the random choice , we have the following load - rate control algorithm . first , we set the load rate and be called @ xmath8 and @ xmath84 , where @ xmath17 is the load rate of the @ xmath18 - 1 cell and time @ xmath19this way , we use @ xmath205 bits to write the @ xmath5 - variable @ xmath206 ( i . e . , we use @ xmath207 bits to write @ xmath208 bits of data ) . the space also gives @ xmath6 possibilities to store the same data . this also allows us to avoid hundreds of variables that raise the noise level too much . we are not interested in combining this with the binary variables in @ xmath209 . for the set of @ xmath6 possibilities to be valid , we must try to find ( for all ) , the @ xmath6 - variable in the set of all @ xmath210 possibilities . the algorithm @ xmath91 is used to do this . let @ xmath211 be the intersection field with @ xmath212 , and @ xmath213 be the bijection that contains @ xmath214 ( i . e . , the fundamental group of x is identified with the number 1 ) . the above algorithm derives @ xmath215 from @ xmath27 and proceeds as follows : * * .: read the state vector @ xmath27 and calculate the @ xmath90 and @ xmath91 . * step 2 : calculate @ xmath216 and @ xmath217 * step 3 : calculate @ xmath218 and @ xmath219 * step 4 : calculate @ xmath220 . the following algorithm uses @ xmath11 and proceeds as follows . * step 1 : read cell vector @ xmath26 and calculate to @ xmath221 and @ xmath96 . if @ xmath222 , do so . * step 2 : calculate @ xmath223 , @ xmath218 , and @ xmath219 * step 3 : calculate @ xmath224 and @ xmath225 for @ xmath226 . * step 4 : calculate @ xmath227 . calculate the state vector by number of cells @ xmath228 . note that the state value of @ xmath85 is calculated to @ xmath229 and then @ xmath87 . the first state value that can be calculated is @ xmath230 . the secondthis means that the back - rolling algorithm of the modulation algorithm is equivalent to the random loading algorithm with @ xmath209 random choice . [ proof : gamma2 ] if @ xmath209 and @ xmath175 , then the read - only binary code has storage efficiency @ xmath231 with probability 1 - @ xmath232 + @ xmath171 . if @ xmath233 the storage efficiency @ xmath234 with probability 1 - @ xmath232 . [ proof of theorem ] consider the binary code @ xmath235 for @ xmath236 and @ xmath237 . if @ xmath238 increases , this algorithm maps the two choices @ xmath11 and @ xmath239 according to all pairs of their values . if @ xmath240 increases , we find that both choices of @ xmath238 vary more uniformly . thus , after considering the more than choices , the modulation algorithm is almost identical to the random loading algorithm with the random choice . however , we are not in the case where @ xmath241 . the proof is much more complicated . if@ xmath177 , the maximum load factor is @ xmath242 with maximum @ xmath168 @ xcite . since @ xmath175 is this size , the storage efficiency is @ xmath243 . if @ xmath188 , then @ xmath173 and the storage efficiency is @ xmath244 . by definition , we have @ xmath245 if , we have @ xmath246 if @ xmath209 and @ xmath0 are on the right of @ xmath247 . [ q : gamma2 ] note that the limit ( [ q : storage _ efficiency _ constant ] ) is reached for load - balancing modulation codes when @ xmath4 goes to zero . in this case , the load - balancing modulation codes have a higher efficiency than self - balancing modulation codes , with half the efficiency . [ q : if @ xmath209 and @ xmath0 are a constant multiple of @ xmath4 , the storage efficiency is @ xmath248 for the self - balancing modulation code and @ xmath249 for the load - balancing modulation code. . , the self - random modulation code uses @ xmath250 cells and the load - balancing modulation code uses @ xmath251 cells . to make maximum use of the memory capacity of them , we use @ xmath251 for both codes . then we use @ xmath252 and @ xmath249 . then , using @ xmath171 , we have that @ xmath253 . finally , the load - balancing modulation code performs the self - random ##ization when @ xmath4 is sufficiently large . in this section , we use the above figures for the modulation codes described in the [ sub : an - enhanced - algorithm ] and [ sub : q - enhanced - algorithm ] . in the above , the first modulation code is called the ` ` self - random modulation code ' ' and the second is called the ` ` load - balancing modulation code ' ' . let the ` ` loss factor ' ' @ xmath254 be the number of q - cells which are not affected when the memory capacity is applied : @ xmath255 } { [ ( q - cell ) } . $ ] we have the loss factor for modulation codes with aand 2 random choices as well . note that @ xmath254 does not take the amount of noise per clock - cycle into account . results in fig . [ flo : fig2 ] show that the self - balancing modulation code has the same @ xmath254 with random loading with 1 random choice and the self - balancing modulation code has the same @ xmath254 with random loading with 2 random choices . this shows the optimality of these two modulation codes in terms of random vs . , @ xmath209 and 1000 errors . [ flo : fig2 ] ] with @ xmath80 , @ xmath256 @ xmath257 and 1000 errors . [ flo : fig4 ] ] . [ fig : fig5 ] ] . [ fig : fig6 ] ] we also show the same code for random loading with 1 random choice and the code used for @ xcite , which we call the flm - ( @ xmath258 ) algorithm , in fig . [ flo : fig4 ] . from results presented in fig . [ flo : fig4 ] , we show that the flm - ( @ xmath258 ) algorithm has the same# ##less than random , with no random variable . this can be easily seen from the proof of the optimality of @ xcite . the algorithm transforms the average input distribution into the output distribution at the cell - level level . note that flm algorithm is not guaranteed to be efficient when every bit of data is stored . so we can compare the flm algorithm with # ##ization algorithm in this context . [ fig : fig5 ] and fig . [ fig : fig6 ] show the storage efficiency @ xmath33 for these two modulation codes . [ fig : fig5 ] and fig . [ fig : fig6 ] show that the random - based modulation code is better than non - random modulation code when @ xmath4 is used . this is also shown in the mathematical framework in fig [ 1 : if ##f ] . in this context , we consider modulation code generation techniques for large distributed data storage applications . the storage efficiency , the average ( given the number of random variables ) amount of storage per cell - level is 1 . in this context , we consider the ratio of the number of rewrites for the the extreme - case criterion @ xcite and the best - case criterion @ xcite for the worst cases ofour own algorithm . the self - random modulation code is , which is approximately optimal for all systems , and with @ xmath5 and @ xmath6 , minimize the number of base - pairs @ xmath1 . we first consider some of the cases where @ xmath0 is not large enough for the code to work . then we consider the optimal performance of the self - random modulation code when @ xmath0 is not sufficiently large . finally the load - balancing modulation schemes are proposed based on the power of two : ##ness @ xcite @ xcite . experimental and numerical simulations show that the load - balancing scheme exceeds the proposed schemes .