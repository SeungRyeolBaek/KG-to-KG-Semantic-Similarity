model selection is an important step in many areas of machine learning . if a particular model is not selected , any hope for the selection or optimization of the model ##ic ##m is lost . given a set of observed data , the goal of model selection is to select the model that best fits the observed data and captures the observed data . model selection parameters are chosen such that they strike a balance between the _ goodness - of - fit ( gof ) _ , and the _ generalizability _ / _ complexity _ of the model . goodness - of - fit is how accurately the model captures the properties of the data . generalizability / complexity is the combination of the performance of the model on observed data and how well the model fits / captures the observed data . models with more complexity than this will suffer from overfitting and poor performance , and models that are too complex will underfit and have poor gof @ xcite . cross - validation @ xcite , bootstrapping @ xcite , akaike information criterion ( aic ) @ xcite , and robust information criterion ( bic ) @ xcite , are all good examples of robust model selection . with cross - validation techniques such as cross - validation and testing , the model##ization error of the model is estimated using monte carlo methods . in contrast with random - sampling methods , the kernel estimation methods in aic and bic do not take time to estimate the model error , and are highly accurate . in these methods , _ _ _ _ is chosen so that the model error is reduced by removing the model error ##áµ¢ from the data . a large number of kernel methods have been developed with different motivations that lead to different statistical properties . for example , the large model error in bic favors large samples , whereas aic works best when the dataset has a very large sample size . kernel methods are simple , and efficient statistical methods that are capable of performing on high dimensional data with arbitrarily large samples . they have been widely used in a range of applications such as optimization , and modeling . in kernel estimation , the data are transformed from their original space to a higher dimensional feature space , the residual kernel feature space ( rkhs ) . the idea behind this method is to transform the linear mapping between the points in the feature space into an easy - to - compute linear correlation function on the feature space . for example , in kernel estimation the target variable is represented as a linear combination of the input variables .every algorithm that can be implemented through this matrix performs a kernel selection . this process , called kernelization , makes it possible to transform weaker , already known , model selection methods into stronger , faster optimization methods . the research on kernel methods has , historically , mostly focused on model selection and on tuning the ridge parameters , but with recent work being done on kernel - based model selection @ xcite . in this study , we use a kernel - based optimization strategy for linear regression analysis . in kernel model selection ( krr ) , tuning the ridge parameters to obtain the most likely value with respect to the model at hand and the simulation data is the goal of the kernel model selection algorithm . in kernel model selection , the success of the model selection criterion is proven simply by setting a boundary condition where the sample size goes to infinity and running the simulation data for all sample sizes . kernel methods require a certain upper bound of the success of @ xcite . proving the general ##ization of the model selection criterion _ kernel _ selection _ is difficult . the specific ##s of the kernel selection does not apply here . the conditions for that are : the failure of the kernel to converge , such as failing / overfitting @ xcite is not apparent ( for @ xmath##1 the estimates of ( @ xmath2 , the inverse of @ xmath3 , which is independent of @ xmath2 ) and the estimates of the kernel of 2 are possible to use in rkhs . they have kernelized the kernel model selection methods and published the results of their kernel model selection methods . kobayashi and komaki @ xcite used the kernel - based kernel information criterion ( kric ) from the same data to calculate the kernel function in kernel linear regression and support vector model ( svm ) . rosipal et al . @ xcite used kernel information criterion ( cic ) for parameter selection in kernel principal branch regression , because of their better performance compared to aic and bic in generalized linear regression . demyanov et al . @ xcite , used a method of calculating the kernel function using akaike information criterion ( aic , @ xcite and fisher information criterion ( bic , @ xcite ) , and used it for parameter selection in svms using the same data . as pointed out by van emden @ xcite , the kernel model is the model with the most independent parameters . .using a complexity measure that measures the number of model parameters allows us to estimate the most likely parameters . in this way , we construct a linear step - ##wise model and define the complexity term as the maximum number of parameters based on the data . using the complexity measure in this way , captures the complexity of each parameter of the model . we call this optimization _ _ kernel - based selection criterion ( kic ) _ . the optimization criteria for general computational complexity ( gpr ; @ xcite ) , and kernel - based computational complexity ( icomp ; @ xcite ) include kic in being a kernel - based complexity measure . however , the results differ because these two measures measure the complexity of the data parameters rather than the model parameters . although we do not measure the complexity property of kic directly , we can measure the performance of kic based on theoretical and experimental datasets using state - of - the - art methods similar to no - way - out - of - validation ( loocv ) , kernel - based icomp , and linear log - likelihood based gpr . the research is summarized as follows . in section [ 1 : krr ] , we give an overviewof the ##ized ##e . kic is described in detail in section [ sec : kic ] . section [ sec : om ] is just a brief description of the degree to which kic is used , and in section [ sec : exp ] we describe the use of kic in series of tests . in regression analysis , the error is of the form : @ xmath4 where @ xmath5 can be either a linear or non - linear regression . in linear regression we have , @ xmath6 , where @ xmath7 is an unknown vector ( data vector ) of size @ xmath8 , @ xmath9 is a pseudo - random vector of random coefficients of size @ xmath10 , and @ xmath11 , is an unknown vector of regression coefficients , where @ xmath12 is the variance . we can see that the error ( error ) of @ xmath13 is an @ xmath1 - dimensional error whose coefficients are , i . e . , , @ xmath14 , where @ xmath15 is the @ xmath1 - dimensional variance , and @ xmath16 is the expected variance . the regression coefficients are the expected variance , @ x##math17 , with kernel function @ xmath18 , and kernel function @ xmath5 . when @ xmath19 , the problem is ill - posed , in that some kind of solution , such as tikhanov method ( ridge regression ) is used , and the solutions are the following : : @ xmath20 where @ xmath21 is the regression coefficient . the estimated regression coefficients for ridge regression @ xmath22 are : @ xmath23 . _ _ _ ridge regression ( krr ) , the regression coefficient @ xmath9 is non - explicitly transformed in rkhs with the help of @ xmath24 . the estimated regression coefficients based on @ xmath25 are : @ xmath26 where @ xmath27 is the kernel function . problem [ x : 0 ] does not have an exact solution for @ xmath28 because of @ xmath24 ( the kernel function allows one to avoid explicitly transforming @ xmath25 that could be considered , if not in rkhs , if possible ) , so a kernel function is used ( e . g . @ xcite ) that transforms @ xmath24 : @xmath29 and @ xmath30 . the definition of krr is equivalent to calculating the kernel function out of the matrix coefficients , where the kernel function is : @ xmath31 and @ xmath32 are the corresponding rkhs . for @ xmath33 , and @ xmath34 we have : @ xmath35 where @ xmath36 denotes the kernel function , and @ xmath37 . the main objective of this paper is to define a new kernel - based information measure ( kic ) for the model definition of kernel - based models . key to the kic lies in the goodness - of - fit and the complexity of the model . gof is defined as the maximum - likelihood - based model ( we call the maximum likelihood ) and the complexity measure is a measure based on the likelihood function of the kernel of the model . in the following paper we expand on these results . the definition of the emden @ xcite for the information measure of a random variable is based on the correlation among the variables and the associated correlation function . the best model is the model with the least random variables . this decreases the estimated variance and increases the complexity . in this paperwe focus on this case of the complexity measure . given an @ xmath2 - independent probability distribution @ xmath38 , the entropy of the random matrix , @ xmath39 , is given by the ' s of @ xcite , @ xmath40 where @ xmath41 , @ xmath42 are the mean and the average entropy , and @ xmath43 is the @ xmath44 - variance of @ xmath39 . @ xmath45 if and only if the covariates are independent . the complexity measure in equation is with orthonormal basis because it is dependent on the size of the random matrix , @ xmath46 @ xcite . to overcome these problems , bozodgan and haughton @ xcite developed icomp , along with a complexity measure based on the inverse random matrix , which gives an upper bound for the complexity measure in equation : @ xmath47 this complexity measure is equal to the total entropy ( @ xmath48 ) and computational complexity ( @ xmath49 ) of the elements of the random matrix . for values of @ xmath50 , is the limit .random variables , and vice versa . zhang @ xcite computed the following version of this complexity measure @ xmath50 , that is , the kernel - based version of the objective function : @ xmath51 the complexity measure in point group selection ( gpr ; @ xcite ) is computed as @ xmath52 , a reduction from the kernel of @ xmath42 ( as described in the [ 1 : 1 ] ) . in contrast to icomp and gpr , the complexity measure in kic is computed on the gram - schmidt ( sd ) version of the kernel matrix , @ xmath53 . using this complexity one finds a model with the independent parameters . in the following section , we explain in detail how to compute the random variable - based version of the complexity measure , and the definition of the complexity measure . + in kernel - based model selection algorithms such as icomp , and gpr , the complexity measure is computed on the kernel matrix that is of size @ xmath54 for @ xmath9 of size @ xmath10 . the idea of this algorithm is to measure the size of the independent parameters , which are of thesize of the kernel is @ xmath2 . in the other case , the effect of the size of the kernel is zero because of the size of the kernel . to define a parameter class that depends on @ xmath2 , we define component - wise variance as an linear combination of variance for each component of the model . let @ xmath55 denote the parameter class of the model ##s of : @ xmath56 where @ xmath57 and @ xmath58 , and @ xmath59 the variance of krr is given by @ xmath60 . the quantity @ xmath61 = \ alpha ^ { \ operatorname { tr } [ [ ( 1 + \ alpha } ) ^ { - 1 } ] $ ] can be written as the sum of variance for the component - wise parameter vector , if the following sum of variable - wise variance is obtained : @ xmath62 where @ xmath63 and @ xmath64 are the k - th pair of parameters @ xmath65 and @ xmath66 . with this result obtained , the quantity @ xmath67 can be written as : @ xmath68 where @ xmath6##9 is a variant of @ xmath70 , the rkhs given by @ xmath71 . the variance @ xmath28 in this case is given by @ xmath72 where @ xmath73 , and since @ xmath69 the variance [ 1 : 1 ] is equivalent to @ xmath74 . let @ xmath75 be the inverse matrix of @ xmath74 and @ xmath69 = @ xmath76 . we have @ xmath77 , \ end { t } \ ] ] where @ xmath78 is the identity associated with @ xmath71 . since @ xmath79 , we have @ xmath80 = \ operatorname { tr } [ \ sigma _ { \ sigma } ] . \ end { aligned } \ ] ] . the second case with real - valued variance ##s determines the size of the kernel of the model ( and the size of the variance of the kernel ) . . + gretton et al . @ xcite ##s a kernel - based complexity criterion , called the gram - schmidt complexity criterion ( hsic ) , which issee below . suppose @ xmath81 , and @ xmath82 are unit vectors with linear components @ xmath83 , and @ xmath84 , where @ xmath85 , and @ xmath86 are rkhss . the cross - correlation function corresponding to the joint joint distribution @ xmath87 is a linear combination , @ xmath88 such that : @ xmath89 , \ end { aligned } \ ] ] where @ xmath90 is the inner product , @ xmath91 = e [ k ( \ cdot , y ) ] $ ] , and @ xmath92 = e [ k ( \ cdot , y ) ] $ ] , for @ xmath93 , and the operator , @ xmath36 . the hsic norm for the rkhs @ xmath85 , and @ xmath86 is the positive semi - norm of the cross - correlation function and is defined as : @ xmath94 \ end { aligned } \ ] ] * = 2 . * suppose @ xmath95 , and @ xmath96 are vectors , forfor @ xmath97 , and @ xmath98 , @ xmath99 , and @ xmath100 , @ xmath101 if and only if @ xmath102 , and @ xmath7 are equal ( as stated in @ xcite ) . + by replacing the hsic of the matrix s with the two of @ xmath103 we can check the order of the coefficients . since @ xmath104 is a strictly positive positive - definite matrix , @ xmath105 , and the inverse of the square root of the symmetric matrix is equal to : @ xmath106 = \ phi _ { i = 1 } ^ { _ _ } ^ { \ nonumber \ \ ~ ~ & = \ phi ^ { \ operatorname { tr } [ k ( k + \ alpha i ) ^ { - 1 } + ( k + \ alpha ii ) ^ { - 1 } ] \ { { tr } \ ] ] kic is defined as : @ xmath107 where @ xmath108 is the independent variable acting on the [ s : t ] . the definition of @ xmath109 ##8##s a complexity measure that is sensitive to changes in data ( similar to icomp ##s ) . the parameter kic is the optimal fit . ] . the maximum maximum - likelihood ( pll ) of krr for uniformly distributed data is given by : @ xmath110 the measures ##s @ xmath22 , and @ xmath111 are defined by using the kic hash function . @ xmath112 we can see the use of [ @ xmath113 $ ] , and @ xmath61 $ ] as complexity measures . the same study showed that the [ subsec : realdata ] used two datasets , and correlated with kic . we define these two measures as : @ xmath114 , \ end { aligned } \ ] ] @ xmath115 . \ end { aligned } \ ] ] for both kic _ 1 , and kic _ 2 , similarly to kic , @ xmath116 , but because the complexity measure is based on @ xmath16 , @ xmath111 for kic _ 2 is : @ xmath117 = 0 . \ end { aligned } \ ] ] if wefor @ xmath118 , @ xmath111 is the solution of the cubic cubic problem , @ xmath119 , where @ xmath120 . in the case of kic _ 1 , the @ xmath111 is the exact solution of the cubic cubic problem : @ xmath121 where @ xmath122 $ ] . we compare kic with loocv @ xcite , matrix - based icomp @ xcite , and the estimate of maximum likelihood using gpr ( denoted by gpr ) @ xcite to estimate the regression of regressors . the reason to compare kic with icomp and gpr is that for both of these methods the scale factor measures the number of the parameters as a function of the dimensions in different dimensions . loocv is a simple and widely used method for model selection . * loocv : * non - linear model selection methods for cross - validation are very efficient @ xcite . for example , the one - parameter - of - cross - validation ( loocv ) : the computational time of @ xmath123 the number of parameter values ( @ xmath124 ) the computational timeof the model selection ( @ xmath125 ) for @ xmath126 of data . to support self - selection algorithms with fast learning times , the closed form expressions for the efficiency factor of the algorithm under certain conditions are used . we use the kernel - based regression method of loocv for the regression given by @ xcite : @ xmath127 ^ { - 1 } [ i - j ] { \ | _ i ^ { } { align } \ | { align } \ ] ] where @ xmath128 is the density function . * * the log of marginal likelihood ( gpr ) * is a kernel - based regression method . for a given data set @ xmath129 , and @ xmath130 , the corresponding prior distribution is based on a sample @ xmath5 such that , @ xmath131 , where @ xmath39 is the mean . marginal likelihood is used as the model selection parameter in gpr , since it distinguishes between the goodness - of - fit and fit of the model . finding the log of marginal likelihood is the best estimate for model selection . the log of marginal likelihood is defined as : @ x##math132 where @ xmath133 denotes the model of interest , @ xmath134 , denotes the model , and @ xmath135 is a positive constant . without lack of reference in this article gpr : the model selection algorithm is defined in gpr . * icomp : * the model - based icomp algorithm in @ xcite provides an efficient basis to select the model and is given by @ xmath136 , where @ xmath50 , and @ xmath39 appear in papers [ q : cicomp ] , and [ p : sigmaicomp ] . in this paper we compare the performance of kic on experimental , and theoretical datasets , and compare with other model selection methods . kic was first developed on the basis of the @ xmath137 from a set of random samples sampled at regular intervals [ @ xmath138 $ ] . to improve sensitivity to noise , the same noise was applied to the @ xmath139 model with two mean - to - variance ( nsr ) ratios : @ xmath140 , and @ xmath141 . the [ sinc ] is the sinc , andthe following datasets . the following experiments were conducted : ( 1 ) shows how kic balance affects gof and complexity , ( 2 ) shows how kic and mse of test sets change when the sample size and the amount of complexity in the sample , ( 3 ) shows the effects of using multiple models , and ( 4 ) shows the effects of kic and complexity estimation . these experiments were conducted multiple times with randomly selected datasets , and with test sets of over 100 . * table 1 . * the effect of @ xmath21 on complexity , goodness - of - fit and kic balance was measured by comparing @ xmath142 , with krr ##2 , calculated from the normal distribution with a standard deviation , @ xmath143 , calculated from the same data ##set . the results are summarized in table [ test _ test _ kic ] . the model associated with @ xmath144 overfits , because it is more complicated , while @ xmath145 is a simple model that underfits . as the complexity of @ xmath21 increases , the model complexity increases but the goodness - of - fit is not affected . kic balance . thesethe parameters , which are a way to select a model that has good stability , as well as goodness of fit to the data . * figure 1 . * the effect of the sample size was measured by using sample sizes , @ xmath1 , of 50 , and 100 , for a sample of four sets of data : ( @ xmath146 ) : ( @ xmath147 ) , ( @ xmath148 ) , ( @ xmath149 ) , ( @ xmath150 ) . the same distribution was measured with @ xmath151 . the kic distribution and mean squared error ( mse , @ xmath152 ) , for example @ xmath153 @ xmath154 are shown in figure [ kic - mse ] . the data with nsr = @ xmath141 have higher mse values , and higher error rates , and therefore higher kic values compared to data with nsr = @ xmath140 . in both cases , kic and mse are with different values with respect to @ xmath21 . the parameters and the sample size have an effect on kic for selecting the best model ( as@ xmath21 ) . * * * . * the possibility of using the random kernel , @ xmath155 , versus the cauchy kernel , @ xmath156 , was suggested , where @ xmath157 , and @ xmath158 allowed the use of the marker - based methods proposed by icomp , kic , gpr , and loocv . the results were reported in graphs [ random kernel ] and [ cauchy kernel ] . the graphs showed two examples with markers , @ xmath159 , and @ xmath160 of the normal distribution of mse values . as expected , the mse of both methods is larger when nsr is used , @ xmath161 , and smaller for the average of the two data sets ( random kernel ) . loocv , icomp , and kic reported better , and better than gpr versus the random kernel for experiments with nsr @ xmath162 . in the other cases , the same results ( smaller mse ) were obtained with kic . all methods produced larger mse values than the random kernel versus the cauchy kernel . gpr withthe cauchy test gives results consistent with kic , but with a standard deviation closer to 1 . * * * . * we tested the frequency of selecting / tuning the parameters of the model for comparison with loocv . we used two frequencies of different frequency , @ xmath163 , and nsr @ xmath164 . the frequencies to select / tune were @ xmath165 @ xmath166 , and @ xmath167 for the original model . the frequency of selecting the parameters is shown in figure [ loocv ] for loocv , and in figure [ kic _ n ] for kic . the more frequent sampling , the more accurate the frequency . the results show that kic is more reliable for selecting the parameters , than loocv . loocv is more sensitive to sample size . it provides a more reliable basis for comparison with @ xmath168 * . + we used the samples taken from the following datasets ( www . canada . carleton . edu / ~ / / ~ ) : ( 1 ) core dataset ( 4177 samples , 4 samples ) , ( 2 ) sub - set of datasets ( 41datasets ; 8192 instances , 8 dimensions ) , and ( 2 ) pu - family of datasets ( 8 datasets ; 8192 instances , 8 dimensions ) . for the kin dataset , the goal is to determine the number of fish . we use 100 samples with probability [ 0 , 1 ] . the process is repeated several times to determine the confidence level . in each iteration 100 instances are selected randomly as the data set and the remaining 4077 instances as the data set . the kin - family and pu - family datasets are mathematical models of a fish , taking into account values of parameters such as whether the fish motion is linear ( n ) or nearly linear ( m ) , and whether the level of confidence ( confidence ) of the fish is : low ( m ) , or high ( n ) . the kin - family contains : kin - 8fm , kin - 8fh , kin - 8 nm , pu - 8nh datasets , and the pu - family contains : kin - 8fm , kin - 8fh , kin - 8 nm , and pu - 8nh datasets . for the pu - family of datasets , thethe angular position of the x - axis robot arm , the distance of the # ##point of the robot arm from the starting point are measured . the angular position of each member of the robot arm is determined by the relative positions , the distances , and the lengths of the arms . we compare kic _ 1 ( [ q : kic1 ] ) , kic _ 2 ( [ q : kic2 ] ) , and kic with loocv , icomp , and gpr in the three datasets . the results are shown as v - shapes for the [ kin ] , [ kin - family ] , and [ cat - family ] for kin , kin - family , and cat - family datasets , respectively . the best results for the three datasets were obtained using kic , and the second best results were for loocv . for the second dataset , better results were obtained for kic and loocv , that were smaller than icomp , and the best mse values obtained using sgpr . kic _ 1 , and kic _ 2 obtained larger mse values , which are smaller than for the other data . for the cat - family datasets, and for kin - 8fm , kic gets better results than gpr , icomp , and loocv . kic _ 1 , and kic _ 2 get better results than gpr , and loocv for kin - 8fm , and kin - 8 nm , which are datasets with low levels of noise , and lower mse values for datasets with low noise ( kin - 8fh , and kin - 8nh ) . for the noise - based datasets , kic gets the best results for all datasets except for the kin - 8 ##fm , where the lowest mse is obtained by loocv . the median of kic is comparable to icomp and better than gpr for kin - 8 ##fm dataset . for kin - 8fm , kin - 8fh , and kin - 8nh , although the median of mse for loocv and gpr is comparable to kic , kic gets a less large mse ( less interquartile in the sample size ) . the median mse values for kic _ 1 , and kic _ 2 is comparable to the median mse values of the two methods for kin - 8fm, and alpha - 8 ##f , where the noise level is low , to alpha - 8fh , and alpha - 8nh , where the noise level is high . the resistance of kic _ 1 , and kic _ 2 to noise is due to the lack of variance in their models . kic _ 2 has a higher interquartile of mse than kic _ 1 for datasets with high variance , which explains the use of @ xmath109 for the formula ( see [ ref : kic2 ] ) rather than @ xmath16 ##9 ##a . we propose a new kernel - based complexity criterion ( kic ) for parameter selection in regression analysis . the complexity criterion of kic is defined on a component - wise basis which directly measures the complexity of each parameter used in the model ; whereas in methods such as kernel - based icomp and gpr , this basis is defined as a sparse basis , which measures the total number of the involved parameters . we have experimental evidence of how kic , loocv ( with kernel - based closed form version of the model ) , kernel - based icomp , and gpr ,in the artificial noise and natural data datasets : abalon , the ##net , and kin ##ect . in these cases , kic successfully improves the quality of analysis and interpretation of the data , is sensitive to noise ( although for artificial noise we have higher confidence levels than normal ) and sample size , is efficient in generating / adjusting the # and the ##s , and has significantly lower or higher prediction error rates with respect to competing methods , even using different regressors . the possibility of using different algorithms was also demonstrated since the choice of a common kernel plays an important role in competing methods . kic demonstrated superior performance for different algorithms and for the only one with lower mse . this work was published by fnsnf ##2 ( p1tip2 _ 148352 , pbtip2 _ 140015 ) . we want to thank david gretton , and zoltn szab for the pioneering work .