additive models @ xcite are an important class of models for semiparametric analysis or statistics . some advantages for the use of additive models are their greater accuracy when compared to linear or nearly linear models and their better interpretability when compared to fully linear models . it is well - known that bad estimators in additive models are in general more sensitive to the problem of sample variance than good ones in fully linear models . some types of additive models belong to the general class of the kernel based models over a finite dimensional vector space @ xmath0 , see e . g . @ xcite . in the recent years some interesting results on the properties of the kernel based models for additive models have been obtained when the emphasis is on learning and when the appropriate least squares loss function is used , see e . g . @ xcite , @ xcite , @ xcite , @ xcite , @ xcite , @ xcite and the articles below . of course , the least squares loss function is good and has some interesting mathematical properties , but it is not a lipschitz , and therefore , kernel based models based on this loss function often rely on poor numerical optimization problems , .if the kernel is bounded . this is in sharp contrast to s ##s ##nr on a lipschitz continuous influence function and on a general loss function , where bounds and lower bounds for the maxbias function and for the continuous influence function are known , see e . g . @ xcite for the special kernel and @ xcite for general kernel . however , we can also consider the case of regularized kernel based methods based on a special kernel and lipschitz continuous loss function , for the general kernel , and use the classical error function @ xmath1 for and @ xmath2 which has a convergence penalty but not a convergence penalty , see e . g . @ xcite . the special kernel based methods are also often called support vector methods ( svms ) , although the term was originally used for the methods based on the general kernel loss function and for these cases only , we refer to @ xcite . in this article we raise the important question , whether an svm with an additive kernel can have a much higher learning rate in higher dimensions than an svm with a general kernel , either a kernel or rbf kernel , if the property of the additive kernel is violated . our previous paper compared learning rates forquantile regression relies on the lipschitz kernel and non - linear conditional error function , which is also a kernel space in the model , see e . g . @ xcite and @ xcite for kernel quantile regression and @ xcite , @ xcite , and @ xcite for kernel - quantile regression . we will not answer the question how to determine whether the loss of the additive kernel is additive because this would be a matter of a test of our own . of course , a better solution would be to test different methods and have their results compared for different purposes . for the same reason we will also not consider variance . loss of the kernel space given by the kernel for linear regression is included in @ xcite . in this way we measure learning rates for these methods . let us model the model with a complete support vector space @ xmath3 as the input space and a complete subset @ xmath4 of @ xmath5 as the output space . the borel - space @ xmath6 and @ xmath7 is drawn to measure the learning rate and the continuous and normally distributed measure @ xmath8 is drawn next to @ xmath6 for variance . the loss of@ xmath9 is used to improve the performance of the loss function @ xmath10 with the finite error @ xmath11 . _ in the algorithm we see that @ xmath12 is smooth , @ xmath13 , smooth with respect to the first parameter , and @ lipschitz , for @ xmath14 with the finite error @ xmath15 . _ support vector machines ( svms ) . here are vector - valued approximation algorithms for the sparse uniform hilbert space ( rkhs ) @ xmath0 generated by the generalized measure @ xmath16 . with the generalized loss function @ xmath17 . for problems dealing with heavy - tailed distributions on @ xmath18 , they take the form @ xmath19 where for the generalized borel measure @ xmath20 and @ xmath21 , the measure @ xmath22 is generated by @ xmath23 where @ xmath24 is the general algorithm . the technique to generate a loss function has a long history , see e . g . @ xcite in the context of k - theory . it is shown in @ xcite that @ xmath22 is not aminimizer of the following optimization problem is the _ objective function @ xmath12 if the minimizer is : @ xmath25 the mathematical problem we consider consists of the _ metric space of _ @ xmath26 with _ @ xmath27 a _ _ metric space and the _ subset of _ @ xmath28 where @ xmath29 is the set of quantities @ xmath30 each of which is also defined as a function @ xmath31 from @ xmath3 to @ xmath5 . all the functions from @ xmath32 have the general form @ xmath33 . we note , that this is clearly not a notational error here , because in the above definition each quantity @ xmath34 is an element of the set @ xmath35 which is a subset of the full input space @ xmath36 , @ xmath37 , whereas in the definition of _ @ xmath8 each quantity @ xmath38 is an element of the full input space @ xmath36 , where @ xmath39 . because these formulas can only be used in certain contexts and because we do not have any constraints , we assume this definition is correctand more convenient than replacing these numbers with the numbers . the additive kernel @ xmath40 is defined in terms of the polynomials @ xmath41 , @ xmath27 and @ xmath42 . is the rkhs @ xmath0 which can be defined in terms of the rkhs @ xmath43 given by @ xmath41 and @ xmath27 according to the formula ( [ proof ] ) is @ xmath44 with norm given by @ xmath45 the norm of @ xmath46 is @ xmath47 to illustrate properties of additive kernel , we have two ways of replacing them with additive kernel . the first way is with the rbf algorithm . the second can be found in the [ proofsection ] . [ gaussadd ] = @ xmath48 , @ xmath49 $ ] and @ xmath50 ^ [ . $ ] = @ xmath51 and @ xmath52 . \ ] ] the additive kernel @ xmath53 is given by @ xmath54 . , the additive kernel @ xmath55 is the additive product kernel given by @ xmath##56 is the mercer kernel @ xmath57 [ @ xmath58 ^ 2 $ ] depending only on the parameter . @ xmath59 , @ xmath60 then @ xmath61 where @ xmath62 are the rkhs given by the corresponding - rbf function @ xmath63 . the second class is the sobolev kernel . [ sobolvadd ] [ @ xmath64 , @ xmath65 $ ] and @ xmath58 ^ 2 . $ ] let @ xmath66 : = \ bigl \ { x \ in l _ 2 ( [ 0 , 1 ] ) ; } ^ \ le { \ in l _ 2 ( [ 0 , 1 ] ) \ mbox { ~ for ~ { ~ } | \ le | \ le ##q \ bigr \ } \ ] ] is the sobolev kernel ##s of all square integrable univariate functions whose kernel is also square integrable . this is the rkhs with the mercer kernel @ xmath67 . [ @ xmath68 ^ 2 $ ] . if we take from the mercer kernel @ xmath69 tofor @ xmath67 , be @ xmath70 $ ] for each @ xmath71 . the additive kernel @ xmath72 is also an additive kernel and is an rkhs @ xmath73 \ { \ } . \ ] ] however , the corresponding sobolev kernel @ xmath74 ^ { ) $ ] , consisting of all square integrable functions whose partial derivatives are also square integrable , has no points and is not an rkhs . define the probability distribution of @ xmath6 , @ xmath27 and @ xmath75 . under the assumption that @ xmath76 for each @ xmath71 and that @ xmath43 is included in @ xmath29 of the @ xmath77 - family , it is shown by @ xcite that @ xmath78 has probability as high as @ xmath79 , @ xmath80 and @ xmath81 . the rest of the article follows the same procedure . the [ ratessection ] describes our recent results on learning rates for svms based on additive kernel . learning rates for quan##tile problems are mentioned for some special cases . section [ comparisonsection ] contains a comparison of our results with the learning rates mentioned previously . section [ proofsection ] contains all the problems and the methods which can be used in their application . in this section we describe the learning rates for the support - space generated by the formula for additive functions which can explain the interesting results found in @ xcite . the rates are about the value of the excess risk @ xmath82 and take the triple @ xmath83 with @ xmath84 . they can be computed using four sets of conditions : the hypothesis space @ xmath0 , the measure @ xmath6 , the measure @ xmath12 , and the value of the input measure @ xmath85 . the first condition is about the approximation ability of the hypothesis space @ xmath0 . since the additive function @ xmath19 comes from the hypothesis space , the learning rates of the above conditions depend on the approximation ability of the hypothesis space @ xmath0 with respect to the excess risk @ xmath86 such as the following : ##s . [ defapprox ] the approximation ability of the measure @ xmath8##7 is defined by @ xmath88 to minimize the prediction error , we make an assumption on the minimizer of the risk @ xmath89 for each @ xmath90 , and the integral of @ xmath91 . with the risk @ xmath41 by @ xmath92 we find that @ xmath93 is a positive and bounded operator on @ xmath94 . then we can define the corresponding eigenpairs @ xmath95 and that @ xmath96 is the orthonormal product of @ xmath94 and @ xmath97 by @ xmath98 . then @ xmath99 . then we can define the @ xmath100 - the integral @ xmath101 of @ xmath93 by @ xmath102 this is a positive and bounded operator and its range is well - defined . the value @ xmath103 and @ xmath104 is in this range . [ assumption1 ] we define @ xmath105 and @ xmath106 where for each @ xmath107 and each @ xmath10##8 , @ xmath109 is a function of the function @ xmath110 with some @ xmath111 . the component @ xmath112 of the [ assumption1 ] of the @ xmath113 is called the rkhs @ xmath43 . the general expression in the literature ( e . g . , @ xcite ) for the convergence of the function @ xmath114 for the approximation error ( [ approxerrordef ] ) is @ xmath115 with some @ xmath116 . since the function @ xmath117 is expressed as @ xmath118 in general , this can also be expressed in an additive form . however , the hypothesis function ( [ hypothesis ] ) has an additive form @ xmath119 . so it is possible for one to write an additive expression @ xmath120 for the marginal function @ xmath121 with the value of @ xmath113 satisfying the additive expression @ xmath110 . the following general ##ization applies to the general problem of calculating the approximation error : the function @ xmath113 has a direct relationship to the marginal function @ xmat##h122 holds onto @ xmath27 , because other methods in the field ( e . g . , @ xcite ) can not be used properly . note that in the product space @ xmath123 , there is no known risk function different from @ xmath6 , and the effect of @ xmath124 is not known . our way to solve the problem is to find an error function @ xmath125 . it does not have a solution ( which is not always possible ) . however , it fits the target of @ xmath113 perfectly . when we find all the targets @ xmath126 , we get a good estimate of the risk function @ xmath121 , and therefore a good estimate of the prediction error . this is the end result of the algorithm . [ approxerrorthm ] under assumption [ assumption1 ] , we find @ xmath127 where @ xmath128 is the target given by @ xmath129 the initial value for our error function is exactly the size of the product space given by @ xmath130 - the risk function . let @ xmath131 be the set of points in @ xmath##21 and @ xmath132 for some @ xmath133 the * covering number of @ xmath131 * with respect to the associated ball @ xmath134 , such that @ xmath135 is defined as @ xmath136 and the * @ xmath130 - empirical covering number * of @ xmath137 is defined as @ xmath138 [ assumption2 ] we have @ xmath139 and that for some @ xmath140 , @ xmath141 and some @ xmath142 , the @ xmath130 - empirical covering number of the center ball of @ xmath43 and @ xmath143 the main aim of this paper is to show that the compact ##ification of the hypothesis space gives the smallest upper bound with a thirty - one absolute value for the covering number of the ball of the hypothesis space @ xmath0 , to be found in the [ samplesection ] . [ capacitythm ] under the [ assumption2 ] , for some @ xmath144 and @ xmath145 , we have @ xmath146 the formula for the covering number ofthe theorem [ capacitythm ] is that : the measure @ xmath147 is independent of the size @ xmath148 of the components of the additive model . it is well - known @ xcite in the context of hypothesis testing that the covering numbers of components of the sobolev space @ xmath149 and the [ @ xmath150 ^ \ $ ] of the function space @ xmath151 with the dimension @ xmath152 have the same probability distribution with @ xmath153 : @ xmath154 . the measure @ xmath155 depends only on the number @ xmath148 . the dimension - independent formula for the covering numbers of the rkhss associated with the rbf - models can be found in @ xcite . the special case of theorem [ capacitythm ] describes the properties of the additive model in terms of properties of the corresponding hypothesis space . the general condition for our previous problem is that the probability distribution of the measure @ xmath6 with respect to the hypothesis space . after stating the general condition , we consider the special case for quantile ##s , to prove our previous theorem . the @xmath156 is the quantile constant . the quantile loss function @ xmath157 is defined by the value @ xmath158 to be the @ xmath159 - quantile of @ xmath160 , i . e . , the value @ xmath161 of @ xmath162 the error formula for quantile regression defined here takes the form ( [ algor ] ) with the loss function @ xmath12 defined by the exponential loss function @ xmath163 the failure probability of @ xmath6 for quantile regression is defined for @ xcite as follows . to this end , let @ xmath164 be a joint distribution of @ xmath165 and @ xmath166 . then the real number @ xmath167 is the @ xmath159 - quantile of @ xmath164 , if and only if @ xmath167 belongs to the ( @ xmath168 \ bigr ) \ le \ le \ mbox { ~ ~ and ~ ~ } ( \ bigl ( [ 0 , \ infty ) \ bigr ) \ le ##q - \ le\ bigr \ } \ , . \ ] ] it is well - known that @ xmath169 is a random set . [ noisecond ] = @ xmath166 . 1 . a probability measure @ xmath164 on @ xmath165 is said to have a * @ xmath159 - quantile of type @ xmath170 * , if there exists an @ xmath159 - quantile @ xmath171 and the constant @ xmath172 such that , for [ @ xmath173 $ ] , we have @ xmath174 $ . [ @ xmath175 $ ] . we say that a probability measure @ xmath20 on @ xmath176 has a * @ xmath159 - quantile of @ xmath177 - of type @ xmath170 * if the corresponding probability measure @ xmath178 on @ xmath179 - , has an @ xmath159 - quantile of type @ xmath170 and the constant @ xmath180 where @ xmath181 is the probability given by : ( for) , and @ xmath182 . we can assume that a distribution @ xmath164 with an @ xmath159 - quantile of type @ xmath170 has a corresponding @ xmath159 - quantile @ xmath183 . similarly , if @ xmath164 has a lebesgue distribution @ xmath184 then @ xmath164 has an @ xmath159 - quantile of type @ xmath170 if @ xmath184 is far away from the [ @ xmath185 $ ] since we can [ @ xmath186 \ } $ ] = ( [ tauquantileoftype2formula ] ) . this definition is broad enough to include all distributions used in hypothesis testing such as normal , normal , @ xmath187 , and normal distributions ( with @ xmath188 ) , gamma and log - gamma distributions ( with @ xmath189 ) , and normal and normal distributions ( with @ xmath190 $ ] ) . the following formula , to be stated in the [ proofsection ] , gives the confidence level for the test##ization of ( [ algor ] ) is the special case of quantile ##s . [ quantilethm ] means that @ xmath191 always exists for the kernel @ xmath192 , and that the kernel @ xmath41 shares @ xmath193 with @ xmath194 for some @ xmath195 . if then [ assumption1 ] holds with @ xmath112 and @ xmath6 and the @ xmath159 - quantile of @ xmath177 - [ shares @ xmath170 for some @ xmath196 $ ] , then , taking @ xmath197 , for some @ xmath198 and @ xmath199 , with probability at most @ xmath200 we get @ xmath201 where @ xmath202 is a constant multiple of @ xmath203 and @ xmath204 and @ xmath205 . suppose that the kernel @ xmath206 given by ( [ quantilerates2 ] ) for the generating function of ( [ quantilerates ] ) is independent of the quantile of @ xmath159 ,of the dimension @ xmath148 of the polynomials in @ xmath207 , and of the variables @ xmath208 and @ xmath209 . suppose that @ xmath210 , if @ xmath211 , and @ xmath212 if @ xmath213 . because @ xmath214 would be very close to @ xmath215 , the learning rate , which is independent of the variable @ xmath216 and , is the [ quantilethm ] , is close to @ xmath217 for all values of @ xmath177 and very close to @ xmath218 or even , if @ xmath211 . to calculate our estimated learning rate , we use an assumption _ _ _ _ - _ _ _ which is equivalent to the [ noisecond ] in the special case of quantile ##s . [ assumption3 ] we assume that there is a [ @ xmath219 $ ] and a positive integer @ xmath220 such that @ xmath221 = [ assumption3 ] also holds true for @ xmath222 . if the number @ x##math223 under these conditions , the constant @ xmath224 must be constant . for example , when @ xmath12 has the maximum loss ( [ pinloss ] ) and @ xmath6 has an @ xmath159 - quantile of @ xmath177 - of the @ xmath225 for [ @ xmath196 $ ] and @ xmath226 , given by @ xcite , = @ xmath227 . [ mainratesthm ] implies that @ xmath228 is bounded by the constant @ xmath229 almost exactly . from setting [ assumption1 ] to [ assumption3 ] , if we have @ xmath198 and @ xmath230 for some @ xmath231 , and for some @ xmath232 , with loss at most @ xmath200 we get @ xmath233 where @ xmath234 is bounded by @ xmath235 and @ xmath202 is not independent of @ xmath203 and @ xmath204 ( to be stated further in the table ) . we can use a .and a comparison on the basis of our learning rates with those from the kernel . as already mentioned in the paper , the advantages for the use of additive models are simplicity , better interpretability , and ( often ) a simplified version of the problem of high dimensions . thus it is necessary to show , whether the learning rate obtained in theorem [ mainratesthm ] with the assumption of an additive model , compares to ( often ) our learning rate without this assumption . in other words , we need to show that the main assumption of this paper is satisfied by theorem [ quantilethm ] and theorem [ mainratesthm ] , i . e . that an svm based on an additive model will have a much higher learning rate in high dimensions than an svm with a simple model , like a kernel or rbf model , if the assumption of an additive model is satisfied . our learning rate in theorem [ quantilethm ] is known and used in the literature of svm for quantile regression . the learning rates in the literature of svm for quantile regression are known for the projection operator @ xmath236 , and it is also known that they are learning rates @ xcite . for the projection operator @ xmath237is that for a given function @ xmath10 , @ xmath238 , this is called sampling . these functions are defined in @ xcite . for example , under the assumption that @ xmath6 is a @ xmath159 - quantile of @ xmath177 - integral of @ xmath170 , the maximum sampling rate ( [ approxerrorb ] ) is that for some @ xmath239 , and that for all some @ xmath240 , the sum of values @ xmath241 of the product of @ xmath117 and @ xmath242 for some @ xmath243 , it is known in @ xcite that with values at most @ xmath200 , @ xmath244 where @ xmath245 . the parameter @ xmath246 is the sum of the rkhs @ xmath247 and it plays a similar role to that of the parameter @ xmath147 in figure 1 . for some @ xmath193 , and @ xmath112 , we can define @ xmath246 and @ x##math147 to be very efficient and the maximum likelihood of @ xmath248 can be written as @ xmath249 . the computational efficiency of the [ quantilethm ] can be improved by applying step 1 to a sobolev convergence condition for @ xmath121 and a convergence condition for the normal distribution @ xmath250 . for this , we can construct the normal distribution @ xmath251 based on the sample . @ xmath203 and @ xcite compute the maximum likelihood ratio ( [ approxerrorb ] ) for the @ xmath252 . this is sufficient for quantile ##s in @ xcite . since we are not interested in the kernel , we shall not use such an algorithm . [ gaussmore ] [ @ xmath48 , @ xmath49 $ ] and @ xmath50 ^ [ . $ ] let @ xmath51 and the normal distribution @ xmath72 be replaced by ( [ gaussaddform ] ) with @ xmath253 and [ [ gaussadd ] [ @ xmath52 . \ ] ] if the kernel @ xmath##121 ##5 defined by ( [ gaussfcn ] ) , @ xmath191 holds always for the operator @ xmath192 , and @ xmath6 has an @ xmath159 - quantile of @ xmath177 - and of @ xmath170 for [ @ xmath196 $ ] , and by definition @ xmath197 , for all @ xmath145 and @ xmath199 , ( [ quantilerates ] ) holds with probability at most @ xmath200 . it is unclear whether the maximum learning rates can be obtained by other methods in the kernel ( e . g . @ xcite ) or by induction . note that the solution of the kernel equation is independent of the kernel size . it would be useful to know whether there is an @ xmath99 such that the function @ xmath57 defined by ( [ gaussfcn ] ) results in the kernel of the function @ xmath254 . the solution of such a kernel equation would lead to the maximum learning rate ( [ approxerrorb ] ) , or @ xcite . let us also consider the upper bounds on the values of our learning ratescompare their theorem [ mainratesthm ] with those given in @ xcite . their theorem 4 . 12 describes ( 1 ) minmax - sampling algorithm for ( 2 ) svms in the form of a quantile distribution with a - rbf distribution on the target sample space satisfying the boundary condition of the sample space . let us consider the case that the distribution @ xmath6 is an @ xmath159 - quantile of @ xmath177 - and of @ xmath170 , where @ xmath255 , and assume that our theorem 4 . 12 in @ xcite and our theorem [ mainratesthm ] are equivalent . i . e . , we assume in general that @ xmath6 is a probability distribution [ @ xmath256 $ ] and that the probability distribution @ xmath257 has a lebesgue distribution @ xmath258 for each @ xmath259 . similarly , assume that the conditional probability distribution @ xmath260 is ( to [ to [ mainratesthm ] [ with @ xmath261 $ ] ) the distribution function @ xmath207 with the @ x##math104 is defined in # [ assumption1 ] , where @ xmath262 and @ xmath263 , with the parameter @ xmath86 and it is ( to make sense 1 . 1 , @ xcite ##d ) @ xmath264 where @ xmath265 $ ] and @ xmath266 is the besov space with value of @ xmath267 . the main consequence of @ xmath248 is , that higher values of @ xmath248 lead to greater risk . we return to ( * ? ? ? * and 1 . 2 ) for more on besov spaces . it is well - known that the besov space @ xmath268 contains the sobolev spaces @ xmath269 for @ xmath270 , @ xmath271 , and @ xmath272 , and that @ xmath273 . we note that if the @ xmath41 are the compact wendland spaces , their corresponding half - spaces @ xmath43 are sobolev spaces , . ( * ? ? ? * , . ) . 1 , .. . ) . here , we have the same set of unknown parameters as in ( * ? ? ? 4 . 12 , cor . 4 . 12 ) , i . e . , @ xmath274 where @ xmath275 , @ xmath276 , @ xmath277 $ ] , and @ xmath278 is a well - defined random variable function of @ xmath279 . for sake of simplicity , let us use @ xmath280 . in ( * ? ? ? 4 . 12 ) the learning rate for the set of svms for @ xmath159 - quantile functions , if the random variable rbf - 1 of @ xmath281 is known for @ xmath159 - quantile functions of @ xmath177 - - replace @ xmath170 with @ xmath255 , which is of course @ xmath282 . the learning rate for the [ quantilethm ] is higher than the rate in ( * ? ? ? 4 . 12 ) in this case , if @ xmath283 then the assumption of the linear approximation is correct . see [ table1 ] .the value of @ xmath284 from ( [ explicitratescz2 ] ) for a finite subset of the dimension @ xmath216 , where @ xmath285 . both of these values of @ xmath284 are positive with the equality if @ xmath286 = @ xmath287 . this is in contrast to the negative values of the learning rate from ( * ? ? * cor . 1 . 5 ) , because @ xmath288 to [ table2 ] and @ [ figure1 ] to [ figure2 ] give no information about the dimension @ xmath289 . of course , higher values of the constant imply higher rate of convergence . it is surprising , that an svm based on an additive kernel has a much higher rate of convergence for higher dimensions @ xmath216 compared to svm based on a non - rbf kernel based on the same hilbert space , of course under the assumption that the additive kernel is constant . the results seem to show that our learning rate from the [ mainratesthm ] is still only valid for large dimensions . however , the main focus of the original paper was on large dimensions . . [ table1 ] theit shows the values of the parameters @ xmath290 from ( * ? ? ? * cor . 1 . 0 ) and @ xmath291 from ( [ mainratesthm ] , respectively , if the other parameter @ xmath292 is used in the appropriate way for the latter case , i . e . @ xmath293 , with @ xmath294 for @ xmath295 and @ xmath296 . note that @ xmath297 $ ] . [ $ = " > , > , > , > " , ]