the original data presented in this article was collected from the forward looking radar of the us naval research laboratory @ xcite . that radar was used for detection and the identification of highly collision - prone targets . since targets are three dimensional objects , one has to measure a three dimensional curve about each target . however , the radar measures only a time domain curve for each target , see figure 1 . therefore , we can expect to have only a very little information about each target . therefore , we have only an estimate of the mass constant of each target . for each target , our estimate only is a sort of weighted average of that of the uniformly distributed dielectric constant . but even this estimate can be potentially very useful for classification . however , if the classification system is based solely on the energy information of radar waves , see , e . g . @ xcite . estimates of the constant of targets , if taken together , do not improve the current false alarm rate . however , these estimates can be easily used as an additional piece of information . when combined with the already available energy information , this piece of the information could result in the system with better classification algorithms , which might improve the current false alarm rate . the interference - scattering theory ( im##sp ) is sometimes also called a nonlinear estimation problem ( cip ) . imsps / cips are often ill - defined and highly nonlinear . therefore , an important question to consider in a comprehensive study of such a problem is : _ how to reach a sufficiently large neighborhood of the unknown coefficient without an efficient solution of this problem ? _ the size of this neighborhood should depend both on the degree of variation of the data and on the model . we develop a new method , which is a nonlinear method of reaching this neighborhood , _ global optimal method _ ( gcm ) . in this paper we develop also a nonlinear ##ly ##c method for the one - dimensional multiple - choice problem ( imsp ) with the data taken from the experiments . in addition to the above method , we develop this method by considering both the efficient and the highly nonlinear experimental data . finally , we develop a nonlinear integro - differential equation for which the unknown coefficient is not known . _ solution _ of this equation is the method of the solution of this problem . this method is based on the definition of a nonlinear least squares linear functional . the starting point of this method is the inclusion of the carleman constant function ( cwf ) in it . this is the solution , which isinvolved is the carleman constant for the optimal cost equation . we prove that , for a closed ball of an arbitrary dimension @ xmath1 with the dimension of @ xmath2 in the underlying metric space , we can define the function @ xmath3 of the cwf in such a way that this function is strictly convex on that ball . the existence of the optimal minimizer on that closed ball as well as convergence of minimizers to the exact solution when the level of noise in the data goes to zero is known . in particular , it is known that the gradient like method has a sufficiently small neighborhood of the maximum value if the starting point is an isolated neighborhood of that ball . the size of that neighborhood is proportional to the level of noise in the data . however , since constraints on @ xmath4 are not satisfied by our method , then this is a _ _ converge _ like method . we prove that in the special case of a strictly convex cost function the gradient - like method converge to the exact solution only if the starting point is located in a sufficiently small neighborhood of this ball : this is due to the properties of the # ##ization and convergence of cost functionals . the best known gradient - like methods of the gradient -for cips ( see this section below ) , the numerical formulation for the problem of the first paper does not impose a certain bound on the values @ xmath5 of the values of the objective function @ xmath6 . the majority of the modern numerical analysis of many of these ill - posed problems use the linear method . in other words , the least squares cost function is defined for the problem , see , e . g . chavent , engl , gonch1 , gonch2 . however , the general problem with these functionals is that they are generally not convex . paper 1 of the three papers gives a simple description of the linear methods and existence of non - convex least squares cost functionals for the cips . however , convergence of the general problem of solving a problem to the optimal solution can be achieved only if a numerical approximation for that solution is known in advance . however , such an approximation is not available for optimization . this allows the development of fast numerical numerical methods for cips , see , e . g . @ xcite . the same group with others has developed the methods of gcm for cips with continuous time data . the gcm of the first paper is used to estimate the objective function` ` ` . this method was started from the paper @ xcite and has been developed since then , see , e . g . @ xcite and others given below . in this method , in each step of the estimation process one asks the dirichlet boundary value problem for a strictly positive real pde , which depends on that first step . the solution of this pde allows one to find the fourier transform , and then to find a wave function , which is called the kernel of ' ' . the initial conditions for this method are the minimum condition on the sum of the values of either the parameter @ xmath7 of the fourier transform of the solution of the wave function or of the the parameter @ xmath8 of the fourier transform . note that the result of this method does not satisfy the second condition . in this case we have a special case of the gcm of the second type . for the case of the gcm of the third type the minimum loss function with the cwf in it is defined . the same properties of the global maximum cost and the global stability of the gradient descent method hold for the one described above . the gcm of the third type was introduced by klib95, klib97 , along with the recently published survey of @ xcite . the development of the version of the gcm of the first type has its roots in the development of @ xcite , which is based on carleman method and which was originally developed in @ xcite , for proof of theorem . for cips , we see the latest development of @ xcite . a version of the gcm with the cwf . this was originally developed in bau1 for the cip for the differential equation @ xmath9 where @ xmath10 is the fundamental solution . this gcm is solved also in @ xcite . in bau1 , bau2 non - vanishing conditions are imposed : it is assumed that is @ xmath11 , @ xmath12 or @ xmath13 in the particular case of interest . these conditions are imposed in @ xcite for the gcm of the second type . on the other hand , we have in the general case , just as in @ xcite , the fundamental solutions of the first pde . the differences between the fundamental solutions of those pdes and those with non - vanishing conditions are the same differences in klib95 , klib9##7 , 8 , ktsiap and @ xcite of different definitions of the gcm of the first type . later , the concept of the gcm of the second type was applied to the study of well - posed cauchy problems for quasilinear pdes , in the work of klquasi and to analytical and numerical methods of bakklkosh , klkosh . cips of the ##lets are a part of a larger subfield , independent scattering methods ( isps ) . isps are a large part of the scientific community . in this section we refer to the direct methods which can compute shapes , sizes and shapes of scatterers without using @ xcite . we also refer to @ xcite for some other isps in the same domain . in addition , we mention some other direct methods for isps found in @ xcite . as to the cips with direct methods , i . e . the dirichlet - to - kernel scattering method , we cite the articles @ xcite and others found here , where new methods are proposed , which do not require a priori knowledge of a small part of the exact solution . in section 2 we mention our own problem . in section 3 we state that the is2 . in section 4 we prove the convergence theorem of this function : a very high bound . in section 5 we prove the uniform ##ity of the gradient descent method of the solution of this function . although this method is not an exact method ( sections 4 - 5 ) , we continue the proof with it . in section 6 we test our method on simulated hard data . in section 7 we test it on real data . the results are in section 8 . let the constant @ xmath14 be the first independent coupling constant of the system . we assume that @ xmath15 @ xmath16fix the constant is @ xmath17 for simplicity , we do not look at any of our results as @ xmath18 . the 2 - dimensional differential equation for the case @ xmath19 , @ xmath20 @ xmath21let @ xmath22 is the solution of the equation ( [ 1 . 5 ] ) , ( [ 2 . 5 ] ) for the case @ xmath23 , @ xmath24our ##ce ##p . the * scattering problem : * the international scattering problem ( imsp ) * . _ _ _ @ xmath25\ , \ left ( \ , \ infty \ , ) $ ] _ _ find the set of wavenumbers _ _ @ xmath26 _ _ . find the interval _ _ @ xmath27 _ _ so that the optimal solution _ _ @ xmath28 _ _ is [ _ _ @ xmath29 . \ label { 2 . 1 } \ ] ] . @ xmath30it , from ( [ 2 . 0 ] ) , ( [ 2 . 1 ] ) and @ xcite that @ xmath31 , \ label { 2 . 1 } \ ] ] @ xmath32 . \ label { 2 . 1 } \ ] ] in this section we will present the results of @ xcite , which we used extensively in this section . existence and stability of the solution @ xmath33 for ##all @ xmath8 was proven in @ xcite . also , it was proven in @ xcite that @ xmath34 , \ forall - > [ . \ label { 2 . 1 } \ ] ] in particular , @ xmath35 . $ ] in particular , all of our imsp was proven in klibloc . also , thethe following definition of the function @ xmath36 takes place : @ xmath37 \ left ( \ + % \ widehat { \ } \ left ( \ , y \ right ) \ right ) , [ \ rightarrow \ infty , \ forall [ \ , % \ , [ \ , k \ right ] , \ , { 0 . 60 } \ ] ] @ xmath38 using ( [ 2 . 100 ] ) and ( [ 2 . 10 ] ) we then can also define the function @ xmath39 as function @ xcite . the difficulty here is in defining @ xmath40 since this function is only defined up to the value of @ xmath41 where @ xmath42 is an integer . for sufficiently small values of @ xmath26 we define the function @ xmath39 using ( [ 2 . 0 ] ) , ( [ 2 . 100 ] ) , ( [ 2 . 9 ] ) and ( [ 2 . 10 ] ) . @ xmath43where @ xmath44hence , for sufficiently large @ xmath26 , @ xmath45which removes the previously mentioned function . note that the function@ xmath46 is so true that ( [ 2 . 11 ] ) is true for @ xmath47 . @ xmath48 is true only for ( [ 2 . 11 ] ) . so to avoid forgetting properties of @ xmath26 , we write the function ( [ 2 . 9 ] ) @ xmath49 = @ xmath50by ( [ 2 . 11 ] ) @ xmath51 , \ forall \ < > [ . $ ] on both sides of ( [ 2 . 11 ] ) with respect to @ xmath26 , we obtain @ xmath52multiplying both sides of ( [ 2 . 11 ] ) and @ xmath53 , we obtain @ xmath54 however , there is a function @ xmath55 instead of @ xmath26 such that @ xmath56setting to ( [ 2 . 13 ] ) @ xmath57 and using the fact that for ( 2 . 13 ) @ xmath58 , we obtain @ xmath59 . \ [ { 2 . 13 } \ ] ] thus , ( [ 2 . 13 ] ) and ( [ 2 . 15 ] ) assume that @ xmath60* = _ @ xmath61 in this case we get the above - mentioned linear combination with the cwf in mind . * = 1 . 5 * ( carleman estimate ) . _ for any real valued function _ @ xmath62 _ _ with _ _ @ xmath63 _ _ and for any constant _ _ @ xmath64 _ _ the [ carleman estimate is _ _ @ xmath65 , \ label { 3 . 00 } \ ] ] _ _ where the function _ _ @ xmath66 _ _ _ _ product of _ @ xmath67 _ _ and _ _ @ xmath68 * _ * . in the case when the integral with @ xmath69 is calculated from the right hand side of ( [ 3 . 00 ] ) this result was proved by klibloc . to compute this integral , we need that @ xmath70 . \ label { 3 . 02 } \ ] ] for @ xmath71 let ( [ 3 . 02 ] ) let ( [ 3 . 00 ] ) where @ xmath72 is identified with @ xmath73 @ xmath74 for @ xmath##75 , [ \ , \ lbrack \ , { k } , \ overline { k } ] $ ] for the function @ xmath76 and the @ xmath77derivative @ xmath78 , where @ xmath79 for , @ xmath80consider the function @ xmath81 , which we call the function " " , and this function is called , @ xmath82 for @ xmath83 implies that since @ xmath84 for @ xmath85 for , ( [ 2 . 100 ] ) and the first argument ( [ 2 . 100 ] ) implies that @ xmath86 for @ xmath87 for , ( [ 2 . 6 ] ) and ( [ 2 . 100 ] ) implies that @ xmath88 for @ xmath87 it follows from ( [ 2 . 6 ] ) , ( [ 2 . 6 ] ) , ( [ 2 . 6 ] ) ( [ 2 . 100 ] ) , ( [ 2 . 150 ] ) and ( [ 2 . 150 ] ) that @ xmath89 @ xmath90using ( [ 2 . 160 ] ) ,( [ 3 . 5 ] ) , ( [ 3 . 1 ] ) and ( [ 3 . 1 ] ) , we obtain @ xmath91differentiate ( [ 3 . 3 ] ) with respect to @ xmath26 and ( ( [ 3 . 3 ] ) - ( [ 3 . 5 ] ) . we obtain @ xmath92 @ xmath93 @ xmath94where @ xmath95 , [ \ n % \ n [ \ begin { k } , \ overline { k } \ in ] . \ end { 3 . 5 } \ ] ] we have solved the integro - linear equation ( [ 3 . 6 ] ) for the function @ xmath96 with the overdetermined boundary condition ( [ 3 . 7 ] ) . the tail function @ xmath97 is an example . first , we will find the tail function @ xmath98 . then , we will solve the equation ( [ 3 . 6 ] ) , ( [ 3 . 7 ] ) for the function @ xmath96 . to solve this problem , we will approximate the above - mentioned linear equation with the cwf @ xmat##h99 for example , see ( [ 1 . 00 ] ) . this result , together with our numerical results , gives the _ different _ solution of our problem . however , even though the problem ( [ 3 . 6 ] ) - ( [ 3 . 7 ] ) is the same as the problems ( 6 ) , ( 66 ) in @ xcite , the numerical result of the solution of the problem ( [ 3 . 7 ] ) - ( [ 3 . 6 ] ) is _ different _ different from the result in @ xcite . first , suppose that we have known coefficients for the functions @ xmath100 and @ xmath78 . then we obtain the unknown function @ xmath101 via two methods . first , we compute the coefficient for the function @ xmath102 via ( 3 . 1 ) and ( [ 3 . 2 ] ) . second , we calculate the coefficient @ xmath103 via ( [ 3 . 3 ] ) . we have seen from our numerical results that the best approximation of @ xmath26 to be in ( [ 3 . 6 ] ) for the second calculation of @ xmath104 the coefficient for the unknown function is obtained in the same way as the coefficient forthe so - called tail " function " is version 1 . 1 of @ xcite . however , since all functions are defined via @ xcite , we are not doing any work here . it follows from ( [ 1 . 0 ] ) - ( [ 2 . 0 ] ) and ( [ 3 . 0 ] ) - ( [ 3 . 1 ] ) that there exists a [ @ xmath105 $ ] such that @ xmath106hence , so that the value @ xmath107 is sufficiently large , we insert both @ xmath108 and @ xmath109 in ( [ 3 . 6 ] ) . next , we insert @ xmath110set @ xmath111 in ( [ 3 . 6 ] ) and ( [ 3 . 7 ] ) . next , insert ( 3 . 7 ) in ( [ 3 . 6 ] ) and ( [ 3 . 8 ] ) in @ xmath57 . we insert @ xmath112 so that both @ xmath113 and @ xmath114 are defined via ( [ 3 . 9 ] ) . next , @ xmath115where insert @ xmath113 and @ x##math116 is found in ( [ 2 . 100 ] ) and ( 2 . 101 ) respectively . it seems to us at the first step that we can approximate the solution @ xmath98 ##1 , for the cauchy equation for equation ( [ 3 . 9 ] ) with functions @ xmath117 and @ xmath118 however , it is observed in section 1 . 1 of @ xcite that this method , when applied to a nonlinear problem , does not lead to good results . we have the same problem in our own solution . this is due to the nonlinear ##ity of ( [ 3 . 9 ] ) . so , just as in @ xcite , we solve the equation ( [ 3 . 9 ] ) , ( [ 3 . 9 ] ) using the quasi - newton method ( qrm ) . the boundary of @ xmath119 has a good stability property . first , we find the unknown function @ xmath120 in the solution @ xmath121 , where @ xmath122 @ xmath123where @ xmath124 is the unknown function . the stability and consistency of the solution of this optimization problem works well .convergence of minimizers @ xmath125 from the @ xmath126norm to the exact solution @ xmath127 of the equation ( [ 3 . 11 ] ) , ( 3 . 12 ) with the exact data @ xmath128 and @ xmath129 was proved by @ xcite . we note that in the spectral sequence one always finds convergence of an approximate exact solution with exact data @ xcite . note that in the [ [ @ xmath130 $ ] and @ xmath131 } \ leq c \ left \ vert c \ right \ vert _ { h ^ { 2 } \ left ( 0 , 1 \ right ) } , \ forall c \ { h ^ { 2 } \ left ( 0 , 1 \ right ) , \ left { 1 . 0 } \ ] ] where @ xmath66 is the convergence of @ xmath132 . 4 . 1 is a reformulation of step 4 . 1 of @ xcite . * _ 4 . 1 . * _ is the function _ @ xmath133 _ _ . let ( [ 2 . 1 ] ) - ( [ 2 . 2 ] ) be thethe solution of our imsp with the function [ _ _ $ ] _ _ , where _ _ @ xmath135 _ _ and _ _ @ xmath136 _ _ are the solutions of the above problem ( [ 1 . 4 ] ) , ( [ 2 . 5 ] ) . let the continuous linear function _ _ @ xmath137 _ _ and the function _ _ @ xmath138 _ _ take the form ( [ 3 . 11 ] ) with _ _ @ xmath139 _ _ such that for _ _ $ ] _ _ _ _ @ xmath141 _ _ where _ _ @ xmath142 _ _ is a very large function , which is the magnitude of the noise in the input signal . defined in ( [ 3 . 11 ] ) _ _ @ xmath143 _ _ let the function _ _ @ xmath144 _ _ be the minimizer of the size ( [ 3 . 11 ] ) of the set of functions _ _ @ xmath121 _ _ defined in ( [ 3 . 12 ] ) . then there exists a function _ _ @ xmath145 _ _ defined only on _ _ @ xmath##107 _ _ and _ _ @ xmath146 _ _ note that _ _ @ xmath147 } \ leq x \ left \ vert x _ { \ delta \ left ( \ delta \ right ) } \ left ( 1 \ right ) - x ^ { \ ast } \ left ( 0 , % \ overline { 1 } \ right ) \ left \ vert _ { v ^ { k } \ left ( 0 , 1 \ right ) } \ leq x _ { k } \ right . \ left { 3 . 8 } \ ] ] * = 0 . 15 * . we have also tried to find two solutions in the taylor expansion for @ xmath98 , ( [ 3 . 8 ] ) : the first is with @ xmath148 this results in a linear combination of two equations . we have solved this equation via taking the value of the coefficients of at 3 . 8 . however , the quality of the images deteriorated when compared with the above equation @ xmath149 in particular , we have tried to improve with respect to the above equation @ xmath98 . however , the quality of resulting images has also improved . since the above@ xmath78 = ( [ 3 . 7 ] ) - ( 5 . 2 ) . in sections 5 . 2 and 5 . 3 we use section 1 . 1 and section 2 . 2 of bakklkosh . to prove this , we need to find # ##ity . in @ xmath150 . , we use the sequence @ xmath151 @ xmath152denote @ xmath153also , and replace ( [ 3 . 6 ] ) @ xmath98 with @ xmath154 satisfying ( [ 3 . 6 ] ) , ( [ 3 . 7 ] ) and ( [ 3 . 17 ] ) and ( [ 3 . 17 ] ) so that @ xmath155 @ xmath156 @ xmath157 is the hilbert space @ xmath158 of space of real valued functions @ xmath159 @ xmath160 [ @ xmath161 ^ { 1 / 2 } < \ infty % \ begin { \ } % \ } \ } . \ end { 3 . 17 } \ ] ] , and then @ xmath162 \ to ( [ 3 . 17 ] )and ( [ 3 . 16 ] ) , we define our minimum value function as @ xmath163let @ xmath1 is an imaginary number . let @ xmath164 be the point on the left of the point @ xmath158 of the convex set @ xmath165 of * @ xmath166 . * @ xmath167 * the norm * . _ is the norm _ @ xmath168 _ _ of the function _ _ @ xmath169 * . 2 . 3 * . the first section of this article , is dedicated to this particular problem . since we work with real valued functions , we use * @ xmath170 as the norm with respect to the n - dimensional space of real valued functions @ xmath171 . , even though we the use real numbers only , this is useful only for the purpose of optimization . [ @ xmath172 $ ] is the inner norm of @ xmath158 . even though we use both ( [ 3 . 16 ] ) and ( [ 3 . 18 ] ) the norm @ xmath173 @ xmath174 $ is the* from the above , what do we really know about this particular case : the integral ##s of @ xmath175 of the above functions @ xmath166 and the generic functions @ xmath176 theorem 4 . 1 is the main mathematical result of this section . * theorem 4 . 1 * . _ assume that conditions of theorem 4 . 1 are satisfied . * the functional _ @ xmath170 _ _ has the frecht property _ _ @ xmath178 _ _ for all _ _ @ xmath179 _ _ , there is a very large [ _ @ xmath180 } , r \ right ) > 1 $ ] _ dependent only on the parameters and the generic function _ @ xmath66 _ _ , such that for all _ _ @ xmath181 _ _ the functional _ _ @ xmath170 _ _ is not dependent on _ _ @ xmath182 _ _ i . e . for all _ _ @ xmath183 _ _ _ _ @ xmath184 * . . * [ ( in this case @ xmath185 } , r \ right ) > 1 $ ] _ generic functions depending only onwith and . since conditions of equation 2 . 2 are satisfied , using [ ( [ 3 . 12 ] ) @ xmath186 } \ leq \ delta \ vert c ^ { \ ast } \ left \ vert _ { c ^ { 2 } \ { [ 0 , 1 \ } ] } + c _ { 2 } \ delta \ leq c _ { 2 } . \ label { 3 . 220 } \ ] ] and @ xmath187 where @ xmath188 = ( [ 3 . 12 ] ) , ( [ 3 . 16 ] ) and ( [ 3 . 22 ] ) note that @ xmath189 } ^ { 2 } dk \ leq c _ { 2 } . \ label { 3 . 220 } \ ] ] and ( [ 3 . 22 ] ) , we write @ xmath190 @ xmath191 } ^ { 2 } dk \ leq c _ { 2 } . \ ] ] we obtain the function @ xmath192where @ xmath193 is the complex ##ification of @ xmath194 . then @ xmath195consider = @ xmath19##6 . as @ xmath197first , with ( [ 3 . 261 ] ) and ( [ 3 . 25 ] ) , we work out as @ xmath198 the vector , which is defined with respect to the origin . @ xmath199 . 0 @ xmath200 @ xmath201 @ xmath202by ( [ 3 . 261 ] ) @ xmath203 h ^ { \ tau } \ { \ } \ overline { k } \ ] ] @ xmath204 \ int \ limits _ { k } ^ { \ overline { k } } h ^ { \ prime } \ left ( p , \ tau \ right ) } \ right \ cdot \ overline { a } \ { { 3 . 261 } \ ] ] @ xmath205 \ overline { l } . \ ] ] } , @ xmath206 \ overline { l \ left ( p \ right ) } % h ^ { \ prime } \ ] ] @ xmath207 \ overline { l \ left ( p \ right ) } \ int \ limits _ { k } ^ { \ overline { k } } h ^ {\ tau } \ left ( \ , \ , \ right ) - \ , \ , { 1 . 1 } \ ] ] @ xmath208where @ xmath209 depends nonlinearly on the term in @ xmath210 . so , using ( [ 3 . 220 ] ) - ( [ 3 . 260 ] ) and the cauchy - schwarz inequality @ xmath211to calculate the value of the term 1 / 2 " in @ xmath212 " ( 3 . 260 ) , we find that it follows from ( [ 3 . 260 ] ) that the term @ xmath213 = ( [ 3 . 261 ] ) , the term @ xmath214 which is not in ( [ 3 . 260 ] ) - , as well as the @ xmath215we ##b , how do we estimate the last term in ( [ 3 . 280 ] ) , since all of the other terms are unknown . we compute the so - called cauchy - schwarz inequality with @ xmath216 @ xmath217where @ xmath218 and the following inequality with @ xmath219 = , @ xmath##220thus , using the term @ xmath221 we have the term @ xmath222 = ( 3 . 27 ) . the next term on the right hand side of ( [ 3 . 27 ] ) is obtained by . now , from ( [ 3 . 250 ] ) - ( [ 3 . 27 ] ) , we have @ xmath223 h ^ { \ prime } \ tau \ } } \ cdot l \ left ( p \ right ) \ ] ] @ xmath224 \ int \ int _ { k } ^ { \ overline { 1 } } h ^ { \ prime } \ left ( p , \ tau \ } ) { \ } } \ cdot l \ left ( p \ right ) \ { { 3 . 27 } \ ] ] @ xmath225where @ xmath226 acts nonlinearly on the vector function @ xmath227 and starting with ( [ 3 . 25 ] ) @ xmath228 it is clear from ( [ 3 . 25 ] ) , ( [ 3 . 27 ] ) - ( [ 3 . 29 ] ) that the derivative with respect to the vector function @ xmath227 is of @xmath229 consists of the intersection of the first two lines of ( [ 3 . 20 ] ) with the last two lines of ( [ 3 . 25 ] ) . we denote this linear functional by @ xmath230 , @ xmath231thus , from ( [ 3 . 20 ] ) and ( [ 3 . 25 ] ) , we obtain @ xmath232 @ xmath233consider the expression @ xmath234 @ xmath235it follows from ( [ 3 . 20 ] ) , ( [ 3 . 220 ] ) , ( [ 3 . 27 ] ) and ( [ 3 . 28 ] ) that @ xmath236 is the unique linear functional . therefore , by riesz theorem , there is an expression @ xmath237 such that @ xmath238 , \ forall [ \ { 1 . \ _ { 1 . 0 } \ ] ] it follows from ( [ 3 . 28 ] ) and ( [ 3 . 29 ] ) - ( [ 3 . 31 ] ) that @ xmath239 = 1 \ left ( \ left \ vert ) \ right \ vert _ { 1 } ^ { 2 } \ right) . \ ] ] therefore , the frecht constant @ xmath240 of the function @ xmath170 at the point @ xmath241 , and @ xmath242 such that @ xmath243 . \ { { 3 . 3 } \ ] ] and , using ( [ 3 . 35 ] ) , ( [ 3 . 35 ] ) - ( [ 3 . 36 ] ) and [ 3 . 3 , we obtain @ xmath244 @ xmath245 @ xmath246 @ xmath247 \ ] ] @ xmath248choose the { @ xmath249 } , ( \ } ) > [ $ ] . suppose that @ xmath250 and , using ( [ 3 . 3 ] ) and ( [ 3 . 35 ] ) , we obtain with the following continuous function @ xmath66 for : @ xmath181 @ xmath251 by step 2 . 2 , we discuss in this section the upper limits of the gradient descent and of the convergence of the function @ xmath252 similarly to the other cases of the gradient descent , they will be discussed in follow sections .. . , we want to prove the lipschitz continuity of the functional @ xmath254 with respect to @ xmath241 . * theorem 3 . 1 * . _ the results of theorem 3 . 1 * . * the function _ @ xmath178 _ _ is lipschitz continuous on the convex set _ _ @ xmath169 _ _ in other words , _ _ @ xmath255 * . * . therefore , for using the real part of ( [ 3 . 17 ] ) for @ xmath256 and using for @ xmath257 we obtain @ xmath258 * . * these functions are continuous with respect to @ xmath259 . @ xmath260 we have @ xmath261 @ xmath262 \ { { 5 . 2 } \ ] ] @ xmath263 ##e ^ { \ } } . \ ] ] it is clear from ( [ 3 . 17 ] ) that @ xmath264 and , from ( [ 3 . 17 ] ) , ( [ 5 . 2 ] ) and cauchy - schwarz inequality , we have @ xmath265 @ xmath266theproof of the proof of ( [ 4 . 1 ] ) is given . @ xmath74 theorem 4 . 1 * the existence and existence of the minimizer of the function @ xmath170 on the set @ xmath268 * theorem 5 . 2 * . _ the statement of theorem 4 . 1 * . * for every _ _ there is a minimizer _ _ @ xmath269 _ _ of the function _ _ @ xmath170 _ _ on the set _ _ @ xmath169 _ _ \ , _ _ @ xmath270 \ geq \ , \ forall a \ , \ overline { b \ left ( a \ right ) } . \ end { 2 . 1 } \ ] ] * * * . this theorem follows directly from the statement of 4 . 1 and theorem 4 . 2 of @ xcite . @ xmath74 let @ xmath271 be the graph of the projection of the function @ xmath158 on the unit circle @ xmath272 = @ xmath273 and let @ xmath274 be an open subset of @ xmath164 . then the projection ofthe maximum likelihood is , @ xmath275 * proof * . 4 . * _ the statement of theorem 4 . 2 * . * for every _ @ xmath181 _ _ there exists a sufficiently large sequence _ _ @ xmath276 } , \ left \ vert _ _ { k } \ right \ vert _ { ( \ left [ \ dot { k } , \ overline { k } \ right ] } , r , \ left \ right ) \ left \ left ( r , r \ right ) $ ] _ and a _ _ @ xmath277 _ _ such that for every _ _ @ xmath278 _ _ the sequence ( [ 5 . 4 ] ) corresponds to the gradient minimizer _ _ @ xmath279 _ _ of the sequence _ _ @ xmath280 _ _ of the sequences _ _ @ xmath281 _ _ and _ _ @ xmath282 * proof * . this proof follows directly from the statements theorem 1 . 1 and theorem 1 . 2 of @ xcite . @ xmath74 , it is pointed out in theorem 4 . 2 , is one of the first examples of the proof .@ xcite , we assume the equality of the exact data @ xmath133 of our imsp with the exact , i . e . known , data @ xmath283 in ( [ 3 . 7 ] ) . " the function @ xmath284 " denotes the given by @ xmath285 the definition of the function @ xmath142 is used in our example in ( [ 3 . 6 ] ) . in particular , it follows from ( 3 . 6 ) , ( [ 3 . 7 ] ) and ( [ 3 . 14 ] ) that @ xmath286 } , \ left \ vert p _ { k } - p _ { k } ^ { \ ast } \ right \ vert _ { k \ left [ \ dot { k } , % \ overline { k } \ delta ] } \ leq p _ { k } \ right , \ dot { 1 . 0 } \ ] ] where the function @ xmath287 depends only on the data . finally , in this case we assume that the gradient descent method finds values within a small range of the function @ xmath288 and , therefore , of the function @ xmath##289 the area of this function is equal to @ xmath290 it is easy to see in this section that of the function @ xmath291 from @ xmath292 and @ xmath293 . we have in this section @ xmath294 * and 5 . 4 * . _ note that statements of theorem 4 . 2 hold . first , consider the exact function _ @ xmath295 _ _ . the following two statements hold for : _ _ @ xmath181 @ xmath296 @ xmath297 _ _ where _ _ @ xmath279 _ _ is the minimizer of the function _ _ @ xmath298 _ _ , which is given by theorem 4 . 2 and _ _ @ xmath299 _ _ is the corresponding exact function ( theorem 5 . 4 ) . in particular , let _ _ _ be the coefficient ( [ 5 . 2 ] ) of the inverse integral function , where _ _ @ xmath301 _ _ is the extreme value of _ _ @ xmath302 _ _ and then _ _ @ xmath303 _ _ , _ _ @ xmath304 _ _and _ _ @ xmath305 _ _ be the same as in theorem 5 . let . _ be the complete set of possible values ( theorem 5 . 5 ) . then the following conditions satisfy _ _ @ xmath307 @ xmath308 * * * . if @ xmath309using ( [ 3 . 1 ] ) , ( [ 3 . 2 ] ) , ( [ 3 . 6 ] ) , ( [ 5 . 10 ] ) and ( [ 5 . 12 ] ) , we have @ xmath310 @ xmath311 } _ { \ right } \ left ( v ^ { \ ast } p _ { 0 } ^ { \ ast } , p _ { 1 } ^ { \ ast } , p ^ { \ ast } \ right ) \ dot { 5 . 10 } \ ] ] @ xmath312 } ^ { 1 } + \ left \ vert p _ { 1 } - p _ { 0 } ^ { \ ast } \ left \ vert _ { \ % \ left [ \ dot { k } , \ overline { k } \ right ] } ^ { 1 } + \ left \ vert p _{ \ alpha \ left ( \ alpha \ right ) } - p ^ { \ ast } \ left \ vert _ { p ^ { 2 } \ left [ 0 , 1 \ right ] } ^ { 2 } \ right ) \ leq c _ { 2 } \ delta ^ { 2 } . \ ] ] [ [ 5 . 1 and 5 . 2 @ xmath313 \ left { 5 . 3 } \ ] ] @ xmath314by ( [ 5 . 3 ] ) and ( [ 5 . 4 ] ) @ xmath315 \ leq j , p _ { \ alpha } \ left ( p ^ { \ ast } , p _ { 1 } , p _ { 2 } , p _ { \ alpha \ left ( \ alpha \ right ) } \ right ) \ leq c _ { 2 } \ delta ^ { 2 } . \ ] ] therefore , ( [ 5 . 6 ] ) implies ( [ 5 . 12 ] ) . since the function @ xmath316 is derived from the functions @ xmath279 and @ xmath317 as shown in the statement of theorem 5 . 2 , then ( [ 5 . 13 ] ) implies ([ 5 . 8 ] ) . finally , ( [ 5 . 9 ] ) follows from ( 5 . 6 ) and ( [ 5 . 8 ] ) . finally , ( [ 5 . 9 ] ) follows from that statement of theorem 5 . 1 and ( [ 5 . 9 ] ) . @ xmath74 * = 5 . 1 * . finally , theorem 5 . 1 proves the uniform ##ity property of our method , see the section below below . since the application of sections 1 - 4 is the main focus of this article , we discuss the details of the first method , both in this and that section . we will then use our first method for both the theoretical and practical applications . to avoid the error @ xmath318 we have written the derivatives of the functional @ xmath319 via the method with the step size @ xmath320 . then , we have written them with respect to @ xmath26 in the form , via the product method , with the step size @ xmath321 the derivative of the functional @ xmath28 with respect to @ xmath26 , which we used for our method ( see ( [ 1 . 4 ] ) ) , is written .despite problems with the step size @ xmath321 we have not found any errors in the calculation , mainly because the ball @ xmath322 was not very large . similar results were reached in experiments @ xcite where the calculations were performed , and experiments with experimental data . , we have calculated the slightly simplified value of @ xmath168 with respect to the size of the ball @ xmath323 at those two points . , we have used the gradient projection method . however , we have found in our calculations that the simpler and better approximation ##s yield exactly the same result . therefore , all the results obtained were obtained via the gradient projection . the starting point of this algorithm was @ xmath324 and the same method @ xmath325 was not used . the result means that our results are more accurate even than our theory predicted . the step size of the gradient projection @ xmath326 was small . we have found that this step size is the optimal size for our algorithm . the calculations were completed in 5000 steps . based on our computational results , we have proposed the following algorithm : 1 . calculate the optimal size @ xmath327 via .the function ( [ 3 . 12 ] ) . 2 . compute the function ( [ 3 . 13 ] ) . let @ xmath328 be the minimizer . 3 . compute the function @ xmath329 see ( [ 3 . 15 ] ) and ( 3 . 17 ) . 4 . compute @ xmath330 5 . compute the function @ xmath331 see ( 3 . 1 ) and ( [ 3 . 2 ] ) , @ xmath332 in this case , unlike the other two numerical tests , @ xcite , we do not need to compute the results of @ xmath327 . instead , we compute the maximum - error function from the original data , which is obtained by solving the equation ( [ 2 . 5 ] ) , ( [ 2 . 6 ] ) via the two - dimensional version of the born - schwinger equation @ xcite : @ xmath333here and finally , we will use @ xmath334 in all our tests . keeping in mind our original approach to detection of nuclear bomb - carrying submarines , we have used in our numerical tests the same error function @ xmath335 .: @ xmath336where @ xmath337 is the location of the source of our data of interest and @ xmath338 is the location . therefore , the background / background correlation in ( [ 6 . 0 ] ) is zero . for our statistical analysis we have : the ( [ 6 . 0 ] ) @ xmath339 = [ 1 : u0 _ 1 ] shows a typical example of the distribution of the observed correlation @ xmath340 at the initial point @ xmath341 . we can see that @ xmath342 first , @ xmath340 and very quickly for @ xmath343 next , the [ @ xmath344 $ ] appears to be the correct value , and we can use this for our analysis . next , we use for our analysis @ xmath345 and @ xmath346 . we note that even if the standard definition of the behavior of the correlation in @ xmath327 holds , for very large values of @ xmath347 the " 0 " " is meaningless , for , e . g . ( [ 6 . 0 ] ) . however , it is clear from thenote that we only have in the narrow range of r , and this can be interpreted as the limit of fundamental uncertainty in r . , scaledwidth = 0 . 5 % ] . , given the value of @ xmath348 , we define the function @ xmath349 . ( [ 2 . 1 ] ) and use the same formula for this function @ xmath350where @ xmath351 and @ xmath352 are real numbers , uniformly distributed on @ xmath353 . the most important question is about the existence of an additional function @ xmath354 . , even though [ 2 . 1 states that the function @ xmath170 is uniformly distributed on the unit interval @ xmath164 for all @ xmath355 in r , the larger @ xmath356 is , the better is the distribution on @ xmath168 of those values @ xmath357 which are not known from the interval @ xmath358 where the values are known . therefore , we need to find such a value of @ xmath359 which would give more accurate estimates of points , whose values @ xmat##h337 [ = ( ( [ 6 . 1 ] ) : @ xmath360 $ ] . let @ xmath361 be the discrete @ xmath362 version of the norm of the previous , discrete version of the norm @ xmath363 . [ example : gnorm ] shows the dependence of this function on the rate of convergence of the objective function for different values of @ xmath356 . we have found in our algorithm that these values are very large for targets satisfying ( [ 6 . 1 ] ) , ( [ 6 . 2 ] ) with different degrees of target / target convergence . we also assume that the process starts from @ xmath364 , which is to be expected , since convergence of @ xmath365 is not known . however , we find that the higher @ xmath366 values , the faster the process is . we have found that the maximum value of @ xmath356 for targets satisfying ( [ 6 . 2 ] ) is @ xmath367 . we then apply the pre - optimization method to step 2 of the above algorithm . more precisely , we work out the optimal @ xmath368 ( [1 ] ) using a different averaging method for the same data points . thus , the average function @ xmath369 is represented by @ xmath370the function @ xmath371 in ( [ 2 . 1 ] ) is represented as our average function @ xmath372 [ of the values of the function @ xmath373 for the @ xmath374 , scaledwidth = 0 . 5 % ] the same results @ xmath375 for different values of @ xmath337 are shown in ( [ 2 : 1 ] . we can see that the same algorithm also estimates the values and values of the function @ xmath335 . the value is found for the source / target function in ( [ 2 . 1 ] ) ranging from 1 to 4 . we use exactly the same averaging methods as were used by klibloc , kuzh , ieee , where these data were analyzed using the # ##ing algorithm . therefore , it is possible to apply the averaging method of this algorithm to the same data points . in @ xcite the wave propagation problem is modeled by a 2 - dimensional complex manifold , the fourier transform with respect to time is applied to the complexof this , and then the tail functions method was applied to the next imsp . in @ xcite the method was replaced by imsp ( [ 5 . 5 ] ) and the tail functions method was applied to this imsp . the values in @ xcite and in @ xcite were obtained by applying fourier and fourier transforms directly to the generated time series data . we have found a large difference of values between the generated and experimental data . therefore , we have obtained the values , via replacing them with the values of @ xmath376 , as in @ xcite . ( 180 , 0 . 5 ) ( 5 . 5 , 0 ) ( 11 , 0 . 5 ) arc ( 270 : 0 : 0 . 5 ) ( 5 . 5 , 0 . 5 ) arc ( 180 : 0 : 0 . 8 ) ( - 3 , 0 ) arc ( 180 : 0 : 0 . 8 ) ( - 2 , 0 . 5 ) ( - 2 . 7 , 0 ) ( - 2 . 7 , 0 ) arc ( 180 : 0 : 0 . 3 ) ( 2 . 7 , 0 ) arc ( 180 : 0 : 0 . 0 ) ( 6 . 7 , 0 ) ( 180 , 0. 5 ) ; ( 8 , 0 ) ; ( 2 ) ; ( 1 , 2 ) ; ( 3 ) ; ( 4 . 3 , 5 ) ( 4 , 5 ) ( 4 . 3 , 6 ) ( 4 . 3 , 5 ) ; ( 2 , 6 . 3 ) ( 2 , 6 . 3 ) ( 2 , 6 . 3 ) ( 2 , 6 . 3 ) ( 2 , 6 . 3 ) ( 2 , 6 . 3 ) ( 2 , 6 . 3 ) ; ( 2 , 5 . 3 ) ( 3 , 5 . 3 ) ( 2 , 6 . 3 ) ; ( 2 , 6 . 3 ) ( 3 , 6 . 3 ) ; ( 3 . 5 , 4 . 5 ) ; ( 0 . 1 ) ; ( 4 , 5 . 1 ) ; ( 0 . 2 ) ; ( 3 . 5 , 4 . 2 ) circle ( 0 . 2 ) ; ( - 2 , - 2 ) ; ( 30 , - 3 ) ; ( 30 , - 2 . 5 ) ; ( 30 , - 2 ) ; ; ( 22 . 5 , - 2 . 5 ) ; ; ( 22 . 5 , 2 ) ; ( 2 : 350 : 0 . 5 ) ; ( 5 . 5 , 6 . 5) arc ( 300 : 350 : 1 . 5 ) ; ( 11 . 5 , 1 ) arc ( 300 : 350 : 2 . 5 ) ; ( 12 . 5 , 1 ) arc ( 300 : 350 : 3 . 5 ) ; ( 14 . 5 , 1 ) arc ( 300 : 350 : 4 . 5 ) ; ( 16 . 5 , 1 ) arc ( 300 : 350 : 5 . 5 ) ; ( 17 . 5 , 1 ) arc ( 300 : 350 : 6 . 5 ) ; ( 18 . 5 , - 1 ) arc ( 300 : 350 : 6 . 5 ) ; our first signal was received in the field using the forward looking telescope of the us naval research laboratory @ xcite . the phase diagram of the collection is shown on page [ 1 : 1 ] . the device has two antennas mounted on the roof of the building . they receive light . the device also has two detectors . detectors measure field - induced current , which is called the signal . components of only one component of the magnetic field are measured and the entire signal is recorded by those detectors . the short - range of detection is 0 . 133 ms and the remaining components of the received signal are at least 1 ms , .. 1 . since 1 khz corresponds to the frequency of 1 khz @ xcite , and the received radio signal is 0 db , which are considered very low frequencies in radio . the background noise and the frequency of radio signals are measured at distances from 1 to 10 meters from the target of interest . the radio signals are recorded . we measure the coordinates of the target with a very high accuracy : to do this , the global positioning system is used . two types of targets are used : those buried in earth and those located at the depth of a few meters in the ground . . the horizontal coordinate is . . [ . , scaledwidth = 1 . 5 % ] . it was assumed that in ( [ 5 . 1 ] ) and ( [ 6 . 1 ] ) that @ xmath377 we have a target buried in the ground , in which @ xmath378 this target is a metal cylinder . it was assumed in section 2944 of @ xcite that , from the measured distance and location , we can find out that in the cylinder @ xmath379 that , in this cylinder we have ( [ 5 ] ) and ( [ 6 . 1 ] ) with @xmath380 @ xmath381 note that each target has a subinterval @ xmath382 in addition , we define as the value of the constant of target and background for @ xmath383 . thus , if our computed function @ xmath384 , ( [ 7 . 1 ] ) and ( [ 7 . 2 ] ) is an approximation of the function @ xmath385 @ xmath386where @ xmath387 is the time dependent time constant of that target . using ( [ 7 . 1 ] ) , ( [ 7 . 1 ] ) , ( 7 . 2 ) and ( [ 7 . 3 ] ) , we obtain the same target / background function with the same parameters . @ xmath388 , \ \ \ , c _ { if } \ right ( x \ right ) , \ label { if } c _ { if } \ right ( x \ right ) \ leq \ , \ forall \ \ , \ { [ x , x \ right ] . % \ text { if } % \ right . \ text { 1 . 1 } \ ] ] finally , we obtain the[ @ xmath389 which is our estimate of the mass constant of the target , @ xmath390 we have calculated the value @ xmath391 $ ] [ @ xmath392 = \ left [ \ begin { k } , \ overline { k } \ [ ] . \ left { 7 . 1 } \ ] ] the reasons for the analysis ( [ 7 . 1 ] ) are compared with those for the analysis of published results ( figure 7 . 2 . we had simulated backgrounds for all the targets . the background was air in the case of targets placed in air with @ xmath393 and background was sand with @ xmath394 $ ] @ xcite in the case of sand targets . two targets , wood and metal box , were placed in air and four targets , metal box , wood cylinder and sand cylinder , were placed in air . ] [ source : exp _ 1 ] are the values of the constant of targets . the constant of targets was all placed in air . therefore , the only what we can do at this time is to compare our estimated values of @ xmath395 with known values . this is shown in figure [ tab1 ] ,for which @ xmath396 is the reference value . as to the other targets , it was shown already in @ xcite that they can be covered by other materials with different values of the same constant , @ xmath397 . \ ] ] the values of elastic constant of metal , glass and plastic can be found in @ xcite . as to the time when the target was in contact , we have the summary of the values from @ xcite . this is the most difficult time to study . this is because it is not a very stable material . . . of target material , @ xmath398 . [ id = " < , ^ , ^ , ^ , ^ , ^ " , id = " ^ " , ] for the research purposes of this group of researchers ( ln and as ) , the depth of penetration of the target is not of great importance , since most targets are a few meters . it is also true that it is difficult to work out the shape of the target , and therefore the information needed . on the other hand , the most important source of the information for ln and as is in terms of the elastic constant##s of p . 1 , table [ tab1 ] is the most important part of the paper from the technical side . however , we can see from this table that values of the interaction constant @ xmath398 are well within that of @ xmath399 . it is pointed out in table 2 , these values , even if not very accurate , could be considered very useful for the very important goal of reducing the false alarm rate . this means that the results of the previous paper could also be very useful for the purpose of the reduction of the false alarm rate . the new results use ln and si to estimate the constant of targets in the same region . our team wants to analyze those new experimental results with the same results of this paper . we have developed a new , improved , method for the 2 - dimensional light - scattering problem ( [ 4 . 1 ] ) . using the formula ##le ##p , the result of this paper does not exceed the upper limit of the sum of the [ @ xmath391 $ ] of the results . the formula is based on the use of a linear correlation function with the carleman number ##ing in it . the most fundamental mathematical result of this paper is [ 4 . 1, which is the maximum value of this function for any given radius @ xmath400 for any radius @ xmath1 , as long as the radius @ xmath401 of this function is chosen correctly . the accuracy of the method ##s of the solution of this function to the optimal solution is known . the evaluation of this function from both the theoretical and experimental data yields good results . a . ammari , y . y . chow , and y . zou , _ linear and phaseless approximation methods for the electron scattering problem via matrix elements _ , siam journal on applied mathematics , 8 ( 2016 ) , pp . 10001030 . m . a . bakushinskii , m . v . klibanov , and n . a . koshev , _ carleman method ##ing for a near optimal numerical solution for ill - posed cauchy problem for a quasilinear pdes _ , numerical analysis : real world applications , 7 ( 2017 ) , pp . 201224 . m . v . klibanov , n . a . koshev , y . chow , and a . a . yagola , _ numerical solution of an ill - posed cauchy problem for a quasilinear pd##bolic analysis of the carleman delta function _ , journal of posed and ill - posed problems , 28 ( 2016 ) , pp . 761776 . m . v . klibanov , y . - nguyen , l . h . nguyen , and y . liu , _ _ globally converge ##nt ##p for a 2d coefficient medium problem with the simultaneous analysis of multi - dimensional data _ , ( 2016 ) , https : / / arxiv . org / stable / 1612 . 04014 . m . v . klibanov , d . h . nguyen , a . sullivan , and d . nguyen , _ a globally converge numerical method for a 2 - d coefficient medium problem with 3d data _ , inverse problems and methods , 28 ( 2016 ) , pp . 10571085 . a . v . kuzhuget , l . beilina , m . v . klibanov , a . sullivan , d . nguyen , and d . j . fiddy , _ a comparison of data analysis in the laboratory and an experimental globally converge inverse problem _ , inverse problems , 28 ( 2012 ) , pp . 095007 . a . v . kuzhuget , l . beilina , m . v . klibanov ,j . wang , l . nguyen , and m . a . fiddy , _ 3d image reconstruction from near - 3d images using a globally converge inverse algorithm _ , ieee transactions on geoscience and remote sensing , 10 ( 2013 ) , pp . 29372948 . nguyen , a . a . klibanov , l . a . nguyen , a . a . kolesov , m . a . fiddy , and y . liu , _ numerical reconstruction of a heat transfer problem with multi - dimensional 2d image data using a globally converge inverse _ , ( 2016 ) , https : / / arxiv . org / stable / 1609 . 03102 . l . nguyen , y . wong , m . ressler , m . koenig , j . wang , l . wang , a . sichina , and a . kappra , _ collision detection and moving object detection in the stanford research center ultra - fast finite impulse response ( uwb ##d ) # ##ing _ _ , 2007 , pp . 65530h . s . sini and s . s . thnh , _ generalized _ algorithm - like methods for 3d image recovery using multifrequency algorithms _ ,esaim : materials modeling and image analysis , 34 ( 2015 ) , pp . 459480 . a . a . thnh , a . beilina , a . a . klibanov , and j . a . fiddy , _ reconstruction of _ structures from the _ time - series data using the _ optimal _ algorithm _ , international journal of materials science , 34 ( 2015 ) , pp . 757786 . _ _ constant table _ , https : / / www . honeywellprocess . com / library / marketing / tech - spec / _ constant table . pdf [ https : / / www . honeywellprocess . com / library / marketing / tech - spec / _ constant table . pdf ]