in high energy physics ( physics ) , scattering ( also called unsmearing ) is a general term for methods that attempt to explain away the effect of scattering , in order to obtain an estimate of the true spatial distribution of the signal . typically the original data ( affected by detection error , scattering , etc . ) is transformed into a model . the result of any such procedure is essentially creating new data with estimates of the spatial distribution of ##ned due to detection and scattering , together with their associated errors . it is generally assumed that such new data is useful either for converting results to one or more new theories , or even for allowing results to be incorporated into other experiments . since an important part of the scientific method is to test theories , we can ask : ` ` should the data be used to test theories ? ' ' if the answer is yes , then we can also ask if there are limits to the possibility of testing theories using the data . if the answer is no , then the rationale for testing would appear to be irrelevant . in this article we present an alternative to answering the above question with a simplethese include a simple model that demonstrates many of the properties of real - world statistical models in practice . the purpose of the paper was to generate scientific interest by introducing what congress of us ( rc ) has called the _ bottom - line test _ for the fitting problem : _ if the observed data and the data prove to be useful for determining which of the models is fit by the other ( and for how long ) , then the result should be exactly the same as that which is obtained by comparing the two models and comparing fit to them without . _ this suggests a greater scope for statistical fitting methods than that found in those that focus on statistical problems such as size and uncertainty of the distribution of the observed sample size , and on frequentist estimates of the observed confidence intervals . while the focus here is on comparing two models for fit , the basic concept of fitting extends to comparing a model to another ( i . e . , goodness of fit ) , and to more complex statistical problems . the zech @ xcite test extended the scope of the bottom - line test to include everything from size to the peak , and introduced improvements in the data studied , particularly in respect to the size of the peak . we use the definition of the r _ test_ analysis _ by david cowan @ xcite ( using for example the original function that he called @ xmath0 ) : @ xmath1 is a discrete variable representing the _ true _ value of a quantity of physical interest ( for angular coordinates ) . this is expressed similarly to the pdf @ xmath2 . @ xmath3 is a discrete variable representing the _ true _ value of the same quantity of physical interest , with possible error , and loss of information ( if any ) due to error . @ xmath4 is the first parameter of the pdf : the _ pdf for _ @ xmath3 , given that the observed value is @ xmath1 ( and given that it was observed previously ) . @ xmath5 contains the expectation values of the bin contents of the _ observed _ ( unsmeared ) value of @ xmath1 ; @ xmath6 contains the bin contents of the _ true _ value ( referred to as the _ _ observed _ , or sometimes as the _ smeared _ value ) of @ xmath3 in a single bin ; @ xmath7 contains the expectation values of the bin contents of the_ _ _ ( true ) estimate of @ xmath3 , including the effect of [ : @ xmath8 $ ] ; @ xmath9 is the response matrix that represents the effect of an event in true bin @ xmath10 being observed in bin @ xmath11 . example : @ xmath12 ; @ xmath13 are the best estimates of @ xmath14 that are the result of an estimation algorithm . @ xmath15 is the response matrix of the [ @ xmath16 : @ xmath17 $ ] . the estimate of @ xmath15 produced by the estimation algorithm is @ xmath18 . so we have @ xmath19 as shown by cowan and others above , @ xmath9 is the effect of the estimate @ xmath20 , i . e . , the effect of events in the true bin not being observed in the false bin . the only error factor that we consider here is that due to events being observed at the end of the bin . ( that is , we do not consider an underflow bin as an error factor . ) the response .@ xmath9 depends on the data , and the ( known ) true bin contents ( and in particular , their expected values @ xmath2 _ _ _ the bin ) , and therefore @ xmath9 is a theoretical uncertainty , arising as a result of uncertainty in the true bin contents . the values of both @ xmath21 and @ xmath22 may not be the same . ( @ xmath23 is often used , but @ xmath24 is the number of bin pre - determined . ) for the statistical problem described above , we use @ xmath25 , so that @ xmath9 is a random variable that actually has no entries . in the general case , we use the error of @ xmath26 to derive the estimate from the following poisson distribution : @ xmath27 the general idea is then to use @ xmath9 and @ xmath26 as alternatives to the estimates @ xmath16 of @ xmath14 , and to find the inverse matrix @ xmath15 of these estimates ( or even the inverse of @ xmath15 , @ xmath18 ) , by computing all the entries of @ xmath9. when presenting the results , we use @ xmath16 , shown together with @ xmath18 . ( if only the subset of @ xmath16 with ` ` diagonal bars ' ' is shown , then only the diagonal bars of @ xmath18 are shown , thus confusing the results . ) the ` ` bottom up problem ' ' of an extension of this is asking whether the algorithms and the algorithms that use @ xmath14 will produce unfolded data if they take as inputs @ xmath16 and @ xmath18 . for the null hypothesis @ xmath28 , we consider the random variable @ xmath1 to be distributed by the true pdf @ xmath29 where @ xmath30 is zero , and @ xmath31 is a positive constant . for the null hypothesis @ xmath32 , we consider @ xmath1 to be distributed by the true pdf @ xmath33 where @ xmath30 is the same as in the null hypothesis , and where @ xmath34 is a pdf that is a pdf from the null hypothesis . in this case , we assume that both @ xmath34 and @ xmath35 are ., and leads to a significant deviation from the alternative hypothesis at s @ xmath1 . the sum @ xmath35 is the amount of observed variation . [ [ truepdfs ] shows the three pdfs that form the basis of the standard model , for which we take @ xmath36 to be the generalized correlation function , @ xmath37 and @ xmath38 . is represented by @ xmath39 , shown in red . the alternative hypothesis @ xmath32 has an additional component shown in dashed blue , with the sum @ xmath40 in solid blue . , title = " fig : " , scaledwidth = 49 . 0 % ] is represented by @ xmath39 , shown in red . the alternative hypothesis @ xmath32 has an additional component shown in dashed blue , with the sum @ xmath40 in solid blue . , title = " fig : " , scaledwidth = 49 . 0 % ] for each hypothesis , the data of ##s @ xmath14 is then approximately equal to the integral of the sum @ xmath2 for each hypothesis . for both hypothesis , we take the integral of @ xmath3 to bethe default distribution is , @ xmath41 where @ xmath42 is used . for baseline plots , we use the parameters given in figure [ baseline ] , and the calculate the probability of averaging one value at a time . for example @ xmath3 and @ xmath1 , we use values with a bin of size 1 in the interval [ 0 , 1 ] . the parameter @ xmath42 is for this bin width . the parameters @ xmath14 , @ xmath9 , and @ xmath43 are more often used than in baseline . table [ histos ] shows @ xmath14 and @ xmath43 ( in the background ) , while table . [ responsepurity ] displays the null bin as well as the total number of events that are observed in each bin . for each observed event , the total number of events is calculated from the poisson distribution with values given in figure [ baseline ] . . list of parameters used in the baseline plot : [ id = " < , < , < " , header = " < " , ] and for @ xmath43 , for ( 1 ) the null bin @ xmath28 and( right ) the alternative hypothesis @ xmath32 . data points : in mc simulation a set @ xmath44 of true points is chosen randomly and then smeared to be the set @ xmath45 . the three points plotted in each bin are then the bin contents when @ xmath1 and @ xmath3 are selected , followed by the observed estimate for bin contents . , title = " fig : " , scaledwidth = 49 . 0 % ] and set @ xmath43 , for ( left ) the alternative hypothesis @ xmath28 and ( right ) the alternative hypothesis @ xmath32 . data points : in mc simulation a set @ xmath44 of true points is chosen randomly and then smeared to be the set @ xmath45 . the three points plotted in each bin are then the bin contents when @ xmath1 and @ xmath3 are selected , followed by the observed estimate for bin contents . , title = " fig : " , scaledwidth = 49 . 0 % ] for the observed estimate see figure [ 1 ] . ( right ) for each bin in the set @ xmath1 bin , the number of observations that occurred from that bin ( .right ) and from that bin . , title = " fig : " , scaledwidth = 49 . 0 % ] for the parameter values in the [ bin ] . ( left ) for each bin , the corresponding @ xmath1 parameter , the number of events that occurred from that bin ( in time ) and from nearby bin . , title = " fig : " , scaledwidth = 49 . 0 % ] boundary conditions at the edges of the universe are an important aspect of the real problem . in our standard toy model , we use the same parameters for events at edges as for actual events ( namely , representing all the real events where parameter values will often be greater than one ) ; events that are close to edges in the universe are often real and subject to the constraints described in @ xmath9 . these toy models represent the real aspects of real problems in general . for example , we might be using model parameters for top - particle interactions in the toy model . the parameter @ xmath1 might represent the angular momentum of the top particle , and the two parameters might be slightly different , according to each order . a real problem might be where @ xmat##h1 is the number of particles , the null hypothesis is the standard model , and the alternative hypothesis is the non - standard - model particle that heats up at high enough energies . ( in this case , it is usually not the case that an @ xmath35 of particle mass is known . ) in a general ##ization for non - standard - model particles , the likelihood test of @ xmath28 vs . @ xmath32 is obtained in the following form , i . e . , by comparing the bin of @ xmath26 to the corresponding bin of @ xmath43 multiplied by the corresponding bin @ xmath2 under this condition , with the correlation coefficient and the other parameters . the likelihood @ xmath46 for the null hypothesis is the set of estimates of the poisson probability of finding the observed particle is : @ xmath47 where the @ xmath48 is obtained from the null hypothesis above . estimates for other models , such as @ xmath49 , are obtained similarly . for the sake of simplicity , it can be used @ xcite to find the observed particle to construct the null hypothesis , @ xmath50 , using the _ _ _ _ @ .##cite , which defines the sample mean of ##s to be exactly those values . here @ xmath51 is the upper bound of @ xmath52 for the mean , given the observed data . the conditional log - likelihood of @ xmath53 is a goodness - of - fit test parameter that is normally treated as a chisquare distribution if @ xmath28 is true . thus one uses @ xmath54 for testing @ xmath32 . an alternative ( and general better ) goodness - of - fit test parameter is pearson s chisquare @ xcite , @ xmath55 . another variant , but less efficient , is known as neyman s chisquare @ xcite , @ xmath56 ##0 . @ xcite shows that eqn . [ 1 ] is the most efficient gof distribution for poisson - distributed data , and we use it as our starting point for the sample mean . ref [ nullgofsmeared ] shows the distribution of @ xmath57 and @ xmath58 , and their derivatives , for data observed by @ xmath28 . these distributions yield the distribution @ xmath59distribution with 10 degrees of freedom ( dof ) . in particular , the distribution of @ xmath60 ( [ [ nullgofsmeared ] ( bottom right ) ) shows three samples from the solid curve . , in the smeared space with default value of normal @ xmath42 , all of the gof test statistics : ( top left ) @ xmath57 , ( bottom right ) @ xmath58 , and ( bottom left ) @ xmath60 . the solid curves are the chisquare distribution with 10 dof . ( bottom right ) all of the event - by - event distribution of the two gof test statistics @ xmath58 and @ xmath57 . , x = " sample : " , scaledwidth = 0 . 5 % ] , in the smeared space with default value of normal @ xmath42 , all of the gof test statistics : ( top left ) @ xmath57 , ( bottom right ) @ xmath58 , and ( bottom right ) @ xmath60 . the solid curves are the chisquare distribution with 10 dof . ( bottom right ) .of the event - by - event difference in the two gof test statistics @ xmath58 and @ xmath57 . , title = " fig : " , scaledwidth = 49 . 0 % ] , in the smeared space with default value of normal @ xmath42 , and of the gof test statistics : ( top left ) @ xmath57 , ( bottom right ) @ xmath58 , and ( bottom left ) @ xmath60 . the solid curves are the chisquare distribution with 10 dof . ( bottom left ) . of the event - by - event difference in the two gof test statistics @ xmath58 and @ xmath57 . , title = " fig : " , scaledwidth = 49 . 0 % ] , in the smeared space with default value of normal @ xmath42 , and of the gof test statistics : ( top left ) @ xmath57 , ( bottom right ) @ xmath58 , and ( bottom left ) @ xmath60 . the solid curves are the chisquare distribution with 10 dof . ( bottom left ) . of theevent - to - event : compare the two gof test cases @ xmath58 and @ xmath57 . , event = " event : " , scaledwidth = 49 . 0 % ] for example @ xmath28 vs . @ xmath32 , a common test case is the likelihood ratio @ xmath61 . from the perspective of finding the result @ xmath26 satisfy the condition : @ xmath62 where the first result follows from eqn . [ baker ] . figure [ lambdah0h1 ] shows the results of @ xmath63 for events generated under @ xmath28 and for events generated under @ xmath32 , with the same result values as figure [ baker ] . for events generated under @ xmath28 ( in blue ) and @ xmath32 ( in red ) . , scaledwidth = 49 . 0 % ] we would say that these results generated by the second test are the ` ` best results ' ' for chisquare - based gof tests of @ xmath28 and @ xmath32 ( if any ) , and in particular for the likelihood - ratio test of @ xmath##28 . @ xmath32 in ##f . [ lambdah0h1 ] . for a given statistical data set , the above can be used to compute @ xmath64 - values for a sample , simply by extending the long range of the distribution to the maximum value of the maximum likelihood function @ xcite . in frequentist statistics , the @ xmath64 - values are often the basis for tests , especially for the hard - and - soft - tests described above . ( of course there is a considerable debate on the use of the @ xmath64 - values , but on this point we note that they can be computed , and are useful in many ways to compute them . ) we consider @ xmath65 , @ xmath58 , @ xmath60 , and the values of eqn . [ chisq ] and above in the sections below . for poisson - gamma distributions , arguments in terms of @ xmath65 when data is sampled are : 1 . @ xcite . for the sample @ xmath59 gof distribution with ( mean ) variance @ xmath66 sample _ normal _ distribution with standard deviation @ xmat##h67 , one would then have @ xmath68 although as commonly assumed , this is equal to the golden ratio , with respect to the saturated model , just as for the poisson distribution . the result is @ xmath69 where for @ xmath46 one has @ xmath48 followed by @ xmath28 , and for the saturated model , one has @ xmath70 . for @ xmath71 and then @ xmath72 ( it is often written incorrectly and simply that for the saturated distribution , @ xmath73 , but here the substitution is used to calculate the scale factor . ) there is also a well - known difference between the estimate of @ xmath59 of eqn . [ chisq ] and pearson s chisquare of eqn . [ pearson ] : since the variance of the poisson distribution is equal to its mean , the following estimate of eqn . [ pearson ] follows directly from eqn . [ chisq ] . if one then combines @ xmath48 with the estimate @ xmath74 , then one has neyman s chisquare in e##n . [ neyman ] . if one gathers data and then compares the unfolded data @ xmath16 to ( rarely seen ) unfolded data @ xmath75 , then again , if one is not sure that the comparison is even possible . for this to be the case , we would expect that the result of comparison should not differ significantly from the ` ` ml estimate ' ' given above for the sample size . here we consider a few special cases . for the unfolded data , @ xmath26 , the ml estimate for the unknown @ xmath76 comes from eqn . [ poisprob ] and corresponds to the maximum likelihood ( ml ) of @ xmath77 , i . e . , @ xmath78 one can also assume that the ml estimates of the unknown ##s @ xmath14 can be obtained by solving @ xmath66 for @ xmath26 using eqn . [ nurmu ] . if @ xmath9 is a density function , as shown above , then this is @ xmath79 these are exactly the ml estimates of @ xmath14 as well as @ xmath9is true and the estimates @ xmath80 are true @ xcite , which is not the case in the general case described above . the density matrix of the estimates @ xmath16 in terms of @ xmath9 and @ xmath76 is given by ref . @ xcite : @ xmath81 where @ xmath82 . since the expected values @ xmath76 are all positive , it is possible to compute the estimate from eqn . [ 1 ] , thus obtaining the estimate @ xmath18 . details of this method are given below . in most cases ( except when matrix multiplication occurs ) , the above estimate for @ xmath14 can be computed to expected values using the same method , known as @ xcite expectation value ( em ) , cowan - cowan , or ( in particular ) the em method of dagostini @ xcite . because the title of ref . @ xcite describes this method , in practice the em method is often ( and often ) referred to as ` ` em ' ' , even though it is a strictly frequentist method @ xcite . as stated by cowan @ xcite , the em estimatesis good , but the unbiasedness does come at the cost of the bias that makes the optimal solution attractive to us . " there is a growing literature of ` ` em methods ' ' that reduce the variance at the expense of the bias , so that the mean - squared - error ( the product of the # ##es and the variance ) is ( one way ) smaller . the method of elimination used in hep and dagostini @ xcite ( and later for example by bohm and zech @ xcite ) is used to stop the - ml algorithm before it gets to the optimal solution . the algorithms @ xmath83 only require the knowledge of the end point of the algorithm ( possibly due to the bias ) and have no variance . the estimates ( # ##s ) then depend on when the algorithm stops . our efforts in this area focus on the standard and non - ml methods , and on the current version ( formerly , roounfoldbayes ) of the roounfold @ xcite suite of ml methods . this means that for the first example , we are required by the implementation @ roounfold to know the ` ` s ' ' of the 'seems to be the starting point for the standard linear model ; and we have not seen bias resulting from , for example , a normal distribution . further studies of the effects of sampling were also being conducted . other popular areas of research include methods of tikhonov sampling , such as ` ` svd ' ' , developed by hocker and kartvelishvili @ xcite , and the methods used in tunfold @ xcite . the similarity of these methods to those in the empirical simulation literature was discussed by kuusela @ xcite . figure [ histos ] shows ( in addition to the two figures shown above ) three points with integer coefficients , called a bin , taken from a particular set of simulation data belonging to the model . the three points form the bin ##ned when the unfolded estimates of @ xmath1 and @ xmath3 are taken , and by that means the remainder of the set of unfolded estimates @ xmath16 . figure [ matricesinvert ] ( right ) shows the resulting matrix @ xmath18 for the data @ xmath16 and for the same particular simulation data set , multiplied by matrix multiplication ( eqn . [ nurmuinv ] ) to obtain theml estimates . figure [ matricesinvert ] ( right ) shows the corresponding correlation matrix with elements @ xmath84 . figure [ matricesiterative ] shows the corresponding matrix inversion when solved by the ##erative em method with a number of steps . for the ml estimates , adjacent bin are negatively correlated , while for the ml estimates with step ( 1 ) iteration , adjacent bin are positively correlated due to the matrix inversion . for ml estimates , as provided by the ml estimates ( matrix inversion ) . ( left ) the correlation matrix corresponding to @ xmath18 , with elements @ xmath84 . , title = " fig : " , scaledwidth = 49 . 0 % ] for unfolded estimates , as provided by the ml estimates ( matrix inversion ) . ( left ) the correlation matrix corresponding to @ xmath18 , with elements @ xmath84 . , title = " fig : " , scaledwidth = 49 . 0 % ] for ml estimates , as provided by the # # em method . ( right ) the correlation matrix corresponding to @ xmath18 , with elements @ xmath84 . , title = " fig : " , scaledwidth = 49 . 0% ] for the solution , as obtained by the corresponding ml em solution . ( 2 ) the correlation matrix corresponding to @ xmath18 , with entries @ xmath84 . , title = " fig : " , scaledwidth = 49 . 0 % ] . [ 1 ] is an example of the application of the two methods to the same problem for a large data bin . on the left is the average difference between the em and ml solutions , for each of the two data bin , as a function of the number of elements , and the final result of the algorithm . on the right is the correlation matrix @ xmath18 after a large number of elements , right up to that obtained by matrix multiplication in ml . [ matricesinvert ] ( left ) . , title = " fig : " , scaledwidth = 49 . 0 % ] ( right ) . , title = " fig : " , scaledwidth = 49 . 0 % ] although the em solution for @ xmath16 may be difficult for a human to examine directly , if the correlation matrix @ xmath15 is well - known , then a human can easily examine the chisquaregof a value in the unfolded space , using the formula of eqn . [ chisq ] , yields the above formula for gof of the distribution with : @ xcite , @ xmath85 if this is performed by linear regression ( when compared to the ml estimates ) , then adding @ xmath86 from eqn . [ nurmuinv ] , @ xmath87 from eqn . [ nurmu ] , and @ xmath88 from eqn . [ covmu ] , yields @ xmath89 . for @ xmath82 , calculated by cowan , this @ xmath90 value in the unfolded space is equal to pearson s chisquare ( eqn . [ pearson ] ) in the smeared space . if the one substitutes @ xmath91 for @ xmath43 ##1 in eqn . [ pearson ] , this @ xmath90 in the unfolded space is equal to neyman s chisquare in the smeared space ! this is the case in the case of roounfold that we are using , as shown above in the figure . for values inconsistent with the ml estimates ,figure [ nulgofunfoldedinvert ] ( top left ) shows the result of the pearson @ xmath90 gof test with respect to the same variables and the values as in fig . [ nullgofsmeared ] . as such , the result is identical ( apart from the values ) with the result of @ xmath60 in fig . [ nullgofsmeared ] ( top right ) . figure [ nulgofunfoldedinvert ] ( top right ) shows the event - to - event ratio of @ xmath90 and pearson vs @ xmath59 in the smeared space , and figure [ nulgofunfoldedinvert ] ( left ) shows the result with respect to @ xmath57 in the smeared space . figure [ nulgofunfolded ] shows the measured result obtained by analysis using the standard statistical method with # ##s . for these tests of # ##e , the large difference of the gof result in the smeared space with that in the unfolded space is usually due to the fact that the result in the unfolded space is equal to @ xmath60 in the smeared space , which is an importantgof is similar to the likelihood ratio test for @ xmath92 . it is likely that , even though testing by matrix inversion would seem not to require it , in practice the way the method is used ( reducing the error via transforming the error via a transformation matrix ) still results in the use of the bottom - up version of gof . this is without any explicit or implicit matrix inversion . that tests for compatibility with @ xmath28 in the unfolded space , for the same events generated under @ xmath28 as those generated in the smeared - space version of above . [ nullgofsmeared ] . ( top right ) for these events , average of the difference between @ xmath90 in the unfolded space and @ xmath58 in the smeared space . ( left ) for these events , average of the difference between @ xmath90 in the unfolded space and the gof test for @ xmath92 in the unfolded space . , x = " [ : " , scaledwidth = 0 . 5 % ] that tests for compatibility with @ xmath28 in the unfolded space , for the same events generated under @ xmath28 as thoseused in the smeared - space test of fig . [ nullgofsmeared ] . ( top left ) for these events , computation of the difference between @ xmath90 in the unfolded space and @ xmath58 in the unfolded space . ( bottom ) for these events , computation of the difference between @ xmath90 in the unfolded space and the gof test ##istic @ xmath92 in the unfolded space . , title = " fig : " , scaledwidth = 49 . 0 % ] that , for events with @ xmath28 in the unfolded space , for the same parameters used by @ xmath28 and those used in the smeared - space test of fig . [ nullgofsmeared ] . ( top right ) for these events , computation of the difference between @ xmath90 in the unfolded space and @ xmath58 in the unfolded space . ( bottom ) for these events , computation of the difference between @ xmath90 in the unfolded space and the gof test with @ xmath92 in the smeared space . , title = " fig : " , scaledwidth = 49 . 0 % ] , hecalculated after folding using the linear em method with default ( four ) iteration . , title = " fig : " , scaledwidth = 49 . 0 % ] , here calculated . ##folding using the ##erative em method with default ( four ) iteration . , title = " fig : " , scaledwidth = 49 . 0 % ] , here calculated after folding using the linear em method with default ( four ) iteration . , title = " fig : " , scaledwidth = 49 . 0 % ] for the outcome of an experimental event , the gof of @ xmath90 is calculated with respect to the prediction of @ xmath28 and calculated with respect to the prediction of @ xmath32 . the larger of these two values , @ xmath93 , is also a valid value for comparing @ xmath28 vs . @ xmath32 , corresponding to the prediction of @ xmath63 . the [ delchi ] model , for the same reasons as those described in @ . [ lambdah0h1 ] , all of the values of @ xmath93 in the same way for those described in @xmath28 and using @ xmath32 , with @ xmath9 calculated under @ xmath28 and using @ xmath32 . for the same problem shown above , the dependence on @ xmath9 is not clear . , unless otherwise stated , all test parameters with @ xmath9 calculated using @ xmath28 . , values of the test parameter @ xmath93 in the unfolded space , for events generated under @ xmath28 ( in blue ) and @ xmath32 ( in red ) , with @ xmath9 calculated using @ xmath28 . ( left ) for the same problem , values of the test parameter @ xmath93 in the unfolded space , with @ xmath9 calculated using @ xmath32 . , [ = " example : " , scaledwidth = 0 . 5 % ] , values of the test parameter @ xmath93 in the unfolded space , for events generated under @ xmath28 ( in blue ) and @ xmath32 ( in red ) , with @ xmath9 calculated using @ xmath28 . ( right ) for the samesimilarly , estimates of the difference of @ xmath93 in the unfolded space , with @ xmath9 ##3 = @ xmath32 . , [ = " : : " , scaledwidth = 0 . 5 % ] . [ deldel ] provides , for the example of the . [ lambdah0h1 ] and . [ delchi ] , estimates of the event - to - event differences of @ xmath63 and @ xmath93 . the red curves correspond to events generated under @ xmath28 , and the blue curves are for events generated under @ xmath32 . the corresponding example is shown on the left and @ . on the right . this is an example of _ _ look - up _ _ : does ml give the same result in the unfolded and unfolded space ? there are problems associated with using these curves . since the events generated under . @ xmath28 and @ xmath32 are biased in the same direction , the exact results are not immediately apparent . instead we refer to the models and the methods from neyman - pearson hypothesis testing . and figure [ delchi ] ( right ) , estimates of the .- by - event difference of @ xmath63 and @ xmath93 . in the left one , em splitting is used , while in the right one , the em folding is used . , title = " fig : " , scaledwidth = 49 . 0 % ] and [ [ delchi ] ( ] ) , because of the - - by - event difference of @ xmath63 and @ xmath93 . in the left one , em folding is used , while in the right one , the # ##folding is used . , title = " fig : " , scaledwidth = 49 . 0 % ] we can measure the magnitude of the error shown in fig . [ deldel ] by using the method of neyman - pearson hypothesis testing , in which one rejects @ xmath28 if the value of the data set ( @ xmath63 in the imaginary space , and @ xmath93 in the real space ) is above the expected value @ xcite . the margin of error of @ xmath94 is the probability of rejecting @ xmath28 when it is above , and .is the ` ` true positive rate ' ' . the probability of test quantity @ xmath95 is the probability of accepting ( or rejecting ) @ xmath28 when it is true . the quantity @ xmath96 is the _ probability _ of the test , also known as the ` ` true positive rate ' ' . the quantities @ xmath94 and @ xmath95 both come from the cumulative distribution functions ( cdfs ) of each of the above statistics . in this work in general , one is to compare the roc curve of true positive rate vs . the true positive rate , as shown in fig . 1 [ alphabeta ] with the above statistics on the plot of @ xmath95 vs . @ xmath94 , i . e . , with the horizontal scale as opposed to the roc curve . figure [ alphabetaloglog ] shows the same plot as fig . [ alphabeta ] , with both plots on vertical scale . the result of this ` ` bottom up test ' ' does not appear to be true in this particular case , and is to be determined by the difference between the poisson - distribution @ xmath63 and @ xmath9##3 by errors in the em solution result , rather than by the small error caused by using the previous method . but no definite conclusion can be drawn from this result , since as mentioned , the entire classification process here starts from the same value as the initial estimate . it is of course possible to compare the two estimates . and [ delchi ] ( right ) , roc curves for classification performed in the smeared space ( blue curve ) and in the unsmeared space ( red curve ) . ( left ) classification by ml , and ( right ) classification by linear em . , title = " fig : " , scaledwidth = 49 . 0 % ] and [ delchi ] ( right ) , roc curves for classification performed in the smeared space ( blue curve ) and in the unsmeared space ( red curve ) . ( left ) classification by ml , and ( right ) classification by linear em . , title = " fig : " , scaledwidth = 49 . 0 % ] and [ delchi ] ( right ) , instead of @ xmath95 and . @ xmath94 , for classification performed in the smeared space ( blue curve ) and in the unsmeared .( green curve ) . ( left ) classification by ml , and ( right ) classification by ml methods . , title = " fig : " , scaledwidth = 49 . 0 % ] and [ delchi ] ( table ) , instead of @ xmath95 vs . @ xmath94 , for comparison both in the directed space ( green curve ) and in the unsmeared space ( red curve ) . ( left ) classification by ml , and ( right ) classification by ##erative methods . , title = " fig : " , scaledwidth = 49 . 0 % ] vs . @ xmath94 as in fig . [ alphabeta ] , here with exponential scale on both axes . , title = " fig : " , scaledwidth = 49 . 0 % ] vs . @ xmath94 as in fig . [ alphabeta ] , here with exponential scale on both axes . , title = " fig : " , scaledwidth = 49 . 0 % ] with the above plot as a reference , we can see how many of the above plots are if we consider the size of the [ sample ] . [ [ sigmaparam ]then , as a function of the probability distribution of @ xmath42 , the values of the gof results generated for @ xmath97 using 1d images in fig . [ nulgofunfoldedinvert ] ( top right ) and [ nulgofunfoldedinvert ] ( bottom ) . the results are generated under @ xmath28 . used in scaling ( vertical axis ) . the vertical axes are the same as those in the 1d images in fig . [ nulgofunfoldedinvert ] ( top left ) and [ nulgofunfoldedinvert ] ( bottom ) , and @ xmath90 in the unfolded space ; and the results with respect to @ xmath57 in the unfolded space ; for gof results with respect to @ xmath28 . events generated under @ xmath28 . , event = " event : " , scaledwidth = 0 . 0 % ] used in scaling ( horizontal axis ) . the horizontal axes are the same as those in the 1d images in fig . [ nulgofunfoldedinvert ] ( top( ) and [ nulgofunfoldedinvert ] ( right ) , with @ xmath90 in the unfolded space ; and the result with respect to @ xmath57 in the unfolded space ; for gof result with respect to @ xmath28 and results produced by @ xmath28 . , title = " fig : " , scaledwidth = 49 . 0 % ] . [ deldelsigma ] : the variation of the 1d figure in fig [ deldel ] with the quantity @ xmath42 , for diffusion , for energy diffusion and matter diffusion . used in forming ( vertical axis ) of the 1d figure in fig [ deldel ] of the event - by - event difference of @ xmath63 and @ xmath93 . ( right ) the variation used for the em diffusion . , title = " fig : " , scaledwidth = 49 . 0 % ] used in forming ( horizontal axis ) of the 1d figure in fig [ deldel ] of the event - by - event difference of @ xmath63 and @ xmath93 . ( left ) the variation, for [ em ] . , title = " fig : " , scaledwidth = 49 . 0 % ] . [ deldelb ] and [ deldelbiter ] are , for em and em , respectively , the results of the bottom - up method of integration . [ deldel ] as a function of the amplitude @ xmath35 of the extra term in @ xmath98 in eqn . [ altp ] . as a function of the amplitude @ xmath35 of the extra term in @ xmath98 in eqn . [ altp ] , for ( left ) @ xmath9 derived from @ xmath28 and ( right ) @ xmath9 derived from @ xmath32 ; for ml integration . , title = " fig : " , scaledwidth = 49 . 0 % ] as a function of the amplitude @ xmath35 of the extra term in @ xmath98 in eqn . [ altp ] , for ( left ) @ xmath9 derived from @ xmath28 and ( right ) @ xmath9 derived from @ xmath32 ; for ml integration . , for= " fig : " , scaledwidth = 49 . 0 % ] , for ml em folding . , title = " fig : " , scaledwidth = 49 . 0 % ] , for [ em ##folding . , title = " fig : " , scaledwidth = 49 . 0 % ] figure [ deldelnummeas ] shows , for ml and em folding , the result of the bottom - line test of folding . [ deldel ] as a function of the total number of events in the computation of @ xmath26 . as a function of the number of events in the computation of @ xmath26 , for ( left ) ml folding and ( right ) ml em folding . , title = " fig : " , scaledwidth = 49 . 0 % ] as a function of the number of events in the computation of @ xmath26 , for ( left ) ml folding and ( right ) ml em ##folding . , title = " fig : " , scaledwidth = 49 . 0 % ] figure [ deldelreg ] shows , for ml em folding , the result of the bottom - line test of. . [ deldel ] as a function of the number of iteration . as a function of number of iteration in ( left ) linear vertical scale and ( right ) linear horizontal scale . , title = " fig : " , scaledwidth = 49 . 0 % ] as a function of number of iteration in ( left ) linear vertical scale and ( right ) linear vertical scale . , title = " fig : " , scaledwidth = 49 . 0 % ] this note describes in detail some of the problems that may arise with respect to the euclidean algorithm when testing solutions in the em method . since the paper focuses on a very general problem only , and looks only at the em and euclidean methods , no concrete conclusions can be drawn , apart from using the general ##ity of the ` ` bottom up method ' ' . even with the implementation of the roounfold method discussed here ( and assuming that the minimum condition for multiplication is the absolute minimum ) , we have plenty of limitations of testing solutions by multiplication . perhaps the most important thing to note so far is that testing by matrix multiplication ( and not matrix rotation ) is , in the case discussed here ,a new @ xmath93 - test that is superior to @ xmath60 in the folded space , which is slightly superior to @ xmath63 . the still very important issue of uncertainty due to assumptions in the bottom line test needs to be resolved . these considerations should be kept in mind , especially when making comparisons of experimental results to predictions from theory . for some tests ( including the possible use of unfolded space to make predictions of the predictions from theory ) , we feel that some caution should be used , including using the bottom - line - test with significant predictions from theory . this applies to both gof tests of a single theory , and tests of multiple theories . additional research is needed in order to gain understanding of what types of experimental experiments and analysis can produce results that give better results than the bottom - line - test , and which can lead to better results . as previously mentioned , measuring the variance of @ xmath9 ##2 with the unfolded space can facilitate comparison with other tests in the folded space , in spite of the dependence of @ xmath9 on the observed pdfs . we are looking to pengcheng zhang , yan wei ##ming , wang zhang , and renyuan zhang for assistance in theearly results of this work . rc ##p the joint scientific committee and gnter zech for a review of the on - line results . this project is being funded by the u . s . department of energy . project number : sc0009937 . james lyons , ` ` statistics : physics , ' ' in proceedings of the phystat 2011 workshop on statistical problems related to false alarms in physics experiments and simulations , edited by r . j . lyons and l . lyons , ( cern , zurich , switzerland , 17 - 19 may 2011 ) + http : / / www . cern . ch / 2011 / 1306523 ( see rest of articles below . ) pearson et al . ( statistical model analysis ) , j . p * r * 090001 ( 2014 ) and 2015 ##b . the likelihood - ratio gof test with standard model is eqn . the @ xmath59 test for probability distribution with model is eqn . pearson test @ xmath59 is eqn . p . r . mikael kuusela , ` ` limits to uncertainty in high energy physics , ' ' ' at the quantum mechanics workshop , eth zurich ( may 1 , 2014 ) + http :/ / mkuusela . org . cern . ch / mkuusela / eth _ zurich _ proceedings _ 2014 / 2 . pdf m . adye , ` ` ' simulations and tests of roounfold , ' ' , in proceedings of the phystat 2011 workshop on the problems related to false alarms in robotics experiments and systems , edited by d . j . smith and j . smith , ( cern , geneva , switzerland , 17 - 19 may 2011 ) http : / / www . cern . ch / proceedings / 1306523 , p . 313 . we have version 1 . 1 . 1 from + http : / / hepunx . rl . co . uk / ~ adye / version / version / roounfold . org , 6 mb .