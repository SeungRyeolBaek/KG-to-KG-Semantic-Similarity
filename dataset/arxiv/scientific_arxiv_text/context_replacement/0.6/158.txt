set classification was introduced with the main application to the fields of pattern recognition and information retrieval [ 1 ] . however , it has inspired many researchers , and their contributions to real - life problems are of a great interest . simpson [ 1 ] introduced the fuzzy min - point method ( fmm ) , which provides the best way to classify hyperboxes by a measure of belongingness to a fuzzy box , which is known as the membership function . hyperbox is a fuzzy box , usually characterized by min and max points . fmm classification methods are generally consistent with the use of the membership function . along with this first step , [ 2 ] simpson introduced the idea for a fuzzy tree , among which , the branching , overlapping , and branching curves have proved to be of a great interest to the scientific community . simpson also introduced a cluster algorithm for fmm , [ 3 ] . so many problems in real - life involve set classification and cluster . to solve this problem , gfmm [ 4 ] provides this solution . besides this , the most important problem has proved to be related to the membership function . the fuzzy membership function measures the belongingness to the hyperbox such that the expected value increases rapidly as we move away from the hyperbox . .targets of fmm are the patterns belonging to this region , where the risk of misclassification is much higher . the theta value , theta ( @ xmath0 ) , which determines the size of the hyperbox , has a large effect on this particular problem . smaller theta values cause more error and higher the accuracy , so the accuracy of the method gets reduced , and for larger theta values , accuracy gets reduced . several approaches were introduced to solve this problem . first , the method of matching [ 3 ] [ 4 ] was introduced , which attempted to represent all the overlapping regions . this method solved the previous problem of representing patterns not belonging to any of the hyperbox , in effect increasing the accuracy . exclusion / inclusion - fuzzy ( hefc ) method was introduced in [ 5 ] , which significantly reduced the number of hyperboxes and increased the accuracy . inclusion hyperboxes were used to represent patterns belonging to the same region , and exclusion hyperboxes were used to represent the same region , just as if it were a hyperbox . this method is used as it is in almost all the newly introduced models [ 6 ] [ 7 ] [ 8 ] [ 9 ] . the min - max neural network , with compensatory functions (fmcn ) further improved with [ 7 ] . authors divided the region into three categories , containment , full containment , half containment and no overlap , and suggested a new membership function to determine belongingness based on the overlap size . they also showed that by taking care of this they also related the overlap to the hyperbox size factor , @ xmath0 . an analysis based on pseudo - random neural network ( dcfmn ) [ 8 ] also improved upon fmcn . they solved the problem of overlap size . they also suggested a new membership function based on shape , data structure and data size of the hyperbox . wherein dcfmn improved the algorithm in some cases , there were some new problems . * * dcfmn introduced two new noise suppression variables , @ xmath1 and @ xmath2 . @ xmath1 is used to decrease the size of the noise and @ xmath2 is used to increase the convergence rate of the membership function . these suppression variables significantly improve the accuracy of the algorithm and therefore , determining their values is a difficult task . * there is an implicit assumption that noise in all the hyperboxes is small , which may not be true . however , the accuracy ofthe confidence interval plays a role as well . * mlf shows that this particular method is not universally applicable , and that , it does not work well for the population of neurons belonging to a region . multi - region - spanning random neural network ( mlf ) [ 1 ] addresses the problem of overlapping regions with an alternative approach . it has different parameters for different regions , and monotonically increases the hyperbox size ( @ xmath0 ) . for all cases , mlf has 100 % testing accuracy . since mlf is a technical method , the testing accuracy is much more important than testing quality , and it often limits the application of the algorithm to practical applications . in this paper , we introduce and test a large overlapping region , where misclassification rate is high . to the best of our knowledge , this type of algorithm is used for the first time , at which we have not come across any such previous methods . then we propose a method , based on regression analysis , to evidentially show that testing this newly defined degree of difference between hyperboxes of different size significantly improves the testing accuracy . the paper is presented as follows . mlf is described in section 1 . we describe the - mlf algorithm in .3 . the initial description and the implementation of m - mlf with mlf algorithm are given in sections iv and v , respectively . finally , it is given in section vi . multi - dimensional - array random neural network ( mlf ) is a method which successfully avoids misclassification of patterns relative to other patterns while preserving the underlying structure , which is a binary tree [ 1 ] . in mlf development phase , algorithms are repeatedly used to find the hyperboxes and nodes , each iteration results in a depth . this search process is repeated till the predefined depth is and then it grows . hyperbox size , based on hyperbox size and depth ( @ xmath0 ) , is determined by equation ( 1 ) and expansion is carried out by equation ( 2 ) . @ xmath3 @ xmath4 where , @ xmath5 and @ xmath6 are the node and node dimensions of hyperbox _ a _ and , @ xmath7 is the @ xmath8 dimension of and _ a _ and _ b _ is the number of nodes . then , according to each iteration , @ xmath0 is .in step ( 1 ) @ xmath9 where , @ xmath10 and @ xmath11 stand for current level and previous level , respectively and @ xmath12 , with the value between 0 and 1 , indicates that membership of hyperbox of the pattern is higher than the current level . in the first step , overlap regions are first used recursively , to find the subnet to which a particular pattern belongs to . then , at that point , the class of hyperbox with common membership , with the hyperboxes of the appropriate subnet , is selected as a candidate class . mlf is able to achieve higher learning rate than previous fmm methods . this is equivalent to an optimal solution to the problem of the first class . however , after testing , there is no method for finding another class . the region where the overlap is sufficiently large in which , it is possible to find a class with high degree of membership . as per our results , mlf , and all the previous methods , did not perform well in this region . therefore , a solution of this particular problem , and a method to solve it are needed . in this section , we give examples of the new classification problem , namely , we have a large overlap region .similar to machine learning and finding a solution to help solve the cases related to it . _ figure 1 _ in the d - mlf method , each level of the @ xmath13net has two segments , hyperboxes segment ( bs ) and information segment ( ols ) . bs represent hyperboxes located at that level , whereas ols represent information at that level . along with hyperbox ##es , problem classification ( dc ) . _ figure 2 _ illustrates the process of problem classification in mlf and d - mlf . we define a boundary region that exists between the two hyperboxes , where , according to our model , the rate of misclassification is very low . in the dc method , the boundaries of mlf are defined , in addition to this , we conduct experiments with the data , to test the classification accuracy of the existing boundary region . similar to the mlf classification method , d - mlf classification @ xmath14 uses bs and ols respectively . first , all the data is passed through , resulting in creation and destruction of hyperboxes using equations ( 1 ) and ( 2 ) . then each hyperbox is filled with the creation of hyperboxes to solve the problem using equations (( ) . @ xmath15 where @ xmath16 and @ xmath17 are the n points and @ xmath18 and @ xmath19 are the average values of the n hyperboxes , among which one is found . finally , d - mlf adds a further step to the learning process , known as data size ( dc ) computation , where each of the training patterns belonging to each hyperbox is computed throughout the algorithm . it is computed as follows : @ xmath20 where @ xmath21 is the data size of the @ xmath22 hyperbox , @ xmath23 the number of patterns belonging to @ xmath22 hyperbox and @ xmath24 is the @ xmath8 pattern in @ xmath22 hyperbox . if there is an overlap , patterns belonging to the same pattern are then sent to the nodes , where cas and ols training takes place for the next iteration . this phase of learning is repeated repeatedly to learn all the patterns . due to lack of ols and generation of training errors due to ols , d - mlf and mlf are not high level algorithms . in particular , if the pattern sets arethe first level , the overlap region has to be sorted three times . then , at the second level , data belonging to that region are sorted in order of order of number of samples in that region . this is a surprising result , and contrary to what mlf authors have stated [ 2 ] . after that , the data belonging to that region are not part of the dc decision . this method makes sure that sorting and testing for more than one level are included in the dc decision process . + train = d - mlf - train ( train , @ xmath0 ) + @ xmath25 @ xmath26 = h . sample / h . membercount result : h . sample + = sample ; h . membercount + = sample ; create new hyperbox h ; h . sample = 1 ; h . membercount = 1 ; sdata = samples which are in this class ; h . sample - = 1 ; h . membercount - = 1 ; create new hyper - box h @ xmath27 and add to ols @ xmath28 = d - mlf - train ( sdata , @ xmath29 ) ; add @ xmath27 to @ x##math28 with : @ xmath30 ; @ xmath31 the # mlf is a decision ##al model for the subnets involved . the selected subnet need not be a single node in the network . we do not change this model , but change the model of how subnet makes the decision . a function explained in the equation ( 11 ) is used against the boxes . by recursively applying the ols an additional subnet is selected , to which the box belongs to . the same function explained in the equation ( 12 ) is used , this time , to compare the boxes with the hyperboxes of the selected subnet . @ xmath32 \ \ [ ( - ( ( a _ h ^ i - a _ i ^ i , \ gamma _ i ) ] ) ) \ \ f ( x , \ gamma ) = \ \ { cases } { x } \ ; \ ; \ ; \ ; if \ ; x \ gamma \ ; > \ ; if \ \ { cases } \ ; \ ; \ ; \ ; if \ ; 1 \ ; \ leq \ ; x \ gamma \ ; \ leq \ ; > \ \ { cases } \ ; \ ; \ ; \ ; if \ ; 1\ ; \ ; < \ ; < \ begin { cases } \ begin { cases } \ ] ] where @ xmath33 is belongingness of sample @ xmath34 with @ xmath35 hyperbox . @ xmath36 is the distance between sample and the point with . @ xmath34 and @ xmath37 is the boundary value to represent sample . using these membership values , hyperboxes with these membership values are used to select the pattern . the area of these hyperboxes , represented by @ xmath38 , is known as the boundary region . @ xmath38 is a user defined parameter , stored in the membership value . at this point , it is possible to determine if the pattern belongs to the boundary region . we define @ xmath391 and @ xmath392 as the distance between the pattern and the hyperboxes , respectively . membership value is calculated as follows : @ xmath40 ##1 , based on the membership value , the pattern is selected . if it is in the area outside of the boundary region , we can use the method of mlf , and select the pattern based on the selected membership value , which is also calculated . if the pattern belongs to the boundaryfirst , the distance [ x ] between the edge and the respective nodes of the two hyperboxes is denoted . then , depending on the density matrix , the size of the algorithm is defined as either the number of nodes @ xmath41 among all the hyperboxes , or as the product of the sizes of the remaining two hyperboxes @ xmath42 where @ xmath43 is given by ; @ xmath44 where @ xmath45 is the @ xmath8 . ##ifier for the test sample in @ xmath46 subnet , @ xmath47 the size of subnet @ xmath46 and the first overlap box that enters the subnet if test sample is in this overlap box . and @ xmath48 is the size of ols , which is given by equation ( 10 ) @ xmath49 where @ xmath50 the number of overlap boxes of ols and @ xmath51 is a multiple of the @ xmath35 overlap box for test sample @ xmath34 , given by equation ( 11 ) @ xmath52 and @ xmath53 , given by equation ( 12 ) @ xmath54 where @ xmath55is the average distance distance amongst samples @ xmath46 and the sample size of the sample @ xmath8 hyperbox : min ( min ) @ xmath56 in = d - mlf - test ( mv , sample ) + @ xmath25 out = d - mlf - test ( @ xmath28 , sample ) ; = min ; min = [ ] ; min + = max ( mv , @ xmath57 ) ; = [ min ( mv ) , max ( min ( , @ xmath58 ) ( mv ) ) ) ] ; = eudistance ( mv , h1 . dc , h2 . dc ) ; min = max ( mv ) . class ; min = max ( mv ) . class ; @ xmath31 in this figure , we show the structure of the data model , each corresponding to the design and handling of the data sample of interest . _ _ _ _ is the multi - diamentional data model . we have 6 data samples for design and 6 data samples for handling . hyperbox size parameter ( @ xmath0 ) is fixed at 0 . 5 and sample size parameter ( @ xmath38 ) is fixed at 100 % . the mlf andd - mlf considers the hyperboxes in @ xmath59 layers . d - mlf also has two points ( values ) for each hyperbox , @ xmath60 and @ xmath61 . therefore , the values of @ xmath60 and @ xmath61 are @ xmath62 and @ xmath63 , respectively . patterns which do not belong to boundary region are classified correctly by mlf . but when it comes to boundary region , it fails to properly classify the patterns . whereas the standard d - mlf does well in the boundary region very well , then the classification made is not only based on the data structure , but it also the data structure . it can be shown that the patterns in the above example are not evenly spread out . which is a very common problem in real - world models . this is because of the nature of the data such as size , the distribution of the data , etc . due to them , most of the time , the patterns in the linear hierarchy , and in case of the # ##kowski pattern , the hyperboxes , can not be evenly distributed in all the layers . as mentioned above , our hierarchical model treats them uniformly , withoutregardless of the changes to the quality of the data . validity of this method ( d - mlf ) is determined on the basis of the error rate . several experiments were carried out to test d - mlf on the same datasets . several datasets such as iris , iris , iris , western breast cancer ( west ) , western breast breast cancer ( wdbc ) and others were used . these datasets were taken from the uci collection of machine learning experiments [ 11 ] . in these experiments , hyperbox size limit ( @ xmath0 ) is defined as 0 . 1 , 0 . 2 and 0 . 6 . this allows to compare the quality of the data . as we increase the size of the hyperbox , the number of errors increases , and so does the misclassification rate . we select the data sets for training and testing . the test results are calculated for each iteration . for each iteration , training and testing data are selected randomly . _ table _ _ table here , we compare our method to mlf method , and it has been consistently shown to be better than the previous generation fmm method [ 11 ] . . . [ values = " ^ , ^ , ^ , ^ " , values = " ? ", ] in this paper , we propose a data boundary region and cluster - mlf based method to identify patterns related to that boundary region . the cluster - based method , d - mlf , studies effects of clusters and other patterns in decision making . it has been evidentially shown that the method exceeds all the other available fmm methods . most importantly , we have developed a method suitable for applications in the real world , at the state of the art . d - mlf can help us in areas such as robotics , natural language processing , automated learning , etc . " . a . zadeh , information systems , command and control , vol . 4 , pp . 338 - 353 , 1965 . p . k . simpson , fuzzy min - max neural networks . ii , ieee trans . neural networks , vol . 10 , pp . 776786 , feb . 1992 . simpson , p . k . , fuzzy min - max neural networks - part 1 : classification , ieee trans . systems journal , 3245 1993 . a . gabrys and a . bargiela , a fuzzy min - max neural network for analysis and classification , ieee trans . neural networks , vol . 10 , pp . 76978##3 , 2000 . bargiela , m . pedrycz , and y . tanaka , an inclusion / exclusion - hyperbox approach , int . of intell . , vol . 34 , no . 4 , pp . 9198 , 2004 . m . rizzi , m . panella , and m . a . m . mascioli , adaptive fuzzy min - max networks , ieee trans . neural netw . 12 , pp . 402414 , jan . 2006 . s . nandedkar and s . k . biswas , adaptive fuzzy min - max neural network design with compensatory linear models , ieee trans . neural netw . 12 , pp . 4254 , jan . 2007 . y . zhang , y . liu , y . tanaka , and y . wang , multi - set - level fuzzy min - max networks for fuzzy systems , ieee trans . neural netw . 12 , pp . 23392352 , jan . 2008 . davtalab , m . a . dezfoulian and m . mansourizade , multi - level fuzzy min - max neural network design , ieee trans . neural netw . 12 , pp . 470 - 481 , 2009 . m . bezd##el and d . j . smith , development of the analysis and design of learning by comparison with multi - dimensional data , proc . 2060 - 2066 , nov . 2013 . bache and a . lichman . , uci distance learning project , uci inf . center , irvine , ca , nov . , 2013 . [ not available ] http : / / www . archive . uci . edu / .