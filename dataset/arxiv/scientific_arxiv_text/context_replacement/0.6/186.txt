prior estimation algorithms based on probability theory have received much attention in high - dimensional computer science . a related development is due to the development of @ xcite , which introduces the @ xmath0 - 1 penalty . @ xcite has pointed out that the posterior penalty can be viewed as the variance of the posterior distribution . similarly , the @ xmath1 penalty can be transformed into the posterior distribution . moreover , this penalty can be viewed as a probability scale function . this has also led to further development of the algorithm and its successor @ xcite . there has also been work on nonconvex estimation as a general statistical approach . @ xcite developed their random variable estimation ( lla ) algorithm by combining the expectation generation ( hmm ) algorithm with an inverse gamma transform . in particular , they showed that the @ xmath2 penalty with @ xmath3 can be reduced by replacing the posterior distribution with a probability distribution . further studies have shown that the likelihood resulting from this reduction , called the nonconvex maximum likelihood and defined as : ( [ eqn : logp ] ) above , has an interpretation as a linear combination of probability distributions with an inverse gamma transform : @xcite . later , @ xcite extended this class of normal variance ##s by introducing a new normal variance mixture algorithm . other algorithms include the inverse hyper - transform @ xcite , the inverse prior @ xcite and the dirichlet - transform @ xcite . in addition , various statistical methods have been applied to feature selection @ xcite . for example , in the inverse - poisson process @ xcite , the coefficients are used to select non - real - valued variables , providing a general linear feature selection algorithm in an applied statistical context . the wiener - fisher algorithm is a general statistical algorithm for sampling @ @ xcite . first , @ xcite developed a general framework for normal variance sampling and showed that the algorithm is closely related to lvy ##s . early on , @ xcite constructed random variables as sets of subordinators , which transform finite ##ly normal variance distributions into normal distributions . thus , this provided a general framework for the construction of variance - based models . also , @ xcite introduced the class of @ xmath4 - 1 subordinators and .- - subordinators for the probability distribution of the variables . @ xcite describes the intersection of two nonconvex probability measures , which are referred to as p and exp and denoted as : ( [ eqn : logp ] ) and ( [ eqn : exp ] ) respectively , with the fourier transforms of the gamma and poisson subordinators . each subordinator has a one - dimensional lvy measure that is almost always non - negative @ xcite . in this section we also discuss the use of subordinators to solve nonconvex penalty problems in supervised learning algorithms . continuing from the previous section , we consider supervised learning problems with subordinators which are defined as limiting values of the measure . in particular , we consider two types of compound poisson subordinators : continuous compound poisson subordinators based on a continuous random variable @ xcite and discrete compound poisson subordinators based on a discrete random variable @ xcite . the two lvy measures are equivalent to @ xcite and poisson measure , respectively . we assume that both the gamma and poisson subordinators are the measures of these .properties of the compound poisson subordinators . since the fourier transform of each subordinator is a fourier transform , we have two families of nonconvex penalty functions , whose limiting cases are the nonconvex log and exp . thus , these two families of nonconvex penalty functions can be obtained via substitution of log and exp , while the continuous and discrete compound poisson subordinators are composed of log and poisson processes . note that the local ##ization parameter is a limiting case of the gamma parameter . we have a hierarchical model with local ##ization parameters , giving rise to a sparse model for nonconvex problems . to reduce the complexity , we use the ecme ( for efficient / efficient " optimization " ) algorithm @ xcite which can adaptively compute the local ##ization parameters of all the hierarchical models above . the content of the paper is summarized as follows . section [ sec : abs ] describes the applications of lvy processes to numerical machine learning problems . in section [ sec : abs ] we have two families of compound poisson processes . in section [ sec : blrm ] we apply the lvy process to numerical linear programming and to##se - ecme algorithm for computing the identity matrix . we begin our work on the algorithm in section [ sum : experiment ] , and conclude our work in section [ sum : experiment ] . our work is based on the notion of bernstein and includes bernstein functions as well as subordinators . compare @ xmath5 with @ xmath6 . the function @ xmath7 is said to be completely bernstein if @ xmath8 for all @ xmath9 and bernstein if @ xmath10 for all @ xmath9 . roughly speaking , the _ subordinator _ is a finite - dimensional lvy function that is non - decreasing almost everywhere . our work is primarily motivated by the notion of subordinators discussed in section [ sum : subord ] @ xcite . [ sum : subord ] if @ xmath11 is a subordinator , then the fourier transform of its inverse has the form @ xmath12 where @ xmath13 is the inverse of @ xmath14 and @ xmath15 , depending on @ xmath16 , is referred to as the _ fourier transform _ of the subordinator and has the form .@ xmath17 \ [ ( [ ##x ) . \ ] ] . @ xmath18 and @ xmath19 are the lvy ##s of that @ xmath20 . similarly , if @ xmath15 is an integral function from @ xmath21 defined in expression ( [ eqn : psi ] ) , then @ xmath22 is the fourier transform of the image of the subordinator . it is also known that the fourier transform @ xmath15 is bernstein and the corresponding fourier transform @ xmath23 is also bernstein for example @ xmath24 @ xcite . similarly , the function @ xmath25 , with @ xmath26 , is a bernstein function if and only if it contains the function as an integral ( [ eqn : psi ] ) . similarly , @ xmath15 as defined in expression ( [ eqn : psi ] ) = @ xmath27 . as a result , @ xmath15 is additive , nondecreasing and dependent on @ xmath16 . we are given a set of input functions @ xmath28 , where the @ xmath29 .the error vector and the @ xmath30 are the noise parameters . we will consider the following linear regression coefficients : @ xmath31 where @ xmath32 , @ xmath33 ^ [ $ ] , and @ xmath34 is the corresponding error vector @ xmath35 . we aim at obtaining a linear estimate of the values of regression coefficients @ xmath36 s using a linear nonconvex model . in particular , we obtain the following linear estimates for the regression coefficients @ xmath37 s : @ xmath38 & \ stackrel { iid } { \ sim } , ( \ alpha _ 1 ) , \ \ \ sigma & \ sim \ sigma ( \ alpha _ { \ beta } / 1 , \ alpha _ { \ beta } / 2 ) , \ sim { p } \ ] ] where the @ xmath39 parameters are referred to as the noise parameters , and the corresponding linear model has the following parametrization : @ xmath40 here , we define @ xmath39 and @ xmath41 , that is , @ xmath42 . here @ xmath43 is used as a subordinator . here @ x##math44 , defined by @ xmath16 , is the fourier transform of the subordinator . for @ xmath45 , it can be shown that @ xmath46 is a nonconvex penalty function of @ xmath47 on @ xmath48 . however , @ xmath46 is nondifferentiable at the origin because @ xmath49 and @ xmath50 . therefore , it is possible to prove that . in this case , @ xmath51 defines the prior for @ xmath47 . from the [ 1 : subord ] it follows that the prior can be obtained via the fourier transform . in particular , we have the following result . [ 1 : lapexp00 ] let @ xmath15 be a nonzero penalty function of @ xmath16 . if @ xmath52 , then @ xmath46 is a nondifferentiable and nonconvex function of @ xmath47 on @ xmath53 . then , @ xmath54 where @ xmath43 is the subordinator . note that @ xmath14 is defined as the prior .for @ xmath55 and in the [ ref : blrm ] we can see that @ xmath56 plays the same role as the tuning parameter ( or local hyperparameter ) . therefore , there is an exact relationship between the local expansion parameter and the local tuning parameter ; that is , @ xmath57 . because @ xmath58 , the local tuning parameter @ xmath39 corresponds to the local mixing parameter @ xmath59 . thus we have a simple prior model for the local tuning parameters @ xmath39 and . it is also worth pointing out that @ xmath60 where @ xmath61 denotes the probability distribution with mean given by @ xmath62 , and @ xmath63 denotes the probability distribution of the random variable ( or @ xmath64 ) . thus , we have the corresponding prior @ xmath65 for @ xmath47 . thus , this prior can be written as the posterior distribution for , i . e . , the posterior of @ xmath66 with the parameter @ xmath67 . if @ xmath68 , then @ xmath6##9 is not a normal distribution . obviously , @ xmath70 is not associated with a prior of @ xmath47 . alternatively , we can consider @ xmath70 as the prior of @ xmath66 with prior of @ xmath67 . in this case , we use the method of quasi - sampling for the distribution , which is also used in @ xcite . however , @ xmath71 is improper . this is an interesting case , because we assume that @ xmath72 , @ xmath73 and that @ xmath74 , where @ xmath75 is the cumulative distribution function of @ xmath56 , which leads to the additional condition @ xmath76 . we can solve this case by taking @ xmath77 in general ( [ eqn : 1 ] ) to be a strictly positive gamma function . in addition , we can use the condition @ xmath78 . this in turn leads to @ xmath77 leading to @ xmath79 . in this case we use the method for nonconvex distribution functions . for this case , we can only use a single##ordinator without density , i . e . , @ xmath77 . equivalently , we can show that @ xmath80 . we can define the nonconvex log and exp penalties as two simple functions ( sometimes called * ? ? ? the log penalty is defined by @ xmath81 and the exp penalty is defined by @ xmath82 ) , these two penalties are : and @ xmath16 . therefore , they are @ xmath27 and @ xmath83 . it is also easily shown that @ xmath84 \ { ( du ) } , \ ] ] where the lvy distribution @ xmath19 is given by @ xmath85 the corresponding subordinator @ xmath86 is a proper subordinator , because if @ xmath14 has a mixing distribution with density @ xmath87 , with density given by @ xmath88 we can show that the corresponding pseudo - prior is given by @ xmath89 and , if @ xmath90 , the pseudo - prior has a mixing distribution , which is the mixture of @ xmath91 with the distribution @ xmath92 . .for the exp measure , the lvy measure is @ xmath93 . since @ xmath94 { \ } = \ infty , \ ] ] , @ xmath95 $ ] is the integral measure of @ xmath47 . thus , @ xmath96 is a poisson subordinator . similarly , @ xmath14 is a poisson process with intensity @ xmath97 of values from the distribution @ xmath98 . that is , @ xmath99 which we denote as @ xmath100 . in this section we discuss the application of the poisson subordinators to various nonconvex probability distributions . let @ xmath101 be the set of discrete and normally distributed ( i . i . d . ) - independent random variables with power of @ xmath102 , and let @ xmath103 be a poisson process with density @ xmath104 that is independent of all the @ xmath105 . then @ xmath106 , for @ xmath24 , is the compound poisson process with intensity @ xmath107 ( for @ xmath10##8 ) , and so @ xmath43 is also a compound poisson process . a compound poisson process is a subordinator if and only if the @ xmath105 are compound random variables @ xcite . it is worth pointing out that if @ xmath109 follows the poisson subordinator ##s as above ( [ eqn : possion ] ) , this is equivalent to saying that @ xmath14 follows @ xmath110 . we now have two families of compound random variables @ xmath111 : discrete compound random variables and continuous gamma random variables . thus , we have discrete and discrete compound poisson subordinators @ xmath109 . we can show that both the discrete and poisson subordinators are unique properties of the compound poisson subordinators . in the first case @ xmath111 is a compound random variable . in particular , let @ xmath112 and the @ xmath111 be i . i . d . from the @ xmath113 family , where @ xmath114 , @ xmath115 and @ xmath116 . thethe poisson subordinator can be written as : @ xmath117 the variance of the subordinator is then given by @ xmath118 we denote it by @ xmath119 . the mean and variance are @ xmath120 respectively . the gamma measure is given by @ xmath121 where @ xmath122 is a continuous function of the variable @ xmath123 . \ ] ] the generalized lvy measure is given by @ xmath124 note that @ xmath125 is the gamma function for the random variable @ xmath126 . thus , the lvy measure @ xmath127 is referred to as the generalized gamma measure @ xcite . the gamma measure @ xmath128 is given by @ xcite for statistical analysis . here , we discuss its use in mathematical analysis . it is known that @ xmath128 for @ xmath114 and @ xmath116 satisfy the conditions @ xmath129 and @ xmath130 . moreover , @ xmath131 is a continuous and nonconvex function of @ xmath47 .@ xmath48 , and it is an intersection function of @ xmath132 and @ xmath133 . therefore , @ xmath131 is a c . i . d . @ xmath47 is nondifferentiable at the origin . this means that @ xmath131 can be interpreted as a norm - preserving function . we are assuming as the limiting cases that @ xmath134 and @ xmath135 . [ pro : first ] let @ xmath136 , @ xmath128 and @ xmath137 be given by : ( [ eqn : first _ tt ] ) , ( [ eqn : first ] ) and ( [ eqn : second _ tt ] ) , respectively . then 1 . @ xmath138 and @ xmath139 ; 2 . @ xmath140 and @ xmath141 ; 3 . @ xmath142 and @ xmath143 . this result can be verified by using simple numerical methods . the [ pro : first ] tells us that the limiting cases are the nonconvex ex and exp functions . thus , we havethat @ xmath14 differs in distribution to a normal random variable with mean @ xmath144 and variance @ xmath145 , as @ xmath146 , and to a poisson random variable with variance @ xmath144 , as @ xmath147 . it is also known that @ xmath148 belongs to the corresponding function @ xcite . thus we have shown that @ xmath122 tends to exp at @ xmath147 . we have a special case in equation [ 1 : 2 ] when @ xmath149 . we refer to the corresponding function as the _ mean - variance _ ( lfr ) function . for notational convenience , we can replace @ xmath150 and @ xmath151 by @ xmath145 and @ xmath152 by the lfr function . the definition of the subordinator for the lfr function is given by @ xmath153 we now assume that @ xmath14 is a random ##ized distribution without scale @ xcite , which is a mixture of a normal delta distribution and a normal beta distribution @ xcite . we have thedefinition of @ xmath14 & @ xmath154 . lllll & bernstein functions & lvy & @ xmath137 & subordinators @ xmath14 & + + + & @ xmath155 & @ xmath156 & @ xmath157 & [ @ xmath158 + exp & @ xmath159 $ ] & @ xmath160 & @ xmath161 & improper + lfr & @ xmath162 & @ xmath163 & @ xmath164 & improper + cel & @ xmath165 $ ] & @ xmath166 & @ xmath167 & + + + + in the first step , we have a family of pseudo - poisson subordinators . first , @ xmath111 is compact and has values at @ xmath168 . and it is given by bernstein functions @ xmath169 , where @ xmath170 and @ xmath171 , with the density of given by @ xmath172 then , we let @ xmath173 be .poisson distribution with mean @ xmath174 , where @ xmath114 . then @ xmath14 is distributed similarly to the standard normal ( normal ) distribution @ xcite . the probability density function of @ xmath14 is given by @ xmath175 which is given by @ xmath176 . we also show that @ xmath14 has the same subordinator . then @ xmath177 and @ xmath178 . it can be shown that @ xmath179 has the same mean and variance as the @ xmath119 distribution . the fast fourier transform then gives rise to a new family of bernstein functions , which are given by @ xmath180 . \ ] ] we refer to this family of bernstein functions as _ _ exp - _ _ ( cel ) _ . the first - order measure of @ xmath181 i . i . t . @ xmath182 is given by @ xmath183 the lvy measure for @ xmath181 is given by @ xmath184 the second is given by figure 1 . we call this lvy measure the _ - .##isson _ _ similar to the usual hash function . for @ xmath128 , @ xmath181 can satisfy a set of inequality - preserving nonconvex conditions . similarly , @ xmath181 for @ xmath114 , @ xmath185 and @ xmath116 satisfy the conditions @ xmath186 , @ xmath187 and @ xmath188 . we define a new cel for @ xmath189 as well as the values @ xmath14 and @ xmath137 to form [ tab : 1 ] , where we replace @ xmath190 and @ xmath150 with @ xmath152 and @ xmath145 for notational convenience . we will consider the following examples . [ tab : 1 ] . @ xmath137 is defined as : ( [ eqn : x _ 1 ] ) for both @ xmath185 and @ xmath116 . then we find that 1 . @ xmath191 and @ xmath192 . @ xmath193 and @ xmath194 . 2 . @ x##math195 and @ xmath142 . 2 . @ xmath196 and @ xmath197 imply that @ xmath198 this implies that @ xmath127 tends to @ xmath199 , hence @ xmath200 . finally , we get the second part of proposition [ pro : 8 ] - ( 1 ) , which implies that for @ xmath200 , @ xmath14 ##2 in relation to a uniform random variable with shape parameter @ xmath144 and size parameter @ xmath145 . an alternative proof is given in section 2 . proposition [ pro : 8 ] implies that @ xmath181 tends to exp as @ xmath147 , and to log as @ xmath200 . this is an obvious relation between @ xmath128 in expression ( [ eqn : first ] ) and @ xmath181 in expression ( [ eqn : second ] ) ; that is , they have the same conditional probability . we find that for @ xmath201 , @ xmath202 \ ] ] which is a combination of the log and exp functions ,and that @ xmath203 \ ] ] which is the product of the exp and bernstein functions . in particular , the composition of the two bernstein functions is a constant . therefore , the product is called the maximum product of the subordinator , which is also the composition of the subordinators corresponding to the other two bernstein functions @ xcite . this leads us to an equivalent definition for the subordinators corresponding to @ xmath122 and @ xmath204 . that is , we have the following theorem whose proof is given in appendix a . [ lim : poigam ] the subordinator @ xmath14 associated with @ xmath128 is distributed according to the mixture of @ xmath205 distributions with @ xmath206 mixing , and @ xmath14 associated with @ xmath181 is distributed according to the mixture of @ xmath207 distributions with @ xmath208 mixing . thus , the following theorem gives a general ##ization of the subordinators : @ xmath145 = 0 . [ lim : lim ] let @ xmath209 be a positive integer and @ xmath210 $] . 1 . if @ xmath211 where @ xmath212 $ ] or @ xmath213 , then @ xmath14 converge in probability to @ xmath56 , as @ xmath214 . 2 . if @ xmath215 where @ xmath216 \ ] ] or @ xmath213 , then @ xmath14 converge in probability to @ xmath56 , as @ xmath214 . the proof is given in table a . since @ xmath14 converge in probability to @ xmath56 " and @ xmath14 converge in probability to @ xmath56 , " we have that @ xmath217 . , consider the two nonconvex special cases shown in table [ 1 : 2 ] . we have the following example . that is , when @ xmath213 and for a given @ xmath116 , we have @ xmath218 \ leq \ frac { 1 } { \ gamma } { + } \ } \ leq \ frac { 1 } { \ gamma } [ s { - } \ exp ( { - } \[ ( ) ] \ leq \ frac { 1 } { \ gamma } \ , \ , ( { \ gamma } ^ { + } , \ , ) \ leq \ , \ ] ] with \ , when @ xmath219 . the proof is given in section 2 . this proof is also given in table [ sec : test ] . [ table [ sec : test ] with @ xmath220 and @ xmath71 . ] we apply the generalized poisson subordinators to the general linear search problem described in table [ sec : test ] . with @ xmath221 , we define the matrix formula for the joint solution of the @ xmath37 in the above table . that is , we assume that @ xmath222 & \ stackrel { ind } { \ sim } & \ ( eta _ j | ) , \ sim ( ( \ eta _ j ) ^ { - 1 } ) , \ \ f _ { j ^ { * } ( t _ j ) } ( \ eta _ j ) & { \ propto } & \ eta _ j ^ { - 1 } f _ { * ( t _ j ) } ( \ eta _ j ) ,\ begin { s } \ ] ] which means that @ xmath223 the gamma - beta - distribution of the @ xmath37 s is given by @ xmath224 we can prove by using [ x : s ] that the full conditional distribution @ xmath225 is symmetric . therefore , the p _ a posteriori _ ( s ) distribution of @ xmath226 is based on the following optimization algorithm : @ xmath227 here , the @ xmath59 s are the distribution parameters and the @ xmath228 s are the distribution parameters . also , it is known that @ xmath43 ( or @ xmath55 ) is used as a subordinator w . r . t . @ xmath56 . the full conditional distribution @ xmath229 is in w . r . t . the form , which is @ xmath230 . therefore , it is an inverse conditional distribution of the form @ xmath231 . \ ] ] in the first step , we found an inverse distribution of the form @ xmath232 ( i . e . , @ xmath233 ) . therefore , @xmath229 is not the probability distribution function in this case . however , based on @ xmath234 \ int _ { j = 1 } ^ { \ exp ( - \ frac { \ int _ 1 } { \ sigma } | b _ j | ) \ vadjust { \ sigma } \ ] ] and the definition of [ [ ] : \ ] ( see figure 1 ) , we know that the conditional distribution @ xmath235 is proper . however , the conditional distributions @ xmath236 satisfy the assumption of @ xmath237 only . therefore , the power series method is not yet available and we return to the prior method to find the error . note that if @ xmath238 is proper , the conditional error distribution is given by @ xmath239 _ | b _ j | = [ \ int _ { j } ^ { \ infty } \ exp \ , [ - b _ j \ , \ , ( \ frac { | b _ j | } { \ sigma } \ big ) \ , ] [ ( | b _ j | / \ big ) , \ ] ] which is independent of @ xmath240. therefore , the marginal distribution @ xmath241 is independent of the involved term . therefore , we can assume that @ xmath242 which is proper . as shown in fig [ 1 : 2 ] , except for those with @ xmath243 which can be transformed into a proper prior , the maximum likelihood function can not be transformed into a prior . in this case , our marginal distribution is not dependent on the prior t - distribution @ xmath244 . we ignore the first involved term , because it is proper if @ xmath244 is proper and it is independent of @ xmath240 if @ xmath244 is proper . using the @ xmath245th and @ xmath246 of @ xmath247 in the t - form of the # ##e , we have @ xmath248 = ( \ eta _ j | t _ p ^ { ( k ) } , \ alpha ^ { ( k ) } , p _ k ) } + \ eta _ j + \ log p ( \ sigma ) \ \ & \ propto - \ frac { 1 + \ alpha _ { \ sigma } } { 1 } \ to \ log{ - } \ frac { \ | { \ bf b } { - } { \ bf a } { \ bf b } \ | _ p ^ 2 + \ beta _ { \ sigma } } { n \ , } - ( n + 1 ) \ , \ , \ \ & \ , - \ frac { - } { \ sigma } \ beta _ { p = 2 } ^ 2 . we choose the terms that are solutions of both @ xmath240 and @ xmath226 . in particular , we only need to consider @ xmath249 for the m - step . taking that @ xmath250 and taking the terms w . r . t . @ xmath236 on both sides of the differential equation , we find that @ xmath251 the m - step is @ xmath252 w . r . t . @ xmath253 . in particular , it is known that : @ xmath254 the above convergence algorithm is similar to the fast linear approximation ( lla ) algorithm @ xcite . however , it has the same convergence conditions satisfied by @ xcite and @ xcite . subordinators allow us to compute the convergencedistinguish between the local state parameters @ xmath59 s and the global state parameters @ xmath39 s ( or @ xmath41 ) . however , when we consider the parameter definitions , it is unclear how to calculate these local ##ization parameters . we use the ecme ( for unknown / " unknown parameter " ) function @ xcite for information about the @ xmath37 s and @ xmath59 s parameters . for this purpose , we use the @ xmath59 likelihood function @ xmath255 , or , @ xmath256 because the full conditional distribution is \ sigma and given by @ xmath257 \ sim \ sim \ pi ( \ alpha _ { t } , ) / [ \ pi ( | x _ t | / \ sigma ) + \ alpha _ { t } ] \ pi ) . \ ] ] note that we cannot estimate the full conditional distribution by using the weighted log - likelihood @ xmath238 , because our estimated likelihood ##s in time [ 1 : 1 ] do not give exact estimates . however , if @ xmath238 were used , the weighted likelihood function would depend on @ xmath59 . as a result, the full conditional distribution of @ xmath59 is either no longer known or is not yet known . figure [ fig : graphal0 ] - ( right ) shows the above model for the linear inverse linear regression , and figure [ fig : alg ] shows the ecme , where the e - step and m - step are approximately equivalent to the e - step and the m - step of the above model , with @ xmath258 . the cme - method compares the @ xmath59 prior with @ xmath259 in order to be sure that @ xmath260 , it is necessary to assume that @ xmath261 . for the other case , we use @ xmath262 . we run experiments with the prior @ xmath263 for example . this prior is different from the @ xmath264 - 1 prior , but it is a normal prior . therefore , the full conditional distribution of @ xmath59 i . i . t . with correlation function @ xmath265 = \ , ; that is , @ xmath257 \ sim \ , \ , ( { \ pi _ 1 } { + } ) , \ ; \ /( { \ sigma _ i } { + } \ sqrt { | sigma _ i | / \ } } ) \ } ) . \ ] ] finally , the cme - code for implementing the @ xmath59 algorithm is given by @ xmath266 the fundamental theorem of the ecme algorithm was proved by @ xcite , who showed that the ecme algorithm preserves the monotonicity property from the original algorithm . however , the ecme algorithm based on t - code is also given by @ xcite . . the general form of the ecme is [ t = " < , < " , ] our algorithm is based on a set of three data , which are distributed according to @ xcite . in particular , we consider the following three data : small , " medium " and large . " data t : : : @ xmath267 , @ xmath268 , @ xmath269 , and @ xmath270 is an @ xmath271 matrix with @ xmath272 on the diagonal and @ xmath273 on the semi - diagonal . data t : : : @ xmath274 , @ xmath275 , @ xmat##h276 and @ xmath277 non - negative such that @ xmath278 and @ xmath279 , and @ xmath280 . data matrices : : : @ xmath281 , @ xmath282 , @ xmath283 , and @ xmath284 ( data matrices ) . for a linear model , we generate @ xmath285 data matrices @ xmath286 such that each value of @ xmath286 is drawn from a random probability distribution with parameters @ xmath287 and data matrices @ xmath270 , @ xmath288 , and @ xmath289 . we generate a linear model @ xmath290 with random probability distribution @ xmath286 and random noise . we generate @ xmath240 such that the signal - to - noise ratio ( snr ) is a constant value . in the example of @ xcite , we use @ xmath291 for generating the models . we use the model prediction error ( spe ) to estimate the model prediction error . the example ##itythat for spe is @ xmath272 . the prediction error is based on the correctly predicted ones and incorrectly predicted ones in @ xmath292 . the snr and spe are measured by @ xmath293 for each model ##set , we have training data of size @ xmath294 , plus small sample data and simulation data , all of size @ xmath295 . for each model , the non - selection errors are determined by the validation based on finding the correct prediction error . with the size @ xmath292 based on the training data , we measure spe with the validation data . this process is repeated @ xmath296 again , and we compute the mean and standard deviation of spe and the average of all - nonzero entries . we use ` ` ' ' to calculate the average of correctly predicted zero ##s in @ xmath226 , that is , @ xmath297 ; if all the nonzero entries are correctly predicted , this average should be @ xmath298 . we put the results in table [ fig : toy2 ] . it is assumed that our results in table [ fig : graphal0 ] - ( 1 )slightly better than the other two settings in the [ x : graphal0 ] - ( b ) and ( c ) in both the prediction performance and the simulation performance . however , when the size of the dataset takes large steps , the prediction performance of the second setting becomes worse . the other nonconvex settings are good , but they exceed the first . thus , we see that er , exp , lfr and cel all exceed @ xmath264 . the @ xmath264 model also suffers from the effects of rounding errors during the em ##ulation . as we see , the settings derived from lfr , cel and exp as well as those with @ xmath299 are incorrect , while the em ##s from @ xmath264 are correct . the experimental results show that these two settings work well , even better than the normal ones . [ . @ xmath300 has " r " and " p " where @ xmath301 is the basis of @ xmath302 and that @ xmath303 . ] note that in our case the case with @ xmath37 leads to a very fine structure of @ xmat##h59 . finally , it is possible to also test the linear relationship between @ xmath37 and @ xmath59 . let @ xmath304 be the value of @ xmath59 obtained from our ecme test ( alg . " ) , and @ xmath305 be the value of @ xmath306 such that @ xmath307 . " [ 1 : tb1 ] shows the case of @ xmath308 vs . @ xmath300 with f , exp , lfr and cel " = f " and data l . " we assume that @ xmath308 is a c . r . t . therefore , @ xmath308 is positive when @ xmath300 has a negative value . a similar effect is also observed for data l . " this result shows that the subordinator is a useful computational tool for data analysis . in this way we have introduced subordinators into the field of nonconvex random variables . this leads us to a new method for constructing measure - preserving semi - fields . in particular , we have proved the existence of many different poisson subordinators: the positive poisson gamma subordinator and the negative gamma subordinator . in addition , we have shown the difference between the two families of compound poisson subordinators . that is , we have shown that the two families of compound poisson subordinators have the same learning function . therefore , their values at each step have the same mean and variance . we have developed the ecme algorithm for general decision decision problems based on the nonconvex log , exp , lfr and cel algorithms . we have done the same work with the state - of - the - art algorithms . the researchers have shown that our nonconvex log approach is very useful for higher - order decision optimization . our work can be translated into a general decision problem . it is also possible to develop a general decision algorithm based on the mcmc algorithm . we would like to address this issue in our research . note that @ xmath310 & = \ log \ log [ ( - \ frac { 1 } { 1 { + } \ big } \ exp ( - \ frac { \ big } { 1 { + } \ big } \ big } ) \ big ] - \ log \let [ 1 - \ frac { 1 } { 1 { + } \ gamma } \ big ] \ \ & = \ sum _ { k = 1 } ^ { \ infty } \ frac { 1 } { k ( 1 { + } \ gamma ) ^ k } \ left [ 1 - \ exp \ big ( { - } \ frac { \ big } { 1 { + } \ gamma } ) \ big ( \ big ) \ big ] \ \ & = \ sum _ { k = 1 } ^ { \ infty } \ frac { 1 } { k ( 1 { + } \ gamma ) ^ u } \ int _ { 1 } ^ { \ infty } ( 1 - \ exp ( - - - ) ) \ int _ { \ frac { \ big } \ gamma } { 1 { + } \ gamma } } ( - ) ^ u . \ big { u } \ ] ] we also have that @ xmath311 . we also have an extended version of the [ 1 : 1 ] - ( 1 ) , which is easily obtained from the above formula . let @ xmath312 a single iteration of @xmath313 and a different gamma distribution @ xmath314 . if @ xmath315 belongs to a gamma distribution , @ xmath316 , @ xmath317 is in addition to a gamma distribution , with scale @ xmath315 and scale @ xmath272 . since @ xmath318 we have that @ xmath319 and that @ xmath320 and @ xmath321 this brings us to @ xmath322 thus , we have that @ xmath323 is a mixture of @ xmath324 with @ xmath325 and . that is , @ xmath326 , @ xmath327 , @ xmath328 and @ xmath329 , we have that @ xmath330 we can consider a mixture of @ xmath331 with @ xmath332 which is @ xmath333 . consider @ xmath334 , @ xmath335 , @ xmath336 and @ xmath337 . then , @ xmath338 since @ xmath339 =1 $ ] , we also need to consider the case that @ xmath213 . consider that @ xmath119 , whose mean and variance are @ xmath340 whenever @ xmath213 . by chebyshev ##ski theorem , we have that @ xmath341 similarly , we have that @ xmath342 similarly , we have that ( 1 ) . we also have that @ xmath343 which implies that @ xmath344 for @ xmath345 . similarly , we have that @ xmath346 \ leq0 $ ] . as a result , @ xmath347 for @ xmath345 . then for @ xmath348 , which is also derived from that @ xmath349 since @ xmath350 = \ frac { \ gamma } { \ exp ( \ gamma s ) } - \ frac { \ gamma } { [ + \ gamma s } < 1 $ ] for @ xmath345 , we have that @ xmath351 for @ xmath345 . now note that @ xmath352 \ gamma _ { s = 0 } ^= \ psi ^ { - 1 } \ exp \ big ( - t _ j \ psi \ big ( \ frac { | b _ j | } { \ sigma } \ big ) \ big ) . \ ] ] to verify that @ xmath353 is correct , one is to assume that @ xmath354 \ sum _ { i = 1 } ^ 2 \ psi ^ { - 1 } \ exp \ big ( - t _ j \ psi \ big ( \ frac { | b _ j | } { \ sigma } \ big ) \ big ) ^ { \ bf z } < \ infty } . \ ] ] it is then found that @ xmath355 \ nonumber \ \ & = \ exp \ big [ { - } \ frac { 1 } { 2 \ sigma } ( { \ bf z } { - } { \ bf z } ) ^ t { \ bf y } ^ 2 { \ bf y } ( { \ bf z } - { \ bf z } ) \ big ] \ big \ exp \ big [ - \ frac { 1 } { 2 \ sigma } { \ bf i } ^ 2 ( { \ bf i} _ { - { \ bf x } ( { \ bf x } ^ { { \ bf y } ) ^ { + } { \ bf x } ^ { ) { \ bf x } \ [ ] , \ bf { + } \ ] ] where @ xmath356 and @ xmath357 are the post - penrose - components of and @ xmath358 @ xcite . then we consider the well - known case that @ xmath359 and @ xmath360 . note that if @ xmath358 is nonsingular , then @ xmath361 . in this case , we consider a standard standard normal distribution @ xmath362 . then , we consider a conventional standard normal distribution @ xmath363 @ xcite , the variance of which is given by @ xmath364 . \ ] ] where @ xmath365 , and @ xmath366 , @ xmath367 , are the variance components of @ xmath358 . in this case , we can use @ xmath368 . therefore , @ xmath369 = { \ bf x } <\ infty } $ ] . , this is the inverse of @ xmath370 because @ xmath371 \ | _ { j = 1 } ^ p \ exp \ big ( { - } t _ j \ psi \ big ( \ frac { | b _ j | } { \ sigma } \ big ) \ big ) \ leq \ exp \ big [ { - } \ frac { - } { { \ bf } \ | { \ bf y } - { \ bf x } { \ bf y } \ | _ j ^ p \ big ] . \ ] ] we can see that @ xmath372 \ | _ { j = 1 } ^ p \ exp \ big ( - t _ j \ psi \ big ( \ frac { | b _ j | } { \ sigma } \ big ) \ big ) . \ ] ] = @ xmath373 { \ bf y } $ ] . since the matrix @ xmath374 is positive semidefinite , we have @ xmath375 . based on equation ( [ eqn : pf01 ] ) , we can find @ xmath376 \ big##propto ##de ( { \ bf z } | { \ bf x } , \ big ( { \ bf z } ^ { { \ bf z } ) ^ { + } ) { \ alpha } ( \ big | \ frac { \ alpha _ { \ sigma } { + } ^ { + } 2p { - } n } { - } , \ alpha { + } \ beta _ { \ sigma } ) . \ ] ] therefore , we find that @ xmath377 d { \ bf b } { \ big } < \ infty , \ ] ] and therefore , @ xmath377 \ alpha _ { j = 1 } ^ { \ exp \ big ( - b _ j \ big \ big ( \ frac { | b _ j | } { \ sigma } \ big ) \ big ) d { \ bf b } { \ big } < \ infty . \ ] ] and @ xmath378 is unique . therefore , we find @ xmath379 } { \ alpha ^ { \ frac { j + \ beta _ { \ sigma } + 2p } { 2 } + 1 } } \ alpha _ { j = 1} ^ p \ big \ { \ exp \ big ( { - } b _ ) \ big \ big ( \ frac { | b _ j | } { \ sigma } \ big ) \ big ) \ frac { t _ 1 ^ { { \ alpha _ t } { - } } } \ exp ( { - } { \ beta _ t } b _ j ) } { \ big ( { \ alpha _ t } ) } \ big \ } \ \ & \ triangleq \ ( { \ bf b } , \ psi , { \ bf t } ) . \ big { - } \ ] ] in this way , we have @ xmath380 } { \ gamma ^ { \ frac { - + \ gamma _ { \ sigma } + 2p } { - } + 1 } } \ gamma _ { n = 1 } ^ { \ frac { - } { \ big ( { \ alpha _ t } { + } \ psi \ big ( \ frac { | b _ j | } { \ sigma } \ big ) \ big ) ^ { { \ alpha _ t } } } ^ { \ bf t } { \ big } . \ ] ] similar to the above example, we also know that @ xmath381 because @ xmath382 . as a result , @ xmath383 is true . now , consider the case that @ xmath384 . that is , @ xmath385 and @ xmath386 . in this case , if @ xmath387 , we have @ xmath388 and @ xmath389 . as a result , we have the same probability for @ xmath390 . therefore , the following still hold . polson , m . j . and bernardo , j . g . ( 2010 ) . ` ` act globally , not locally : using statistical inference and optimization . ' ' in bernardo , j . g . , bayarri , m . a . , berger , m . a . , dawid , j . a . , heckerman , m . , smith , a . k . a . , and bernardo , j . ( eds . ) , _ _ _ _ _ . cambridge university press . the authors would like to thank the editor and the other editors for their helpful comments and comments in the original version of this article . the authors would also like to thank the other editors forwith some detailed comments on earlier works . this book has been published in china by the natural history society of china ( no . 61070239 ) .