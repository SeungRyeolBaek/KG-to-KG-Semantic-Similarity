given suitable statistical models to model the system , we wish to determine whether our models are consistent with the appropriate model . " selecting models involves developing and applying time series methods to the data values and then applying an optimization algorithm to select a ` good ' model @ xcite . however , it is not possible to determine the values of these model ##s ( e . g . , data set , model ##ity # , nonidentifiable etc ) . the same problem has motivated the development of approaches that embrace the model - based approach such as chemical reaction rate analysis and statistical methods @ xcite . however both of these approaches are limited to determining the values of models in steady - state @ xcite . inspired by techniques previously developed in applied differential topology @ xcite and algebraic topology @ xcite , methods for distinguishing between models without unknown parameters have been developed for steady - state models @ xcite , extended to models of wnt models @ xcite , and finally extended to only the same data set @ xcite . finally , these methods model the system @ xmath0 in only two dimensions @ xmath1 using techniques from the algebraic topology and determine whether the steady - state models are consistent with this particular version of thethus , is a _ _ - state _ _ @ xcite . } the model uses # ##h and ##ization , and thus provides a linear trade - off for the comparison with experimental data . therefore , we have a method for comparing variables with _ _ and _ _ via the _ _ _ _ _ . we consider variables of the form @ xmath2 and @ xmath3 where @ xmath4 is a known input into the system , @ xmath5 , @ xmath6 is a known output ( value ) from the system , @ xmath7 , @ xmath8 are independent variables , @ xmath9 , @ xmath10 is the corresponding @ xmath11dimensional parameter value , and the parameters @ xmath12 are linear functions of their values . the behavior of the system can be described in terms of a taylor series where @ xmath13 is the input at the time and @ xmath14 is the output . in this case , we try to construct our linear model by comparing variables we did not consider in the equations from linear models . from the series , we obtain a differential equation , where the associated monomials have values that are independent of the variables @xmath15 . we have linear system of equations for 0 , 1 , and their partial derivatives and we write this linear system of equations as @ xmath16 , @ xmath7 , and call these the input - of _ our _ _ equations _ . finally , we have equations of the form : @ xmath17 where @ xmath18 are the values of the variables and @ xmath19 are the monomials , i . e . monomials of @ xmath20 . we will see here that in the general case , @ xmath21 is a nonlinear linear equation . for non - linear equations , @ xmath21 is linear . if we look into the linear systems of and into the corresponding monomials for each of the time points , we can get a linear system of equations ( each one representing a different time point ) . then we have : and there exist an @ xmath22 such that @ xmath23 . if @ xmath24 of then we are given a zero sum solution and the non - linear solution can be found via the above method ( i . e . , svd ) and we perform the linear ##ity test for @ xcite with thethis is for @ xcite , but for @ xmath23 there will be no solutions . thus , we can test if the resulting system of for @ xmath23 is stable , i . e . has zero or infinitely many solutions . since measurement noise is zero , we have a statistical cut - off for when the model is consistent with the data . now note that we does not have known values for the higher order derivative of , and these have to be known . we construct a model using general population statistics ( gpr ) to estimate the time series error of a gpr . since the data of a population is a model , then we can estimate the higher order derivative of the data as well as the measurement noise , and minimize the error observed during the gpr ( if we can find data with sufficiently much gpr - noise ) . this allows us to fit the data into the model , and test for it with the same data with the statistical cut - off we have . we use our methods both with data from linear and nonlinear regression . we also have an interest in linear regression since a first step in our algorithm is to use matrix analysis to solve equations expressed in terms of input variables , dependent variables , and parameters . for this purpose, we will also need background in the literature from differential algebra , to understand the differential ring concept . for a more detailed description of differential rings and the concepts described above , see @ xcite . in what follows , we hope the reader is familiar with concepts such as _ differential _ and _ integral _ , which are described in more detail in @ xcite . a ring @ xmath25 is said to be a _ differential ring _ if there is a polynomial _ in @ xmath25 and @ xmath25 is closed under differentiation . differential ideal _ is an ideal which is closed under differentiation . a finite description of a differential ring is called a _ differential characteristic set _ , which is a finite description of a possibly infinite set of differential polynomials . we use the following example from @ xcite : let @ xmath26 be a set of differential polynomials , not necessarily polynomials . if @ xmath27 is an auto - reduced set , such that a non - auto - reduced set can be found in @ xmath26 , then @ xmath28 is called a _ differential characteristic set _ . a well - known fact in differential algebra is that differential rings need not be uniquely generated @ xcite . however , a _ differential ring _is generated by the _ ritt - raudenbush ##i _ _ @ xcite . this theorem gives rise to ritt - pseudodivision algorithm ( see below ) , allowing us to find the differential characteristic set of a given differential equation . we then use the algorithm to find the differential characteristic sets and other mathematical properties , and we show that they are relevant to our problem , namely , they can be used to solve the _ input - output equations _ . consider an ode system of the form @ xmath29 and @ xmath30 for @ xmath7 with @ xmath31 and @ xmath32 as two of their variables . let our differential equations be replaced by the differential equations _ by taking the right - hand - side from the ode system to get @ xmath33 and @ xmath34 for @ xmath7 . then the differential characteristic set is of the form @ xcite : @ xmath35 the _ @ xmath36 terms of the differential characteristic set , @ xmath37 , are those terms outside of the system , and when set to the form the _ input - output equations _ : @ xmath38 . , the @ xmath36 term -the equations @ xmath39 are the equations for the variables @ xmath40 with the coefficients of the function field @ xmath10 . note that the differential characteristic set is in general non - rational , so the structure of the input - output equations can be determined simply by changing the coefficients to make them monic . we then use two methods to find the input - output equations . the first method ( ritt s pseudodivision algorithm ) can be used to find the differential characteristic set for a radical differential ideal . the second method ( rosenfeldgroebner ) gives a description of the structure of the differential ideal as the intersection of two differential equations and can also be used to find a differential characteristic set under certain conditions @ xcite . next , we use grbner ##s algorithm to find the _ input - output equations _ . the differential characteristic set of a radical differential ideal is the set of equations for the variables @ xcite . the algorithm to find the differential characteristic set of a radical ( in particular , radical ) differential ideal generated by a finite number of rational polynomals is the ritt s pseudodivision algorithm . we discuss the algorithm in detail below , which comes from the definition of @ xcite . note that our differential equation isthe ideal is a maximal ideal of @ xcite . let @ xmath41 be the root of a polynomial @ xmath42 , which is the largest real root of the polynomial contained in that polynomial . a polynomial @ xmath43 is said to be of _ higher rank _ than @ xmath42 if @ xmath44 and , whenever @ xmath45 , the algebraic degree of the leader of @ xmath43 is less than the algebraic degree of the leader of @ xmath42 . a polynomial @ xmath43 is _ reduced with respect to the polynomial _ @ xmath42 if @ xmath43 contains neither the leader of @ xmath42 with more or less algebraic degree , nor its derivative . if @ xmath43 is _ reduced with respect to @ xmath42 , it can be reduced by using the pseudodivision ##s above . 1 . if @ xmath43 contains the @ xmath46 and @ xmath47 of the leader of @ xmath42 , @ xmath42 is reduced @ xmath48 , and the polynomial is @ xmath47 . 2 . reduce the polynomial @ xmath43 by the product ofthe polynomial degree of @ xmath47 ; let @ xmath49 denote the degree of the term of this differential polynomial in @ xmath50 with respect to the polynomial @ xmath47 . then @ xmath49 is reduced with respect to @ xmath50 . the polynomial @ xmath49 is called the _ pseudoremainder _ of the pseudodivision . the polynomial @ xmath43 is reduced by the pseudoremainder @ xmath49 and the algorithm is repeated with @ xmath51 in place of @ xmath50 and so on , until the pseudoremainder is reduced with respect to @ xmath42 . this algorithm is applied to a set of differential polynomials , such that each polynomial is reduced with respect to the other , to form an algebraic - algebraic polynomial . the result is a radical algebraic polynomial . using the differentialalgebra function in maple , one can construct a function of the rank of the differential ideal generated by a polynomial , as the product of two differential ideals with respect to a prime polynomial and as a radical differential ideal with a single value @ xcite . similarly , the rosenfeldgroebner function in maple has two values : sys andlet , where sys is a subset of set of model equations and inequations which are regular polynomials in the independent and dependent variables and their rank and radius is the regular polynomial ring generated by the radical differentialring of the independent and dependent variables and the rank for them @ xcite . then rosenfeldgroebner gives a basis of the derivative of the polynomial ring generated by sys , with the definition of # # ##s and of the polynomial ring generated by the inequations . in sys this basis consists of a set of regular polynomial rings with respect to the rank of r . note that rosenfeldgroebner gives the prime basis only if the differential ring is in @ xcite . thus , both rational and rational grbner bases can be used to compute the input - output relations . to construct an algebraic grbner basis , one must compute a finite number of derivatives of the model equations and then compute the coefficients of the equations as indeterminates of the differential ring . @ xmath52 , @ xmath53 , @ xmath54 , . . . , @ xmath55 , @ xmath56 , @ xmath57 , . . . , @ xmath58 , @ x##math59 , @ xmath60 , . . . , etc . . the grbner basis of the equations generated from this simple system of ( linear ) equations with an infinite solution where the input variables and their coefficients are rational , can be found . details of this method can be found in @ xcite . other grbner bases have been developed by carr ferro @ xcite , ollivier @ xcite , and others @ xcite , but currently there are no applications to linear equations in @ xcite . we now know how to use the differential equations generated from these equations ( @ ritt ##ner pseudodivision , the groebner basis , or some other basis ) for system design / optimization . suppose our input - output equations , or differential equations , are of the form : @ xmath17 the coefficients @ xmath19 are differential monomials , i . e . monomials of the input / output variables @ xmath61 , @ xmath62 , @ xmath63 , etc , and the coefficients @ xmath18 are rational coefficients in the input parameter space @ xmath10 . in order to correctly assign the rational coefficients @ xmath18 to the differential equations##mials @ xmath19 , we change the input / output variable to make it monic . in other words , we can re - write our input - output equations as : @ xmath64 by @ xmath65 is a linear system in the input / output variables @ xmath61 , @ xmath62 , @ xmath63 , etc . if the values of @ xmath61 , @ xmath62 , @ xmath63 , etc , are known for a finite number of time instances @ xmath66 , then we could factor the values of @ xmath19 and @ xmath65 for each of these time instances to produce a linear system of equations in the variable @ xmath67 . we consider the solution of the linear input - output equation . if there is @ xmath68 polynomial in @ xmath67 , we write the equation : @ xmath69 we write this linear system as @ xmath23 , where @ xmath28 is the @ xmath70 by @ xmath68 polynomial of the form : @ xmath71 @ xmath22 is theis of the form @ xmath72 ^ t $ ] , and @ xmath73 is of the form @ xmath74 ^ t $ ] . for the case of two input - output equations , we have the following : linear system of equations @ xmath23 : @ xmath75 where @ xmath28 is the @ xmath76 : @ xmath77 ##7 . for error - free ( rc ) models , this system @ xmath23 should have a unique solution for @ xmath22 @ xcite . in other words , the solution @ xmath67 of the input - output equation should be uniquely determined from the input / output equations @ xcite . the basic idea of this method is the following . given a set of rc models , we find their corresponding linear equations and we find the solutions of @ xmath20 , etc , for all the instances @ xmath78 , by setting up the linear system @ xmath23 for each instance . the solution to @ xmath23 should be unique for the rc model , but there should be a solution for all of the rc models . even under these conditions , we should be able toselect the candidate model since the input / output data corresponding to that model should satisfy their differential invariant . therefore , one should be able to reject the candidate models since the input / output data should also satisfy their differential invariant . however , with imperfect data , there could be no solution to @ xmath23 except for the correct model . however , with perfect data , one should be able to select the correct model . on the other hand , if there is a solution to @ xmath23 for each of the candidate models , then the problem is to see how ` ` well ' ' some of the models behave and reject them all . we now know how to select models . let @ xmath80 and be the control vector @ xmath81 where @ xmath82 . then , in our case , @ xmath83 , where @ xmath84 is also the vector @ xmath73 . next , we consider the behavior of the ( a particular type of ) control of both @ xmath28 and @ xmath84 . let @ xmath85 and @ xmath86 be the joint control of @ xmath28 and @ xmath##84 , let , and assume that @ xmath87 and @ xmath88 depend only on @ xmath85 and @ xmath86 , respectively . our goal is to calculate the _ rank _ of the quantum problem from values of @ xmath85 and @ xmath86 respectively . we can know how to calculate the rank of an arbitrary matrix , but not compute it . the singular value of the matrix @ xmath80 can be represented by @ xmath89 ( note that we have already calculated the range of singular values of @ xmath28 from @ xmath90 to @ xmath68 . ) the rank of @ xmath28 is written @ xmath91 . the rank of @ xmath28 is written @ xmath92 . thus , @ xmath93 corresponds to the null norm . the basic idea would be to accept only the null norm that has no consequence , i . e . , @ xmath94 , and then to compute its solution in terms of @ xmath85 and @ xmath86 . if these conditions are not satisfied , then we say that .that is true . in other words , we will need _ necessary _ not _ _ _ for to get a proof , i . e . , we need not test ( but not prove ) the null hypothesis . we will refer to this problem as _ test _ the _ problem . we will have two test cases . the first , weyl s inequality , is very simple . let @ xmath95 . then @ xmath96 weyl s inequality can be used to prove @ xmath91 without use of the @ xmath85 . let @ xmath97 and assume that @ xmath98 . let @ xmath99 [ cor : weyl - 1 ] then , if is not true , then @ xmath100 . test the null hypothesis . let @ xmath94 , ( @ xmath101 ) = \ operatorname { [ } ( n ) \ leq \ { ( 1 , n ) $ ] . then , @ xmath102 ) = [ $ ] . but we do not have access to @ xmath103 $ ] and we will have only the [ \ { @ xmath104 $ ]. for the null hypothesis , @ xmath105 ) \ leq \ | [ \ begin { a } - b , \ begin { b } - a ] \ | \ leq \ | \ begin { a } - b \ | + \ | \ begin { b } - a \ | . \ begin { eqn : n - 1 } \ begin { a } \ ] ] [ r : sparse - matrix ] \ | [ cor : weyl - matrix ] . in other words , if does not exist , then has no trace . this algorithm will fail to successfully test the null hypothesis if @ xmath28 is ( small ) augmented - matrix . as an example , suppose that @ xmath106 and then @ xmath107 consist of a single element ( @ xmath108 ) . ( @ xmath101 ) \ leq [ $ ] , ( @ xmath102 ) = [ $ ] ( which is small ) . assuming that @ xmath109 and @ xmath110 are small , @ xmath111 ) $ ] will therefore also be small . in addition , we should consider that thenote that @ xmath101 ) = \ operatorname { n } ( ( ) $ ] . however , we can only set lower bounds on the numerical rank ( we can only check if the expected error is ` ` ' small ' ' ) , so this is not possible in practice . an alternative method is to use the _ _ _ matrix obtained by thresholding . how to implement such a method , however , is not at all straightforward and can be a very difficult matter even if the algorithms have limited dynamic range . the method is valid if @ xmath112 since ( @ xmath102 ) = \ operator _ { n + 1 } ( \ begin { a } , \ end { b } ) = [ $ ] ] . however , this is not a significant improvement over that given above since if @ xmath28 is numerical - rank , then it must be proved that is not . as a matter of fact , we can apply the [ matrix : n - rank ] to a general linear model . we start by selecting the input and output data and then add a small amount of weights to the input data and proceed to update the linear model . in the next step , we can see howto obtain the [ ref : n - 1 ] equation gives the ` ` ` true ' ' condition for the model . next , we take information from the linear 3 - compartment model , according to , and attempt to derive the general form of the linear 3 - compartment model with the corresponding input / output equation . [ ref : mainex ] let our model be a 3 - compartment model of the general form : @ xmath113 @ xmath114 . we have an ode to the first compartment of the matrix @ xmath115 and the second compartment is empty , so that @ xmath116 is the solution . the solution to this class of equations can be easily made of the form : @ xmath117 so that @ xmath118 . the input - output equation for the @ xmath119 linear model with the corresponding input / output to the first compartment has the form : @ xmath120 where @ xmath121 are the coefficients of the characteristic polynomial of the matrix @ xmath28 and @ xmath122 are the coefficients of the characteristic polynomial of the matrix @ xmath123 which is the first row and first column of@ xmath28 } . we then substitute values of @ xmath124 at time instances @ xmath125 into our input - output equation and solve the resulting linear system of equations for @ xmath126 . we found that @ xmath127 , which agrees with the values of the characteristic polynomials of @ xmath28 and @ xmath123 . we now attempt to derive the 2 - compartment model from 2 - compartment model 1 . we solve the input - output equation for the @ xmath128 matrix , with a corresponding input / output to the first row , which has the form : @ xmath129 where and @ xmath130 are the coefficients of the characteristic polynomial of the matrix @ xmath28 and @ xmath131 are the coefficients of the characteristic polynomial of the matrix @ xmath123 which has the first row and first column of @ xmath28 respectively . we substitute values of @ xmath132 at time instances @ xmath133 into our input - output equation and attempt to solve the resulting linear system of equations for @ xmath134 . the corresponding equation for the matrix @ xmath28 with thesubstituted values of @ xmath135 at time instances @ xmath133 are : @ xmath136 the substituted values of the matrix @ xmath137 with the substituted values of @ xmath132 at time instances @ xmath133 are : @ xmath138 we add noise to our vector matrix in the following way . to each entry @ xmath139 , and @ xmath140 , we add @ xmath141 where @ xmath142 is a random real number between @ xmath143 and @ xmath144 , and @ xmath145 equals @ xmath146 . then the noisy matrix @ xmath85 has the following substituted values : @ xmath147 we also add noise to our matrix @ xmath73 in the following way . to each entry @ xmath148 , we add @ xmath141 where @ xmath142 is a random real number between @ xmath143 and @ xmath144 , and @ xmath145 equals @ xmath146 . then the noisy matrix @ xmath149 isthe following are examples : @ xmath150 we use the matrix @ xmath151 and apply the inverse of this matrix to the smallest singular value of @ xmath149 . since the frobenius function of @ xmath151 is @ xmath152 , which is _ less than _ the smallest singular value @ xmath153 , we can reject this model . however , using the 2 - compartment model instead , we are able to reject the 3 - compartment model . we will consider the statistical significance of the value of . first , we use a noise model . if the values @ xmath109 and @ xmath110 are known , e . g . , @ xmath154 and @ xmath155 for all @ xmath156 ( and the singular value of @ xmath145 is the ` ` ' ' ' @ xmath85 and @ xmath86 ) , the matrix [ 1 : 1 - 1 ] can be solved for all . finally , it is possible to treat the parameters as independent random variables , which are not random . second , we can use a noise model of the same@ xmath157 where @ xmath158 is a ( random ) variable that depends on @ xmath85 and associated with @ xmath159 , @ xmath160 is the hadamard ( entrywise ) inner product @ xmath161 , and @ xmath162 is a real - valued random variable whose entries @ xmath163 are elements of x . in our case of this , the entries of @ xmath158 depend on those of @ xmath85 as well . take @ xmath164 for the column vector @ xmath165 . note that we can also calculate the ` ` noise ' ' of @ xmath166 . then the only possible matrix entries are @ xmath167 with the additivity of @ xmath168 for all gaussians . however , the above statement is not true since @ xmath169 ` ` noise ' ' @ xmath170 in the sense that the former has variance @ xmath171 , and the latter has variance of @ xmath172 . in other words , we are not going in the opposite direction .this is taken into account in @ xcite . ] @ xmath173 therefore , @ xmath174 therefore , to first consider : @ xmath145 , @ xmath175 the equivalent result holds for @ xmath159 . all of the bounds in the derivation above are valid in @ xmath109 and @ xmath110 ( for example [ th : augmented - matrix ] , the result is simply the sum of these bounds ) and so can be written as @ xmath176 with the remainder . the selection rule is given as follows . let @ xmath177 be the joint distribution , i . e . , @ xmath111 ) $ ] theorem [ th : augmented - matrix ] . then since @ xmath178 where we have made explicit the dependence of both tests on the same underlying random variable @ xmath179 , the ( joint ) distribution function of @ xmath177 will be that of @ xmath176 , i . e . , @ xmath180 . , @ xmath181 [ eqn : prob - matrix ] note that if , e . g ., @ xmath182 ( i . e . , if @ xmath84 is chosen , ) , which leads to , @ xmath183 . similarly , we can assign an @ xmath184 - value to a particular value of @ xmath177 , . the last term for is of the form @ xmath185 . note that @ xmath186 rejects the null hypothesis . in a simple statistical hypothesis testing approach , we can also reject the null hypothesis if for at least @ xmath187 , where @ xmath187 is the expected confidence level ( e . g . , @ xmath188 ) . we can proceed to , @ xmath189 , where we can assume that @ xmath190 . this can be done in several ways . a simple way is to assume that @ xmath191 where @ xmath192 is the frobenius distribution , ( @ xmath193 and @ xmath194 is the normal distribution ) . ] with @ xmath195 degrees of freedom . for , @ xmath196 and , the terms and can be made explicit : thethis is equality in the sense that @ xmath197 . the result is that @ xmath198 = @ xmath199 a slightly simpler solution is to use the algorithm @ xcite @ xmath200 where @ xmath201 and @ xmath202 are the @ xmath203th row and @ xmath204th column , respectively , of @ xmath205 . the @ xmath206 bound can then be computed using a probability distribution via @ xmath207 as well as by using a probability distribution ( see below ) . variants of this algorithm exist . first , we will refer to a variant of tropp @ xcite . the result follows from fig . 1 of @ xcite . let @ xmath190 , where = @ xmath163 . then for all @ xmath208 , @ xmath209 [ th : hadamard - levy ] the bound for @ xmath210 can then be computed as follows . find @ xmath211 such that @ xmath212 . then by finding [ th : hadamard - levy ] , @ x##math213 \ , dt , \ text { \ } \ ] ] where @ xmath214 and @ xmath215 are the ` ` ' ' ' components of the distribution for @ xmath158 and @ xmath159 , respectively . the number in parentheses corresponds to @ xmath216 \ \ & = \ frac { 1 } { \ sigma _ { a } ^ { 2 } \ sigma _ { b } ^ { 2 } } \ left [ ( \ sigma _ { a } ^ { 2 } + \ sigma _ { b } ^ { 2 } ) \ left ( 1 - \ frac { \ sigma _ { a } ^ { 2 } } { \ sigma _ { a } ^ { 2 } + \ sigma _ { b } ^ { 2 } } } \ right ) ^ { 2 } + \ sigma _ { b } ^ { 2 } \ left ( 1 - \ frac { \ sigma _ { b } ^ { 2 } } { \ sigma _ { a } ^ { 2 } + \ sigma _ { b } ^ { 2 } } \ right ) ] ^ { 2 } \ right ] \ \ & = \ frac { 1 }{ \ sigma _ { a } ^ { 2 } \ sigma _ { b } ^ { 2 } } \ left [ ( \ sigma _ { a } ^ { 2 } + \ sigma _ { b } ^ { 2 } ) \ right ( t - \ frac { \ sigma _ { a } ^ { 2 } } { \ sigma _ { a } ^ { 2 } + \ sigma _ { b } ^ { 2 } } x \ right ) ^ { 2 } + \ frac { \ sigma _ { a } ^ { 2 } \ sigma _ { a } ^ { 2 } } { \ sigma _ { a } ^ { 2 } + \ sigma _ { b } ^ { 2 } } x ^ { 2 } \ right ] \ \ & = \ frac { \ sigma _ { a } ^ { 2 } + \ sigma _ { b } ^ { 2 } } { \ sigma _ { a } ^ { 2 } \ sigma _ { b } ^ { 2 } } \ left ( t - \ frac { \ sigma _ { a } ^ { 2 } } { \ sigma _ { a } ^ { 2 } + \ sigma _ { b } ^ { 2 } } x \ right ) ^{ x } + \ frac { x ^ { 2 } } { \ sigma _ { b } ^ { 2 } + \ sigma _ { b } ^ { 2 } } \ begin { 1 } \ ] ] , is the integral . \ , @ xmath217 \ int _ { 0 } ^ { 2 } \ exp \ left [ - \ frac { a } { x } \ left ( \ frac { \ sigma _ { a } ^ { 2 } + \ sigma _ { b } ^ { 2 } } { \ sigma _ { a } ^ { 2 } \ int _ { b } ^ { 2 } } \ right ) \ left ( [ - \ frac { \ sigma _ { a } ^ { 2 } } { \ sigma _ { a } ^ { 2 } + \ sigma _ { b } ^ { 2 } } } \ right ) ^ { 2 } \ right ] dt . \ sum { 1 } \ ] ] \ , @ xmath218 so that the integral is @ xmath219 dt = \ int _ { 0 } ^ { 2 } \ exp \ left [ - \ frac { ( - - \ , x ) ^ { 2 } }{ - \ alpha ^ { 2 } } \ right ] dt . \ begin { align } \ ] ] the result is @ xmath220 dt , @ xmath221 dt = \ phi \ int _ { - \ alpha x / \ sigma } ^ { ( 1 - \ alpha ) x / \ sigma } u ^ { - a ^ { 2 } / 2 } \ , du = \ sqrt { - \ alpha } \ right \ left [ \ phi \ left ( \ frac { ( 1 - \ alpha ) x } { \ sigma } \ right ) - \ phi \ left ( - \ frac { \ alpha x } { \ sigma } \ right ) \ right ] , \ begin { align } \ ] ] where @ xmath222 is the vector vector correlation function . then , @ xmath223 \ exp \ left [ - \ frac { a } { 2 } \ left ( \ frac { a ^ { 2 } } { \ sigma _ { 2 } ^ { 2 } + \ sigma _ { 2 } ^ { 2 } } \ right ) \ right ] . \ begin { eqn : p1 } \ end { aligned } \ ] ] dt .( see also . ) this is @ xmath224 we can give a method for the second partial error and the squared error using this method , and we apply the same # ##s to the linear and linear models in the next section . the sampling process ( gps ) is a random process @ xmath225 , where @ xmath226 is a mean function and @ xmath227 a variance function . it is often used for estimation / estimation as well . suppose that there is an unknown variance function @ xmath228 that we can easily compute with just random sampling : @ xmath229 , where @ xmath230 for @ xmath231 the normal distribution . we solve the problem of computing @ xmath228 in a linear model by defining it to be a function with the mean and variance , @ xmath232 and @ xmath233 , respectively . consider the prior mean of @ xmath234 ^ { { \ mathsf { t } } } $ ] and the data of @ xmath235 ^ { { \ mathsf { t } } } $ ] and@ xmath236 ^ { { \ mathsf { t } } } $ ] is the prediction function @ xmath237 ^ { { \ mathsf { t } } } $ ] for @ xmath238 the joint distribution of @ xmath239 and @ xmath240 is the same : @ xmath241 where @ xmath242 is the posterior mean and variance , respectively . this allows us to estimate @ xmath239 on the basis of and @ xmath243 . the two components of @ xmath244 are the posterior mean and are the variance associated with this estimation function . this is an estimate for the expected variance @ xmath239 . what if we want to estimate the variance ? ask @ xmath245 for the prediction function @ xmath48 . suppose @ xmath246 has radius of curvature . then , @ xmath247 \ to { x } ( \ boldsymbol { t } ) \ - \ - x ( \ boldsymbol { t } ) \ - x ' ( \ boldsymbol { t } ) \ -\ vdots \ left i ^ { ( 1 ) } ( \ boldsymbol { s } ) \ cr \ begin { pmat } \ sim { \ mathcal { t } } \ left ( \ begin { pmat } [ { . } ] \ mu _ { { \ text { prior } } } ( \ boldsymbol { s } ) \ cr \ - \ mu _ { { \ text { prior } } } ( \ boldsymbol { s } ) \ left \ mu _ { { \ text { prior } } } ^ { ( 1 ) } ( \ boldsymbol { s } ) \ - \ vdots \ left \ mu _ { { \ text { prior } } } ^ { ( 1 ) } ( \ boldsymbol { s } ) \ - \ begin { pmat } , \ begin { pmat } [ { | . . . } ] \ mu _ { { \ text { prior } } } ( \ boldsymbol { s } , \ boldsymbol { s } ) + \ begin ^ { t } ( \ boldsymbol { s } ) } & \ mu _ { { \ text { prior } } } ^ {{ \ mathsf { t } } } ( \ boldsymbol { s } , \ boldsymbol { t } ) & \ sigma _ { { \ text { prior } } } ^ { ( n , 0 ) , { \ mathsf { t } } } ( \ boldsymbol { s } , \ boldsymbol { t } ) & \ cdots & \ sigma _ { { \ text { prior } } } ^ { ( n , 0 ) , { \ mathsf { t } } } ( \ boldsymbol { prior } , \ boldsymbol { t } ) \ - \ - \ sigma _ { { \ text { prior } } } ( \ boldsymbol { s } , \ boldsymbol { prior } ) & \ sigma _ { { \ text { prior } } } ( \ boldsymbol { prior } , \ boldsymbol { prior } ) & \ sigma _ { { \ text { prior } } } ^ { ( n , 0 ) , { \ mathsf { t } } } ( \ boldsymbol { prior } , \ boldsymbol { t } ) & \ cdots & \ sigma _ { {\ text { prior } } } ^ { ( n , 1 ) , { \ mathsf { t } } } ( \ boldsymbol { prior } , \ boldsymbol { s } ) \ cr \ sigma _ { { \ text { prior } } } ^ { ( 1 , 0 ) } ( \ boldsymbol { prior } , \ boldsymbol { s } ) & \ sigma _ { { \ text { prior } } } ^ { ( n , 1 ) } ( \ boldsymbol { prior } , \ boldsymbol { prior } ) & \ sigma _ { { \ text { prior } } } ^ { ( n , 1 ) } ( \ boldsymbol { prior } , \ boldsymbol { s } ) & \ cdots & \ sigma _ { { \ text { prior } } } ^ { ( n , 1 ) , { \ mathsf { t } } } ( \ boldsymbol { prior } , \ boldsymbol { prior } ) \ cr \ vdots & \ vdots & \ vdots & \ ddots & \ vdots \ cr \ sigma _ { { \ text { prior } } }^ { ( n , 1 ) } ( \ boldsymbol { t } , \ boldsymbol { t } ) & \ sigma _ { { \ text { prior } } } ^ { ( n , 1 ) } ( \ boldsymbol { s } , \ boldsymbol { t } ) & \ sigma _ { { \ text { prior } } } ^ { ( n , 1 ) } ( \ boldsymbol { s } , \ boldsymbol { t } ) & \ cdots & \ sigma _ { ( n , 1 ) } ( \ boldsymbol { s } , \ boldsymbol { s } ) \ , \ begin { pmat } \ , ) , \ begin { prior } \ ] ] where @ xmath248 is the posterior distribution for @ xmath249 and @ xmath250 . this posterior distribution is usually of the form . an alternative version of this is the posterior estimate of @ xmath251 for each @ xmath252 . however , if we are interested only in the posterior estimate of each @ xmath253 , then it follows to for each @ xmath254and thus : @ xmath255 the cost of finding @ xmath256 must always be zero for all @ xmath203 . we can consider the special case of the standard normal ( real ) density function @ xmath257 , \ { { x } \ ] ] where @ xmath258 is the squared parameter and @ xmath90 is the random parameter . the density function is one of the most commonly used density functions in statistics . its value can be expressed in terms of the ( probabilists ) hermite polynomials @ xmath259 ( these are also sometimes called @ xmath260 ) . the first three hermite polynomials are @ xmath261 , @ xmath262 , and @ xmath263 . we want to have the values @ xmath264 . then @ xmath265 such that @ xmath266 . then @ xmath267 and @ xmath268 . then , @ xmath269 the linear ##ization allows us to have the values of the hyperparameters @ xmath270 , @ xmath271 , and @ xmat##h90 . in practice , however , these are not well known . in the section below , we deal with this by computing the hyperparameters from the data by computing the coefficients . we do this by using a constant - time algorithm , which must be very close to the sample data size , so we test the samples on a large number of hyperparameter ##s and get the best result possible . this improves the accuracy of the individual hyperparameters which can still sometimes fail . we obtain our results using three models : linear compartment models ( 2 and 3 species ) , lotka - volterra models ( 2 and 3 species ) and others . as the linear compartment model models are described in the previous section , we use the differential equations of the lotka - volterra and fisher - rosenfeldgroebner . we use each of these models to generate time series data , analyze various amounts of data , and calculate the corresponding higher order derivatives of the ##a . as described in the previous section , we require the estimation of the higher order derivatives to have a positive # ##istic ratio , so the resulting result is not ` true ' . in some cases , this can be remedied by increasing the number of data points. using the . . ##l extension , we check each of the parameters for the differential invariant ##s for the parameters . [ ex : lv2 ] the two species lotka - volterra model is : @ xmath272 where @ xmath273 and @ xmath274 are variables , and @ xmath275 are variables . we assume only @ xmath273 is constant and perform differential elimination and obtain our differential invariant in terms of : @ xmath276 : @ xmath277 [ ex : lv3 ] . with the additional variable @ xmath278 , the three species lotka - volterra model is : @ xmath279 where only @ xmath116 is constant . performing differential elimination , the differential invariant is : @ xmath280 [ ex : lor ] the two species model , the standard model , is given by the system of equations : @ xmath281 we assume only @ xmath116 is constant , perform differential elimination , and obtain the following equation : @ xmath282 [ ex : lc2 ] a simple two - species model without parameters can be usedexample : @ xmath283 where @ xmath273 and @ xmath274 are variables , and @ xmath284 are parameters . we assume only @ xmath273 is variable and perform differential elimination and obtain our differential invariant in terms of only @ xmath276 : @ xmath285 [ x : lc3 ] the corresponding higher - dimensional model without parameters is : @ xmath286 where @ xmath287 are variables , and @ xmath288 are parameters . we assume only @ xmath273 is variable and perform differential elimination and obtain our differential invariant in terms of only @ xmath276 : @ xmath289 . assuming @ xmath116 and equation 2 . 2 . 1 have the same input data , we apply our analysis to data taken from each model and perform a comparison . the data are compared and the data points are sampled for @ xmath165 in each model . we apply two rounds of random ##ization to the input data , and then extract the higher order parameters from the data . for example , during our analysis we find that for all models of the lotka -volterra three species model , e . g . @ xmath290 $ ] , we found a positive p - value , which means that we could not reject the higher order values of the model . once the data is collected and initial values are obtained from the linear compartment , the derivative value of is compared against the other two models . these are shown in figure [ 0 - 1 ] , where initial values of 1 , the model 0 , and the means model are selected . we found that we can reject the two species lotka - volterra model and means model for data generated from the lotka - volterra three species ; thus the linear compartment models are rejected . for data from the two species lotka - volterra model , the linear compartment model and two - species lotka - volterra can be rejected until the data arrives and then the user can no longer reject both models . for data simulated from the means model can be rejected the three species linear compartment and two species lotka - volterra model . $ ] and initial values @ xmath291 $ ] . ( 2 ) data generated from two species lotka - volterra model with initial values @ xmath292 $ ] and [[ @ xmath293 $ ] . ( c ) data simulated from the der model with parameter values @ xmath294 $ ] and initial condition @ xmath293 $ ] . ( d ) data simulated from the der model ##al ##der model with parameter values @ xmath295 $ ] and initial condition @ xmath296 $ ] . ] we have shown our model rejection effect for linear models . in this section , we raise some very important questions about linear equations . note that we have assumed that the coefficients are all unknown and we have not taken the exact linear relationships among the coefficients into account . this last problem is the reason our work only involves model rejection and not model acceptance . therefore , the unknown coefficient is only considered as an independent independent variable in our linear system of equations . however , there may be situations where we would like to get all this additional information . we have solved the problem of finding the unknown values . using @ xcite , an algebraic expression for the input - output relations for linear equations was found . in particular , it was found that the equations @ xmath297compartment ##s referring to two interacting systems with at least one compartment and with the same input and output variables can bethe two linear equations consist of the input - output equations . for example , a linear 2 - compartment model with a single input and output in the same compartment and corresponding to a strongly connected graph with at least one leak has the form : @ xmath298 however , our leak ##y algorithm would only work for two such linear 2 - compartment models with the above - mentioned properties . in order to distinguish between two linear models , we need to take additional information into account , e . g . , parameter values . consider the following two linear 2 - compartment models : @ xmath299 @ xmath300 whose two input - output equations are of the form : @ xmath301 note that both of these models are of the above - mentioned form , i . e . linear 2 - compartment models with a single input and output in the same compartment and corresponding to strongly connected graphs with at least one leak . in the first model , there is a leak from the first compartment and an exchange between compartments @ xmath144 and @ xmath128 . in the second model , there is a leak from the second compartment and an exchange between compartments @ xmath144 and @ xmath128 . note that thethen @ xmath302 is selected . in the first model , this reduces our invariant to : @ xmath303 in the second model , our invariant is : @ xmath304 in this case , the right - hand sides of the two equations are the same , but the first equation has two variables ( parameters ) and the second equation has three variables ( parameters ) . thus , if we had chosen from the second model , we could choose to use the first model ( much like the 1 - compartment versus 2 - compartment position relationship in the example above ) . in other words , any point in the span of @ xmath305 and @ xmath306 for @ xmath307 would also be in the span of @ xmath139 and @ xmath140 for . we now consider the case of the coefficient dependency relationships . while we can only use the general coefficient dependency relationships among the coefficients in our linear regression approach to parameter selection , we can incorporate special linear relationships , such as the coefficients being polynomials only . we have already identified the case in which this would happen in the previous example ( from the nonzero parameter values ) . we now consider the case where the coefficients tend to zero .from the explicit expression for input - output equations from @ xcite , we see that a linear model without any leaks has a zero term for the value of @ xmath140 . thus a linear 2 - compartment model with a linear input and output in the same compartment and connected to a simply connected system without any leaks has the value : @ xmath308 . to distinguish between two different linear 2 - compartment models , one with leaks and one without any leaks , we should incorporate this zero term into our equation . consider the following two linear 2 - compartment models : @ xmath309 @ xmath310 whose explicit input - output equations are of the form : @ xmath311 in the first model , there is a leak from the first model and an exchange between compartments @ xmath144 and @ xmath128 . in the second model , there is an exchange between compartments @ xmath144 and @ xmath128 and no leaks . thus , our invariant can be written as : @ xmath312 thus , the left - hand sides of the two equations are the same , but the first equation has two variables ( and ) and the second equation has three variables (2 ) . thus , if we had selected from the first model , we could choose to select the second model . in other words , any data in the span of @ xmath305 and @ xmath306 for @ xmath307 could not be in the span of @ xmath139 and @ xmath306 for . first , we consider the identifiability problem of our model . if the number of parameters is greater than the number of coefficients , then the model is identifiable . on the other hand , if the number of parameters is less than or equal to the number of coefficients , then the model could not be identifiable . therefore , an identifiable model is preferred to an identifiable model . we note that , in our method of constructing the linear algebra @ xmath23 from the input - output data , we could in principle solve for the parameters @ xmath22 and then solve for the data from these unknown parameter values if the solution is valid @ xcite . however , this is not a commonly used method to determine parameter values in general . as mentioned above , the more general linear relations among the coefficients are not taken into account in our linear algebra approach . this isthat there could be two different models with the same corresponding polynomial form of the input - output relationship . if such a invariant can not be rejected , we say that an invariant associated with a given input - output relationship is equivalent to an invariant polynomial satisying the polynomial form of the input - output relationship , as we discussed in the previous section . consider the following two identical two - compartment models : @ xmath299 @ xmath313 whose corresponding input - output equations are of the form : @ xmath314 in the first model , there is a leak from the second compartment and an exchange between compartments @ xmath144 and @ xmath128 . in the second model , there are leaks from both compartments and an exchange between compartments @ xmath144 and @ xmath128 . thus , both models have equations of the form : @ xmath298 since the first model is valid and the second model is not , we have to reject the equations of the second model if the ' s equations can not be rejected . by performing this first non - linear test , one has now used the input - output equations to determine the identifiability @ .##cite . in this sense , our method extends the full range of statistical methods for comparing models with the course data , in that we can " compare competing models , then determine the identifiability of the models using input - output data obtained from the differential equations , determine the validity of the competing models , and use the objective function model selection method to select the best model . " the linear differential elimination selection method does not account for model complexity , unlike other model selection methods . however , we reject when a model may not , for certain parameter values , be consistent with the competing model . we believe that simple models , such as the - - - form could be rejected when it is obtained from a more complex model , such as the single parameter lotka - volterra model , which has a wider range of results . on the other hand , more complex models , such as the linear form , are often also rejected , from data obtained from more complex models . in addition it would be useful to better understand the relationship between structural information and dynamics . we also believe it would be useful to develop new models of dynamics @ xcite . we believe there is a need for additional error - free coplanarity model selection methods . .would be interesting to see which algorithms for matrix multiplication could handle complex numbers , and whether this work could be useful . the authors acknowledge funding from the american institute of physics ( aim ) where this project originated . the authors thank mauricio barahona , paul green , and david sullivant for the funding . we are also grateful to paul green for writing about gps and for his article ##m , which served as an important impetus to get started . gps is also funded by the david and lucille packard foundation . hah received funding from the packard research foundation , epsrc , ep / k041096 / 2 and the stumpf leverhulme travel award . a . aistleitner , _ relations between grbner functions , differential grbner functions , and _ elliptic functions _ , phd thesis , johannes kepler universitt , 2010 . t . akaike , _ a new look at the differential equation theory _ , ieee trans . rev . b , * 21 * ( 1974 ) , pp . 716723 . f . boulier , _ differential equations and linear systems _ , j . opt . rev . , * 21 * ( 2007 ) , pp . 111 - 139 . f . boulier , j . de##zard , m . ollivier , m . petitot , _ methods for the solution of a randomly chosen differential equation _ , in : issac ##s : proceedings of the 1995 international conference on symbolic and algebraic algorithms , pp 158 - 166 . acm press , 1995 . m . carr ferro , robert grbner , and differential equations , in a . huguet and m . poli , eds , proceedings of the 6th international conference on advanced symbolic , algebraic , and error - free algorithms , volume 356 of lecture notes in computer science , pp . 131 - 142 . springer , 1987 . clarke , _ _ _ _ _ , j biophys . , 39 ( 1988 ) , pp . p . clarke , m . smith , and donal oshea , _ networks , systems , and processes _ , springer , new york , 2007 . m . conradi , m . saez - rodriguez , j . m . gilles , m . raisch , _ _ _ - network analysis to determine the molecular clock _ _ , iee proc . 152 ( 2005 ) , pp . p . diop , _ _ _ _ methods and their applications to network analysis _ , * , * ( 1992 ) , pp . 137 - 161 .j . drton , b . sturmfels , j . sullivant , _ lectures on computational systems _ , oberwolfach verlag ( springer , berlin ) vol . 1 . 2009 . m . feinberg , _ chemical reaction network structure and the stability of complex chemical reactors i . the deficiency one and deficiency two systems _ , chem . , * 42 * ( 1987 ) , pp . 22292268 . m . feinberg , _ chemical reaction network structure and the stability of complex chemical reactors ii . the transition states for systems of deficiency one _ , chem . , * 42 * ( 1988 ) , pp . 220 . m . forsman , _ differential operator theory for complex nonlinear systems _ , phd thesis , linkping ##er , 1991 . v . golubitsky , a . kondratieva , a . a . maza , and v . ovchinnikov , _ a test for the rosenfeld - grbner theorem _ , j . science comput . , * 42 * ( 2008 ) , pp . 582 - 610 . m . cohen , m . j . harrington , m . rosen , b . sturmfels , _ computational systems _ : a case study for the application##nt _ _ , j . biol . , * 109 * ( 39 ) ( 2016 ) , pp . 47 - 49 . e . smith , m . myers , k . l . ho , j . smith , m . harrington , _ _ simulation methods for model selection _ , s . s . gunawardena , _ distributivity and processivity of multisite phosphorylation to be achieved by steady - state selection _ , biophys . rev . , 3 ( 2007 ) , pp . gutenkunst , j . a . smith , m . j . casey , j . a . smith , m . j . myers , s . s . sethna , _ single - variable selection in molecular biology _ _ , j comput . biol . , 3 ( 2007 ) , harrington , k . l . ho , m . thorne , m . j . a . stumpf , _ model - based model selection _ based on steady - state coplanarity _ , proc . , * 109 * ( 39 ) ( 2012 ) , pp . 1574615751 . a . kaplansky , _ an introduction to differential equations _ , springer , berlin , 1957 . e . smith. kolchin , _ _ equations and linear systems _ , pure appl . math . , * 1 * ( 1973 ) . j . ljung and m . maclean , _ _ _ identifiability for multi - parameterization _ , automatica , * 2 * ( 1 ) ( 1994 ) , pp . 265 - 276 . maclean , m . rosen , m . j . byrne , j . j . harrington , _ _ - estimation methods for wnt - systems and other lines of analysis _ , proc . , * 112 * ( 1 ) ( 2015 ) , pp . 26522657 . j . smith , _ _ grbner ##s _ , ed . , university of michigan , 1991 . s . s . manrai , s . gunawardena , _ the theory of multisite phosphorylation _ , math . , * 31 * ( 2008 ) , pp . 55335543 . # ##s . , http : / / www . maplesoft . com / view / pdf / view / view . aspx ? view = differentialalgebra s . meshkat , j . smith , and j . m . distefano * , _ introduction to ritt _ pseudodivisionfor example the input - output relations of multi - dimensional models _ , nature biosci . , * 239 * ( 2012 ) , pp . 117 - 123 . m . meshkat , m . sullivant , and m . eisenberg , _ identifiability analysis for certain types of optimal control systems _ , j . opt . biol . , * 34 * ( 2015 ) , pp . 1620 - 1651 . f . ollivier , _ le probleme de lidentifiabilite structurelle globale : solution theoretique , methodes effectives and bornes de complexite _ , doctoral thesis , ecole polytechnique , 1990 . f . ollivier , _ the theory of differential equations _ . in t . sakata , editor , proceedings of the first international conference on computer science , mathematics , and error - tolerant systems , volume 508 of lecture notes in computer science , pp . 304 - 321 . springer , 1991 . orth , j . thiele , j . . palsson , _ what is optimal control _ ? _ nature biotechnol . , * 77 * ( 2010 ) , pp . 1 , 2000 . j . a . hansen , _ optimal control for some_ _ . the mit press : cambridge , 2006 . j . a . ritt , _ nonlinear equations _ , _ ( 1950 ) . j . a . saccomani , m . audoly , and m . dangi , _ _ identifiability of nonlinear equations : the case of boundary conditions _ , automatica * * * ( 2003 ) , pp . 619 - 632 . well - defined upper bounds for products of square roots . math . 1 : 389434 , 2012 . bounds for the absolute value of hadamard numbers . siam j . applied math . 45 ( 10 ) : 10931095 , 1997 .