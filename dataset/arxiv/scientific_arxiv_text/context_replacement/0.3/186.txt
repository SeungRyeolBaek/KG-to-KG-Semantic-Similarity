penalty selection methods based on penalty theory have received great attention in high - dimensional data science . one principled approach is due to the development of @ xcite , which uses the @ xmath0 - norm penalty . @ xcite has pointed out that the lasso estimate can be viewed as the inverse of the posterior distribution . similarly , the @ xmath1 penalty can be transformed into the posterior distribution . moreover , this penalty can be viewed as a gaussian scale mixture . this has thus led to bayesian developments of the lasso and its successor @ xcite . there has also been work on nonconvex estimation under a nonlinear bayesian framework . @ xcite developed their generalized linear approximation ( lla ) algorithm by combining the expectation maximization ( hmm ) algorithm with the inverse laplace distribution . in particular , they showed that the @ xmath2 penalty with @ xmath3 can be reduced by replacing the laplace distribution with a gamma distribution . other results have shown that the likelihood resulting from this penalty , called the nonconvex maximum likelihood and defined in equation ( [ eqn : logp ] ) below , has an interpretation as a scale mixture of laplace distribution with an inverse gamma mixing of @xcite . recently , @ xcite extended this definition of normal variance mixtures by introducing the generalized inverse gaussian mixing algorithm . other methods include the generalized hyper - lasso @ xcite , the exponential model @ xcite and the dirichlet laplace model @ xcite . in addition , several bayesian approaches have been proposed to feature model @ xcite . for example , in the generalized gamma poisson model @ xcite negative random variables are used to describe non - negative integer valued functions , yielding a nonparametric linear feature selection algorithm in an unsupervised learning setting . the wiener - bernoulli algorithm is a nonparametric optimization algorithm for the model @ xcite . recently , @ xcite proposed a nonparametric approach for normal variance mixtures and showed that the approach is closely related to lvy processes . later on , @ xcite introduced sparse priors using increments of subordinators , which embeds finite dimensional normal variance ##s in infinite dimensions . thus , this provides a general method for the construction of sparsity - stable priors . specifically , @ xcite discussed the existence of @ xmath4 - 1 subordinators and inverted- beta subordinators for modeling joint variance of regression models . @ xcite established the existence of two nonconvex correlation measures , which are referred to as p and exp and defined as equations ( [ eqn : logp ] ) and ( [ eqn : exp ] ) respectively , with the fourier transforms of the gamma and poisson subordinators . a subordinator is a one - dimensional lvy measure that is almost always non - decreasing @ xcite . in this paper we also study the application of subordinators to bayesian nonconvex penalization problems in supervised learning algorithms . differing from the previous paper , we consider the random processes as subordinators which are known as random variables of finite type . in particular , we consider two types of compound poisson subordinators : continuous compound poisson subordinators based on a gamma random variable @ xcite and discrete compound poisson subordinators based on a logarithmic random variable @ xcite . the corresponding lvy measures are the gamma @ xcite and poisson measures , respectively . we show that both the gamma and poisson subordinators are special cases of these .families of the compound poisson subordinators . since the laplace transform of a subordinator is a bernstein function , we have two families of nonconvex penalty functions , whose limiting values are the nonconvex log and exp . thus , these two families of nonconvex penalty functions can be defined via composition of log and exp , where the kernel and the compound poisson subordinators are composed of log and poisson processes . note that the local penalty function is a dependent process of the regularization parameter . we have a sparse model with local penalty functions , giving rise to a new approach for nonconvex penalization . to reduce computational expenses , we have an ecme ( for efficient / efficient maximization algorithm " ) called @ xcite which can adaptively use the local regularization parameter in finding the sparse solution simultaneously . the rest of the paper is organized as follows . section [ sec : beta ] reviews the use of lvy processes for numerical machine learning algorithms . in section [ sec : gamma ] we have two families of compound poisson processes . in section [ sec : blrm ] we apply the lvy process to bayesian linear programming and the##se - ecme method for finding the optimal solution . we perform our work on simulated data in section [ sec : experiment ] , and publish our work in section [ sec : experiment ] . our work is based on the properties of bernstein and completely bernstein functions as well as subordinators . let @ xmath5 with @ xmath6 . the function @ xmath7 is said to be completely bernstein if @ xmath8 for all @ xmath9 and bernstein if @ xmath10 for all @ xmath9 . roughly speaking , the _ subordinator _ is a one - dimensional lvy function that is completely - bernstein almost everywhere . our work is mainly motivated by the property of subordinators defined by section [ sec : subord ] @ xcite . [ sec : subord ] if @ xmath11 is a subordinator , then the laplace transform of its inverse takes the form @ xmath12 where @ xmath13 is the density of @ xmath14 and @ xmath15 , depending on @ xmath16 , is referred to as the _ laplace transform _ of the subordinator and has the usual form@ xmath17 \ [ ( ( x ) . \ ] ] = @ xmath18 and @ xmath19 is the lvy function such that @ xmath20 . conversely , if @ xmath15 is an integral mapping from @ xmath21 to by expression ( [ eqn : psi ] ) , then @ xmath22 is the fourier transform of the density of a subordinator . it is also known that the maximum of @ xmath15 is bernstein and the inverse fourier transform @ xmath23 is also bernstein for example @ xmath24 @ xcite . moreover , the function @ xmath25 , with @ xmath26 , is a bernstein function if and only if it is the maximum of an expression ( [ eqn : psi ] ) . clearly , @ xmath15 as defined in expression ( [ eqn : psi ] ) satisfies @ xmath27 . as a result , @ xmath15 is analytic , nondecreasing and dependent on @ xmath16 . we are given a family of bernstein functions @ xmath28 , where the @ xmath29 .the normal vectors and the @ xmath30 are the corresponding parameters . we will consider the following linear regression model : @ xmath31 where @ xmath32 , @ xmath33 ^ t $ ] , and @ xmath34 is the gaussian normal vector @ xmath35 . we aim at obtaining a rough estimate of the number of regression coefficients @ xmath36 s using a bayesian nonconvex approach . in particular , we consider the following linear regression for the regression coefficients @ xmath37 s : @ xmath38 & \ stackrel { iid } { \ sim } p ( \ sim _ 1 ) , \ \ \ sim & \ sim \ sim ( \ alpha _ { \ gamma } / 2 , \ alpha _ { \ gamma } / 3 ) , \ end { \ } \ ] ] where the @ xmath39 coefficients are referred to as the shrinkage coefficients , and the \ gamma prior has the following parametrization : @ xmath40 here , we denote @ xmath39 as @ xmath41 , that is , @ xmath42 . then @ xmath43 is regarded as the subordinator . let @ x##math44 , defined by @ xmath16 , be the laplace exponent of the subordinator . taking @ xmath45 , it can be shown that @ xmath46 is a nonconvex bernstein function of @ xmath47 on @ xmath48 . moreover , @ xmath46 is nondifferentiable at the origin because @ xmath49 and @ xmath50 . thus , one is able to prove that . in this case , @ xmath51 defines a prior for @ xmath47 . from the [ lem : subord ] it follows that the prior can be computed via the fourier transform . in particular , we have the following result . [ thm : lapexp00 ] let @ xmath15 be a nonzero bernstein function on @ xmath16 . if @ xmath52 , then @ xmath46 is a nondifferentiable and nonconvex function of @ xmath47 on @ xmath53 . furthermore , @ xmath54 where @ xmath43 is some subordinator . note that @ xmath14 is defined by the prior .parameter @ xmath55 and in the [ 2 : blrm ] we will note that @ xmath56 plays the same role as the mixing parameter ( or tuning hyperparameter ) . thus , there is an inverse relationship between the latent shrinkage parameter and the local regularization parameter ; that is , @ xmath57 . because @ xmath58 , each local ##ization parameter @ xmath39 corresponds to a local mixing parameter @ xmath59 . thus we have a nonparametric bayesian prior for the latent shrink parameter @ xmath39 . . it is also worth pointing out that @ xmath60 where @ xmath61 is a normal distribution with variance given by @ xmath62 , and @ xmath63 defines the proper density of the random variable ( denoted @ xmath64 ) . therefore , we have a prior of @ xmath65 for @ xmath47 . however , this prior can be expressed as a full scale mixture , i . e . , the prior of @ xmath66 with mixing parameter @ xmath67 . if @ xmath68 , then @ xmath6##9 is not a proper density . thus , @ xmath70 is also a ##ly - prior of @ xmath47 . however , we can define @ xmath70 as the mixture of @ xmath66 with that of @ xmath67 . in this case , we use the terminology of k - priors for the density , which is usually denoted by @ xcite . obviously , @ xmath71 is bernstein . this is an exceptional case , because we have that @ xmath72 , @ xmath73 and that @ xmath74 , where @ xmath75 is the dirac delta function of @ xmath56 , which corresponds to the wiener process @ xmath76 . we can exclude this case by giving @ xmath77 the condition ( [ eqn : 1 ] ) to obtain a strictly positive probability density . in fact , we can use the condition @ xmath78 . this in turn leads to @ xmath77 leading to @ xmath79 . in this case we exploit laplace exponents for nonconvex penalization problems . for this case , we need to find a certain##ordinator without variance , i . e . , @ xmath77 . equivalently , we can assume that @ xmath80 . we can consider the nonconvex log and exp penalties : for : ##ness ( ) : * ? ? ? the log penalty is given by @ xmath81 while the exp penalty is given by @ xmath82 formally , these two functions are the measures @ xmath16 . formally , they are @ xmath27 and @ xmath83 . it is also directly known that @ xmath84 \ { ( du ) } , \ ] ] where the lvy measure @ xmath19 is given by @ xmath85 the corresponding subordinator @ xmath86 is a gamma subordinator , because if @ xmath14 follows a gamma distribution with density @ xmath87 , with variance given by @ xmath88 we also note that the corresponding pseudo - prior is given by @ xmath89 and , if @ xmath90 , the pseudo - prior follows a gamma distribution , which is the mixture of @ xmath91 with mixing of @ xmath92 . asfor the exp case , the lvy prior is @ xmath93 . since @ xmath94 d { } = \ infty , \ ] ] , @ xmath95 $ ] is an improper prior of @ xmath47 . thus , @ xmath96 is a poisson subordinator . similarly , @ xmath14 is a poisson process with intensity @ xmath97 taking values on the interval @ xmath98 . that is , @ xmath99 which we denote as @ xmath100 . in this section we discuss the application of compound poisson subordinators to various nonconvex random variables . let @ xmath101 be a set of independent and normally distributed ( i . i . d . ) real valued random variables with power of @ xmath102 , and let @ xmath103 be a poisson process with intensity @ xmath104 that is independent of all the @ xmath105 . then @ xmath106 , for @ xmath24 , is a compound poisson process with density @ xmath107 ( for @ xmath10##8 ) , and hence @ xmath43 is not a compound poisson process . a compound poisson process is a subordinator if and only if the @ xmath105 are continuous random variables @ xcite . it is worth pointing out that if @ xmath109 is the poisson subordinator of the expression ( [ eqn : possion ] ) , this is equivalent to saying that @ xmath14 is @ xmath110 . we can study two families of nonnegative random variables @ xmath111 : nonnegative continuous random variables and nonnegative discrete random variables . thus , we have continuous and discrete compound poisson subordinators @ xmath109 . we also note that both the discrete and poisson subordinators are limiting cases of the compound poisson subordinators . in the latter case @ xmath111 is a continuous random variable . in particular , let @ xmath112 and the @ xmath111 be i . i . d . from the @ xmath113 family , where @ xmath114 , @ xmath115 and @ xmath116 . thecompound poisson subordinator can be written as : @ xmath117 the variance of the subordinator is then given by @ xmath118 we denote it by @ xmath119 . the mean and variance are @ xmath120 respectively . the fourier transform is given by @ xmath121 where @ xmath122 is a bernstein function of the function @ xmath123 . \ ] ] the generalized lvy measure is given by @ xmath124 note that @ xmath125 is a gamma measure for the random variable @ xmath126 . thus , the lvy measure @ xmath127 is referred to as the generalized gamma measure @ xcite . the bernstein function @ xmath128 was introduced by @ xcite for survival analysis . here , we consider its application to sparsity analysis . it is clear that @ xmath128 for @ xmath114 and @ xmath116 satisfy the conditions @ xmath129 and @ xmath130 . moreover , @ xmath131 is a continuous and nonconvex function of @ xmath47 .@ xmath48 , and it is an increasing function of @ xmath132 on @ xmath133 . therefore , @ xmath131 is , i . r . e . @ xmath47 , nondifferentiable at the origin . this means that @ xmath131 can be treated as a norm - preserving function . we are assuming in the limiting case that @ xmath134 and @ xmath135 . [ pro : first ] let @ xmath136 , @ xmath128 and @ xmath137 be defined by letting ( [ eqn : first _ nu ] ) , ( [ eqn : first ] ) and ( [ eqn : first _ nu ] ) , respectively . then 1 . @ xmath138 and @ xmath139 ; 2 . @ xmath140 and @ xmath141 ; 3 . @ xmath142 and @ xmath143 . this result can be obtained by performing direct algebraic integration . then [ pro : first ] tells us that the limiting cases are the nonconvex log and exp functions . thus , we havethat @ xmath14 increases in time to a gamma random variable with mean @ xmath144 and mean @ xmath145 , as @ xmath146 , and to a poisson random variable with mean @ xmath144 , as @ xmath147 . it is also known that @ xmath148 degenerates to the gamma distribution @ xcite . thus we have shown that @ xmath122 approaches to exp at @ xmath147 . we have another special case in table [ 1 : 2 ] when @ xmath149 . we refer to the time penalty as a _ long - time _ ( lfr ) penalty . for notational simplicity , we can replace @ xmath150 and @ xmath151 by @ xmath145 and @ xmath152 by the lfr function . the value of the subordinator for the lfr function is given by @ xmath153 we can say that @ xmath14 is a squared bessel process without mean @ xcite , which is a mixture of a normal delta distribution and a normal gamma distribution @ xcite . we take thereplacement of @ xmath14 by @ xmath154 . lllll & + + & lvy & @ xmath137 & subordinators @ xmath14 & + + + & @ xmath155 & @ xmath156 & @ xmath157 & [ @ xmath158 + exp & @ xmath159 $ ] & @ xmath160 & @ xmath161 & improper + lfr & @ xmath162 & @ xmath163 & @ xmath164 & + + cel & @ xmath165 $ ] & @ xmath166 & @ xmath167 & + + + + in the first example , we have a family of discrete - poisson subordinators . first , @ xmath111 is discrete and has values of @ xmath168 . and it is distributed by the distribution @ xmath169 , where @ xmath170 and @ xmath171 , with their values is distributed by @ xmath172 second , we have @ xmath173 and apoisson distribution with mean @ xmath174 , where @ xmath114 . then @ xmath14 is distributed similarly to the negative binomial ( normal ) distribution @ xcite . the probability mass function of @ xmath14 is given by @ xmath175 which is denoted by @ xmath176 . we thus know that @ xmath14 is an nb subordinator . let @ xmath177 and @ xmath178 . it can be verified that @ xmath179 has the same mean and variance as the @ xmath119 distribution . the generalized fourier transform then gives rise to a new family of bernstein functions , which are denoted by @ xmath180 . \ ] ] we refer to this family of bernstein functions as _ _ exp - bernstein _ ( cel ) _ . the first - order distribution of @ xmath181 w . r . t . @ xmath182 is given by @ xmath183 the lvy measure for @ xmath181 is given by @ xmath184 the variance is given by = 1 . we call this lvy measure a _ generalized .##isson _ _ similar to the usual levy distribution . for @ xmath128 , @ xmath181 must satisfy a set of order - preserving nonconvex penalties . thus , @ xmath181 for @ xmath114 , @ xmath185 and @ xmath116 satisfy the penalties @ xmath186 , @ xmath187 and @ xmath188 . we have a positive cel for @ xmath189 as well as the penalties @ xmath14 and @ xmath137 . in [ tab : 8 ] , where we replace @ xmath190 and @ xmath150 with @ xmath152 and @ xmath145 for notational convenience . we will consider the following cases . [ tab : 8 ] . @ xmath137 is defined as : ( [ eqn : x _ 8 ] ) for both @ xmath185 and @ xmath116 . then we find that : . @ xmath191 and @ xmath192 . @ xmath193 and @ xmath194 . : . @ x##math195 and @ xmath142 . 2 . @ xmath196 and @ xmath197 imply that @ xmath198 this implies that @ xmath127 converge to @ xmath199 , not @ xmath200 . analogously , we have the following statement of proposition [ pro : 8 ] - ( 1 ) , which implies that for @ xmath200 , @ xmath14 converges in distribution to a pseudo - variable with length parameter @ xmath144 and size parameter @ xmath145 . an alternative proof is given in appendix a . proposition [ pro : 8 ] implies that @ xmath181 converge to exp as @ xmath147 , and to log as @ xmath200 . this gives an exact relationship between @ xmath128 in expression ( [ eqn : first ] ) and @ xmath181 in expression ( [ eqn : second ] ) ; that is , they have the same conditional behaviors . we see that for @ xmath201 , @ xmath202 \ ] ] which is a combination of the log and exp functions ,and that @ xmath203 \ ] ] which is the composition of the exp and bernstein functions . in fact , the product of any two bernstein functions is also zero . thus , the product is also the maximum exponent of the subordinator , which is then the product of the subordinators corresponding to the other two bernstein functions @ xcite . this leads us to an equivalent definition for the subordinators corresponding to @ xmath122 and @ xmath204 . that is , we have the following theorem whose proof is given in appendix a . [ th : poigam ] the subordinator @ xmath14 associated with @ xmath128 is distributed according to the mixture of @ xmath205 distributions with @ xmath206 mixing , while @ xmath14 associated with @ xmath181 is distributed according to the mixture of @ xmath207 distributions with @ xmath208 mixing . therefore , the following theorem gives the limiting value of the subordinators as @ xmath145 = 0 . [ th : sum ] let @ xmath209 be a fixed point of @ xmath210 $] . 1 . if @ xmath211 where @ xmath212 $ ] or @ xmath213 , then @ xmath14 converge in probability to @ xmath56 , as @ xmath214 . 2 . if @ xmath215 where @ xmath216 \ ] ] or @ xmath213 , then @ xmath14 converge in probability to @ xmath56 , as @ xmath214 . the proof is given in appendix a . since @ xmath14 converge in probability to @ xmath56 " and @ xmath14 converge in probability to @ xmath56 , " we have that @ xmath217 if , for the given nonconvex generating function given in appendix [ 1 : 1 ] . we have the following example . that is , when @ xmath213 and for the given @ xmath116 , we have @ xmath218 \ leq \ frac { 1 } { \ gamma } { + } \ } \ leq \ frac { 1 } { \ gamma } [ = { - } \ exp ( { - } \[ ( ) ] \ leq \ frac { 1 } { \ gamma } \ , \ log ( { \ gamma } s { + } ( \ gamma ) \ leq \ , \ ] ] with and only when @ xmath219 . the proof is given in figure 1 . this proof is also shown in figure [ fig : 1 ] . [ see [ sec : exam ] with @ xmath220 and @ xmath71 . ] we apply the above poisson subordinators to the general supervised learning problem presented in figure [ sec : exam ] . for @ xmath221 , we use the same representation for the probability distribution of the @ xmath37 in the above framework . that is , we assume that @ xmath222 & \ stackrel { ind } { \ sim } & | ( t _ j | ) , \ sim ( ( \ eta _ j ) ^ { - 1 } ) , \ \ f _ { t ^ { * } ( t _ j ) } ( \ eta _ j ) & { \ propto } & \ eta _ j ^ { - 1 } f _ { * ( t _ j ) } ( \ eta _ j ) ,\ begin { x } \ ] ] which means that @ xmath223 the maximum marginal pseudo - distribution of the @ xmath37 s is given by @ xmath224 we can see from the [ 1 : 1 ] that the full conditional distribution @ xmath225 is bounded . therefore , the maximum _ a posteriori _ ( map ) estimate of @ xmath226 is based on the following optimization algorithm : @ xmath227 clearly , the @ xmath59 s are local regularization parameters and the @ xmath228 s are global distribution parameters . also , it is known that @ xmath43 ( or @ xmath55 ) is used as a subordinator w . r . t . @ xmath56 . the full conditional distribution @ xmath229 is conjugate w . r . t . the prior , which is @ xmath230 . specifically , it is an improper prior , of the form @ xmath231 . \ ] ] in the second case , we have an improper prior of the form @ xmath232 ( i . e . , @ xmath233 ) . specifically , @xmath229 is not an ordinary gamma distribution in this setting . however , based on @ xmath234 \ int _ { j = 0 } ^ { \ exp ( - \ frac { \ int _ j } { \ sigma } | b _ j | ) \ vadjust { \ sigma } \ ] ] and the proof of theorem [ 1 : 1 ] ( see appendix a ) , we know that the gamma distribution @ xmath235 is proper . however , the correction terms @ xmath236 take the form of @ xmath237 ##d . therefore , the gamma distribution algorithm is not yet known and we resort to an alternative method to approximate the model . note that if @ xmath238 is proper , the corresponding normalizing constant is given by @ xmath239 _ | b _ j | = [ \ int _ { j } ^ { \ infty } \ exp \ big [ - b _ j \ , \ , ( \ frac { | b _ j | } { \ sigma } \ sigma ) \ [ ] , ( | b _ j | / \ sigma ) , \ ] ] which is independent of @ xmath240. . , the prior probability @ xmath241 is independent of the involved term . therefore , we also have that @ xmath242 which is proper . as shown in table [ 1 : 1 ] , except for those with @ xmath243 which can be transformed into a proper prior , the associated bernstein distributions can not be transformed into proper prior . in any case , our posterior distribution is simply based on the proper pseudo - prior @ xmath244 . we ignore the extra normalizing term , because it is infinite if @ xmath244 is improper and it is independent of @ xmath240 if @ xmath244 is proper . for the @ xmath245th and @ xmath246 of @ xmath247 in the first - step of the prior computation , we compute @ xmath248 p ( \ eta _ j | p _ j ^ { ( k ) } , \ eta ^ { ( k ) } , p _ k ) } d \ eta _ j + \ log p ( \ sigma ) \ \ & \ propto - \ frac { 1 + \ alpha _ { \ sigma } } { 1 } \ log \ .{ - } \ frac { \ | { \ bf a } { - } { \ bf x } { \ bf b } \ | _ p ^ 2 + \ sum _ { \ sigma } } { - \ sigma } - ( p + 1 ) \ | \ | \ \ & \ | - \ frac { - } { \ sigma } \ | _ { p = 1 } ^ 2 . we have some solutions that are independent of parameters @ xmath240 and @ xmath226 . in fact , we only need to compute @ xmath249 in the e - step . considering that @ xmath250 and computing the terms w . r . t . @ xmath236 on both sides of the above equation , we find that @ xmath251 the e - step is @ xmath252 w . r . t . @ xmath253 . in addition , it is known that : @ xmath254 the # # ##a is similar to the linear convergence algorithm ( lla ) of @ xcite . however , it shares the linear convergence algorithm found in @ xcite and @ xcite . subordinators allow us to compute the lineardistinguish between the local ##ization parameters @ xmath59 s and the local shrinkage parameter @ xmath39 s ( or @ xmath41 ) . however , when we perform the map calculation , it is challenging how to compute these local ##ization parameters . we propose adding ecme ( for " / expectation maximization either " ) to @ xcite for information about the @ xmath37 s and @ xmath59 s simultaneously . for this purpose , we suggest adding @ xmath59 s to @ xmath255 , or , @ xmath256 because the full conditional distribution is not proper and given by @ xmath257 \ sim \ ga \ sim ( \ alpha _ { t } , ) / [ \ psi ( | x _ t | / \ i ) + \ alpha _ { t } ] \ n ) . \ ] ] recall that we here compute the full conditional distribution by using the proper sub - function @ xmath238 , because our used prior functions in the [ 1 : exam ] do not have proper priors . however , if @ xmath238 were proper , the full normalizing function would rely on @ xmath59 . as a result, the full conditional distribution of @ xmath59 is either no longer known or is not analytically determined . figure [ tab : graphal0 ] - ( 1 ) shows the above model for the linear inverse linear regression , and figure [ tab : alg ] shows the ecme procedure where the e - step and m - step are respectively equivalent to the e - step and the m - step of the above algorithm , with @ xmath258 . the cme - step updates the @ xmath59 prior with @ xmath259 in order to make sure that @ xmath260 , it is necessary to check that @ xmath261 . for the other example , we check @ xmath262 . we compare experiments with the prior @ xmath263 for comparison . this prior is induced from the @ xmath264 - norm distribution , so this is a useful specification . moreover , the full conditional distribution of @ xmath59 w . i . t . its corresponding prior @ xmath265 is \ gamma ; that is , @ xmath257 \ sim \ ga \ sim ( { \ math _ t } { + } 2 , \ ; \ /( { \ beta _ j } { + } \ sqrt { | beta _ j | / \ } } ) \ } ) . \ ] ] thus , the cme - code for computing the @ xmath59 algorithm is given by @ xmath266 the fundamental theorem of the ecme algorithm was proved by @ xcite , who showed that the ecme algorithm retains the monotonicity property from the standard algorithm . moreover , the ecme algorithm based on pseudo - priors was also presented by @ xcite . . the general form of the ecme algorithm [ s = " < , < " , ] our analysis is based on a set of input data , which are generated according to @ xcite . in particular , we generate the following input data : small , " medium " and large . " data s : : : @ xmath267 , @ xmath268 , @ xmath269 , and @ xmath270 . " @ xmath271 matrix with @ xmath272 on the diagonal and @ xmath273 on the semi - diagonal . data t : : : @ xmath274 , @ xmath275 , @ xmat##h276 : @ xmath277 non - zeros such that @ xmath278 and @ xmath279 , and @ xmath280 . # l : : : @ xmath281 , @ xmath282 , @ xmath283 , and @ xmath284 ( all samples ) . for each sample block , we choose @ xmath285 and mean @ xmath286 such that each sample of @ xmath286 is drawn from a standard normal distribution with mean @ xmath287 and data matrix @ xmath270 , @ xmath288 , and @ xmath289 . we create a linear model @ xmath290 with the linear model @ xmath286 and gaussian noise . we choose @ xmath240 such that the signal - to - noise ratio ( snr ) is a specified value . in the setting of @ xcite , we use @ xmath291 in all the samples . we use the standard prediction algorithm ( spe ) to achieve the best prediction results . the best achievablethat for spe is @ xmath272 . the prediction accuracy is determined by the correctly predicted zeros and correctly predicted ones in @ xmath292 . the snr and spe are defined by @ xmath293 for each zero ##set , we have training data of size @ xmath294 , with small test data and large data , each of size @ xmath295 . for each algorithm , the optimal fine tuning parameters are chosen by cross validation based on minimizing the average of errors . with the size @ xmath292 based on the training data , we compute spe on the test data . this process is repeated @ xmath296 times , and we calculate the average and standard deviation of spe and the proportion of zero - nonzero error . we use ` ` ' ' to denote the proportion of correctly predicted zero ##s in @ xmath226 , that is , @ xmath297 ; if all the nonzero errors are correctly predicted , this score should be @ xmath298 . we display the results in table [ tab : toy2 ] . it is assumed that our setting in table [ tab : graphal0 ] - ( 1 )is better than the other two settings in the [ fig : graphal0 ] - ( b ) and ( c ) in both the prediction performance and model learning properties . however , when the size of the dataset takes more steps , the prediction performance of the second setting becomes worse . the several nonconvex penalties are good , but they outperform the others . thus , we see that log , exp , lfr and cel all outperform @ xmath264 . the @ xmath264 problem indeed arises from the problem of rounding error during the em ##ulation . as we see , the priors induced from lfr , cel and exp as well as those with @ xmath299 are incorrect , but the em ##s from @ xmath264 are correct . the experimental results show that these two methods work well , even better than the theoretical methods . vs . @ xmath300 on data p " and data m " where @ xmath301 is the permutation of @ xmath302 and that @ xmath303 . ] recall that in our model the error variable @ xmath37 corresponds to a very fine tuning variable @ xmat##h59 . thus , it is possible to empirically test the inherent relationship between @ xmath37 and @ xmath59 . let @ xmath304 be the estimate of @ xmath59 obtained from our ecme algorithm ( alg . " ) , and @ xmath305 be the permutation of @ xmath306 such that @ xmath307 . " [ fig : tb1 ] depicts the change of @ xmath308 to . @ xmath300 with f , exp , lfr and cel on data s " and data l . " we observe that @ xmath308 is decreasing w . r . t . thus , @ xmath308 = 0 when @ xmath300 takes a negative value . a similar effect is also observed for data s . " this thus shows that the subordinator is a natural selection algorithm for data selection . in this way we have introduced subordinators into the field of nonconvex code analysis . this leads us to a new approach for constructing sparsity - preserving pseudo - codes . in particular , we have illustrated the existence of two different poisson subordinators: the compound poisson - subordinator and the compound - subordinator . in addition , we have shown the difference between the two families of compound poisson subordinators . that is , we have shown that the two families of compound poisson subordinators have the same statistical behaviors . moreover , their distributions at any point have the same mean and variance . we have developed the ecme algorithms for solving sparse learning problems based on the nonconvex log , exp , lfr and cel methods . we have done the experimental work with the state - of - the - art approach . the results have shown that our nonconvex log method is very useful for high - dimensional bayesian problems . our work can be cast into a sparse estimation framework . it is very interesting to develop a fully sparse framework based on the mcmc estimation . we would like to address this issue in our research . consider that @ xmath310 & = \ log \ log [ 1 - \ frac { s } { 1 { + } \ gamma } \ exp ( - \ frac { \ gamma } { 1 { + } \ gamma } \ big s ) \ gamma ] - \ log \k [ 1 - \ frac { 1 } { 1 { + } \ big } \ big ] \ \ & = \ sum _ { k = 1 } ^ { \ infty } \ frac { 1 } { k ( 1 { + } \ big ) ^ k } \ big [ 1 - \ exp \ big ( { - } \ frac { \ rho } { 1 { + } \ big } ) \ big ( \ big ) \ big ] \ \ & = \ sum _ { k = 1 } ^ { \ infty } \ frac { 1 } { k ( 1 { + } \ big ) ^ k } \ int _ { 1 } ^ { \ infty } ( - - \ exp ( - u . ) ) \ int _ { \ frac { \ big } \ big } { 1 { + } \ gamma } } ( - ) { 1 . \ big { - } \ ] ] we can see that @ xmath311 . we can give an explicit definition of the [ 1 : 2 ] - ( - ) , which is easily obtained from the above formula . let @ xmath312 have the action . @xmath313 and is also in distribution @ xmath314 . if @ xmath315 converge to a positive , then @ xmath316 , @ xmath317 converge in distribution to a positive random number with scale @ xmath315 and scale @ xmath272 . since @ xmath318 we have that @ xmath319 and that @ xmath320 and @ xmath321 this leads us to @ xmath322 therefore , we have that @ xmath323 is a mixture of @ xmath324 with @ xmath325 and . that is , @ xmath326 , @ xmath327 , @ xmath328 and @ xmath329 , we have that @ xmath330 we can get a mixture of @ xmath331 with @ xmath332 which yields @ xmath333 . let @ xmath334 , @ xmath335 , @ xmath336 and @ xmath337 . then , @ xmath338 since @ xmath339 =[ $ ] , we also need to consider the case that @ xmath213 . note that @ xmath119 , whose mean and variance are @ xmath340 whenever @ xmath213 . for chebyshev ##ski ##m , we have that @ xmath341 hence , we have that @ xmath342 hence , we have that ( $ ) . we also have that @ xmath343 which implies that @ xmath344 for @ xmath345 . hence , we have that @ xmath346 \ leq0 $ ] . as a result , @ xmath347 for @ xmath345 . similarly for @ xmath348 , which is also derived from that @ xmath349 since @ xmath350 = \ frac { \ gamma } { \ exp ( \ gamma s ) } - \ frac { \ gamma } { s + \ gamma s } < [ $ ] for @ xmath345 , we have that @ xmath351 for @ xmath345 . now note that @ xmath352 \ gamma _ { s = s } ^= \ psi ^ { - 1 } \ exp \ big ( - t _ j \ big \ big ( \ frac { | b _ j | } { \ sigma } \ big ) \ big ) . \ ] ] to prove that @ xmath353 is correct , one is to show that @ xmath354 \ math _ { i = 1 } ^ 2 \ psi ^ { - 1 } \ exp \ big ( - t _ j \ big \ big ( \ frac { | b _ j | } { \ sigma } \ big ) \ big ) ^ { \ bf z } < \ infty } . \ ] ] it is also known that @ xmath355 \ nonumber \ \ & = \ exp \ big [ { - } \ frac { 1 } { 2 \ sigma } ( { \ bf x } { - } { \ bf z } ) ^ 2 { \ bf z } ^ 2 { \ bf z } ( { \ bf x } - { \ bf z } ) \ big ] \ big \ exp \ big [ - \ frac { 1 } { 2 \ sigma } { \ bf z } ^ 2 ( { \ bf z} _ { - { \ bf y } ( { \ bf y } ^ { { \ bf x } ) ^ { + } { \ bf x } ^ t ) { \ bf y } \ [ ] , \ { { + } \ ] ] where @ xmath356 and @ xmath357 are the post - penrose distribution ##geny of the @ xmath358 @ xcite . then we consider the well - known cases that @ xmath359 and @ xmath360 . note that if @ xmath358 is nonsingular , then @ xmath361 . in this case , we consider a standard standard normal distribution @ xmath362 . \ , we consider a conventional standard normal distribution @ xmath363 @ xcite , the variance of which is given by @ xmath364 . \ ] ] . @ xmath365 , and @ xmath366 , @ xmath367 , are the corresponding distributions of @ xmath358 . in this case , we can write @ xmath368 . \ , @ xmath369 = { \ bf y } <\ infty } $ ] . it also preserves the identity of @ xmath370 because @ xmath371 \ prime _ { j = 1 } ^ p \ exp \ big ( { - } t _ j \ psi \ big ( \ frac { | b _ j | } { \ sigma } \ big ) \ big ) \ leq \ exp \ big [ { - } \ frac { - } { { \ sigma } \ | { \ bf y } - { \ bf x } { \ bf y } \ | _ j ^ p \ big ] . \ ] ] we can see that @ xmath372 \ prime _ { j = 1 } ^ p \ exp \ big ( - t _ j \ psi \ big ( \ frac { | b _ j | } { \ sigma } \ big ) \ big ) . \ ] ] = @ xmath373 { \ bf y } $ ] . since the matrix @ xmath374 is positive semidefinite , we have @ xmath375 . based on equation ( [ eqn : pf01 ] ) , we also have @ xmath376 \ big##propto ##de ( { \ bf x } | { \ bf z } , \ big ( { \ bf z } ^ { { \ bf x } ) ^ { + } ) { \ sigma } ( \ sigma | \ frac { \ alpha _ { \ sigma } { + } } { + } 2p { - } 2 } { - } , \ beta { + } \ alpha _ { \ sigma } ) . \ ] ] next , we have that @ xmath377 d { \ bf b } d \ sigma } < \ infty , \ ] ] and finally , @ xmath377 \ big _ { j = 1 } ^ { \ exp \ big ( - t _ j \ big \ big ( \ frac { | t _ j | } { \ psi } \ big ) \ big ) ^ { \ bf b } d \ sigma } < \ infty . \ ] ] and @ xmath378 is correct . next , we have @ xmath379 } { \ big ^ { \ frac { 1 + \ alpha _ { \ sigma } + 2p } { 2 } + 1 } } \ big _ { j = 1} ^ p \ big \ { \ exp \ big ( { - } t _ ) \ big \ big ( \ frac { | b _ j | } { \ sigma } \ big ) \ big ) \ frac { b _ j ^ { { \ alpha _ t } { - } } } \ exp ( { - } { \ beta _ t } b _ j ) } { \ big ( { \ alpha _ t } ) } \ big \ } \ \ & \ triangleq \ ( { \ bf t } , \ sigma , { \ bf t } ) . \ big { \ } \ ] ] in this way , we have @ xmath380 } { \ gamma ^ { \ frac { 1 + \ alpha _ { \ sigma } + 2p } { - } + 1 } } \ alpha _ { t = 1 } ^ { \ frac { - } { \ gamma ( { \ alpha _ t } { + } \ big \ big ( \ frac { | b _ j | } { \ sigma } \ big ) \ big ) ^ { { \ alpha _ t } } } ^ { \ bf t } { \ big } . \ ] ] according to the above ,, we also have that @ xmath381 because @ xmath382 . as a result , @ xmath383 is true . now , consider the case that @ xmath384 . that is , @ xmath385 and @ xmath386 . in this case , if @ xmath387 , we obtain @ xmath388 and @ xmath389 . as a result , we obtain the corresponding correlation function @ xmath390 . thus , the results still hold . polson , n . j . and smith , j . g . ( 2010 ) . ` ` act globally , not locally : in statistical statistics and applications . ' ' in bernardo , j . m . , bayarri , m . a . , berger , m . a . , dawid , m . p . , heckerman , m . , smith , j . g . g . , and west , j . ( eds . ) , _ global _ statistics _ . cambridge university press . the authors would like to thank the editor and the other referees for their constructive work and comments on the original version of this article . the authors would also like to thank the associate editor forgiving more detailed comments on the drafts . this work has been funded in part by the national science foundation of canada ( no . 61070239 ) .