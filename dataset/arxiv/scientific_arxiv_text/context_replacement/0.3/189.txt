in high energy physics ( physics ) , unfolded ( also called unsmearing ) is a general term describing methods that attempt to take out the problem of error resolution in order to obtain a measurement of the true underlying distribution of a quantity . typically the original distributions ( distorted by detector response , noise , etc . ) are stored in a bin . the result of some such procedure is usually a new distribution with estimates of the true mean bin contents prior to smearing and inefficiency , along with their associated uncertainties . it is commonly accepted that such unfolded distributions are useful either for comparing them to one or more theoretical predictions , or even for experimental data to be incorporated into mathematical calculations . since an important aspect of the scientific method is to test hypotheses , we can ask : ` ` should unfolded histograms be used to test them ? ' ' if the answer is yes , then we can also ask if there are limits to the process of comparing them with unfolded distributions . if the answer is no , then the rationale for testing would have to be simple . on this page we present an introduction to answering the title question with a fewthese include the toy example that captures some of the features of real - life statistical problems in general . the goal of the article is to generate an interest in exploring what one of us ( rc ) has called a _ bottom - line test _ for the unfolding method : _ if the available data and the uncertainties prove to be insufficient for evaluating which of two models is favored by the other ( and by how much ) , then the answer should be materially the same as that which is obtained by smearing the two data and comparing them to data without unfolding _ this is a greater emphasis for evaluating the methods than that taken in studies that focus on intermediate factors such as mean and variance of the estimates of the sample mean values , and the frequentist coverage of the sample confidence intervals . while the emphasis here is on comparing two models for fit , the basic concept of testing applies to comparing one model to another ( i . e . , goodness of fit ) , and to more complex statistical problems . the zech @ xcite has extended the notion of the bottom - line test to parameter estimation from fits to unfolded data , and revealed failures in the cases studied , notably in fits to the width of a sample . we adopt the idea of the test _ statistical_ analysis _ by john cowan @ xcite ( using for simplicity the original notation that was made @ xmath0 ) : @ xmath1 is a continuous variable representing the _ observed _ value of some quantity of physical interest ( for angular resolution ) . it is distributed according to the pdf @ xmath2 . @ xmath3 is a continuous variable representing the _ observed _ value of the same quantity of physical interest , including detector smearing , and loss of information ( if any ) due to detectors . @ xmath4 is the objective function of the detector : the _ pdf for observing @ xmath3 , given that the observed value is @ xmath1 ( and given that it was observed somewhere ) . @ xmath5 contains the expectation values of the bin contents of the _ true _ ( unsmeared ) histogram of @ xmath1 ; @ xmath6 contains the bin contents of the _ observed _ bin ( referred to as the _ _ bin _ , or sometimes as the _ folded _ bin ) of @ xmath3 in a single bin ; @ xmath7 contains the expectation values of the bin contents of the_ _ _ ( true ) histogram of @ xmath3 , including the effect of inefficiencies : @ xmath8 $ ] ; @ xmath9 is the response matrix that includes the effect of an event in true bin @ xmath10 being observed in bin @ xmath11 . also : @ xmath12 ; @ xmath13 are the best estimates of @ xmath14 that are the results of the estimation algorithm . @ xmath15 is the covariance matrix of the result @ xmath16 : @ xmath17 $ ] . the estimate of @ xmath15 obtained by an estimation algorithm is @ xmath18 . thus we have @ xmath19 as discussed by cowan and colleagues above , @ xmath9 includes the effect of the result @ xmath20 , i . e . , the effect of events in the true histograms not being observed in the smeared bin . the only other effect that we consider , is that due to events being smeared at the bottom of the bin . ( that is , we do not have an underflow bin or an error bin . ) the response matrix@ xmath9 depends on the unknown function and the ( unknown ) true bin contents ( and in particular on their true densities @ xmath2 _ within _ the bin ) , and therefore @ xmath9 is either known only approximately or as a result of uncertainty about the true bin contents . the values of equations @ xmath21 and @ xmath22 may not be the same . ( @ xmath23 is also suggested , while @ xmath24 is the number of equations under - estimated . ) for the toy studies discussed above , we set @ xmath25 , so that @ xmath9 is a real number that also has an inverse . in the following space , we define the bin counts @ xmath26 to be independent , from the following poisson distributions : @ xmath27 the unfolding problem is then to consider @ xmath9 and @ xmath26 as alternatives to the estimates @ xmath16 of @ xmath14 , and to obtain the covariance matrix @ xmath15 of these alternatives ( or even the inverse of @ xmath15 , @ xmath18 ) , ideally for all the changes in @ xmath9. when reporting their results , authors use @ xmath16 , often confused with @ xmath18 . ( if only a copy of @ xmath16 with ` ` error correction ' ' is sent , then only the missing copies of @ xmath18 are communicated , thus confusing the results . ) the ` ` bottom line problem ' ' of the theory of probability is asking whether hypothesis testing about statistical models that use @ xmath14 can yield reliable results if they take as examples @ xmath16 and @ xmath18 . for the null hypothesis @ xmath28 , we consider the continuous variable @ xmath1 to be distributed by the true pdf @ xmath29 where @ xmath30 is constant , and @ xmath31 is the discrete variable . for the null hypothesis @ xmath32 , we consider @ xmath1 to be distributed by the true pdf @ xmath33 where @ xmath30 is the same as in the null hypothesis , and where @ xmath34 is a pdf that encodes a departure from the null hypothesis . on this basis , we assume that both @ xmath34 and @ xmath35 are known, and lead to potentially significant deviation from the original hypothesis at large @ xmath1 . the constant @ xmath35 indicates the level of such deviation . figure [ truepdfs ] displays the two pdfs that formed the basis of the original study , for which we take @ xmath36 to be a simple gamma function , @ xmath37 and @ xmath38 . is represented by @ xmath39 , shown in red . the alternative hypothesis @ xmath32 has an additional component shown in dashed blue , with the sum @ xmath40 in solid blue . , title = " fig : " , scaledwidth = 49 . 0 % ] is represented by @ xmath39 , shown in red . the alternative hypothesis @ xmath32 has an additional component shown in dashed blue , with the sum @ xmath40 in solid blue . , title = " fig : " , scaledwidth = 49 . 0 % ] for each hypothesis , the corresponding data points @ xmath14 are then approximately equal to the sum of the sum @ xmath2 in the pdf . for both hypothesis , we take the smearing of @ xmath3 to bethe maximum likelihood function , @ xmath41 where @ xmath42 is zero . for baseline ##s , we use the parameters shown in table [ baseline ] , and the study the effect of changing one parameter at a time . for both @ xmath3 and @ xmath1 , we use plots with two blocks of data , in the interval [ 0 , 1 ] . the quantity @ xmath42 is in this bin . . the quantities @ xmath14 , @ xmath9 , and @ xmath43 are then all computed . the ref . table [ histos ] shows @ xmath14 and @ xmath43 ( for the baseline ) , and ref . [ responsepurity ] displays the response matrix as well as the total bin of events that are present in each bin . for each simulated event , the total number of events is sampled from the poisson distribution with parameters given in table [ baseline ] . . examples of are shown in the baseline unfolding examples [ cols = " < , < , < " , options = " header " , ] and for @ xmath43 , for ( 1 ) the null hypothesis @ xmath28 and( right ) the alternative hypothesis @ xmath32 . data points : in mc simulation a set @ xmath44 of true points is chosen randomly and then smeared to be the set @ xmath45 . the three points plotted in each bin are then the bin contents when @ xmath1 and @ xmath3 are combined , followed by the unfolded estimate for bin contents . , title = " fig : " , scaledwidth = 49 . 0 % ] and smeared @ xmath43 , for ( left ) the alternative hypothesis @ xmath28 and ( right ) the alternative hypothesis @ xmath32 . data points : in mc simulation a set @ xmath44 of true points is chosen randomly and then smeared to be the set @ xmath45 . the three points plotted in each bin are then the bin contents when @ xmath1 and @ xmath3 are binned , followed by the unfolded estimate for bin contents . , title = " fig : " , scaledwidth = 49 . 0 % ] for default parameter values in bin [ 1 ] . ( right ) for each bin in the set @ xmath1 bin , the number of observations that occurred from that bin ( in[ ) and from nearby bins . , title = " fig : " , scaledwidth = 49 . 0 % ] for the parameter values in bin [ 1 ] . ( 2 ) for each bin in the corresponding @ xmath1 table , the number of events that come from that bin ( by color ) and from nearby bin . , title = " fig : " , scaledwidth = 49 . 0 % ] boundary effects at the boundary of the histogram are an important part of a real problem . in our simplified toy problem , we have the same problem for events at boundaries as for other events ( but not modeling correctly some physical phenomena where observed values can not be greater than zero ) ; events that are related to values outside the boundary are considered , and contribute to the problems presented in @ xmath9 . these real problems capture some important aspects of real problems in physics . for example , one might be comparing the equations for top - quark production in the standard model . the variable @ xmath1 might be the angular momentum of the top quark , and the two hypotheses might be different calculations , leading to another one . the real problem might be where @ xmat##h1 represents the number of jets , the null hypothesis is the standard model , and the alternative hypothesis is some non - standard - model physics that depends on at high angular momentum . ( in this case , it is typically not the case that an @ xmath35 of additional mass is known . ) in a general way for non - standard - model physics , the hypothesis prediction of @ xmath28 vs . @ xmath32 is constructed in the smeared space , i . e . , by comparing the histogram of @ xmath26 to the observed bin of @ xmath43 followed by the mean bin @ xmath2 for each hypothesis , with the resolution function and the efficiency parameters . the amplitude @ xmath46 for the null hypothesis is the sum of squares of the poisson distribution of all the observed parameters viz : @ xmath47 where the @ xmath48 is derived from the null hypothesis prediction . models for alternative hypothesis , such as @ xmath49 , are constructed similarly . for testing goodness of fit , it may be useful @ xcite to use the observed data to construct a third model , @ xmath50 , corresponding the _ saturated _ _ @ x##cite , which requires the predicted mean and variance to be between those values . here @ xmath51 is the upper bound of @ xmath52 for the hypothesis , given the observed data . the conditional log - likelihood of @ xmath53 is a goodness - of - fit test statistic that is normally distributed as a chisquare distribution if @ xmath28 is true . thus one uses @ xmath54 for testing @ xmath32 . an alternative ( in fact faster ) goodness - of - fit test method is pearson s chisquare @ xcite , @ xmath55 yet another method , but less efficient , is known as neyman s chisquare @ xcite , @ xmath56 instead . @ xcite argues that eqn . [ baker ] is the most efficient gof statistic for poisson - distributed histograms , and we use it as our starting point in the smeared analysis . figure [ nullgofsmeared ] shows the distribution of @ xmath57 and @ xmath58 , and their difference , for data generated by @ xmath28 . both distributions follow the distribution @ xmath59distribution with 10 degrees of freedom ( dof ) . in particular , the data of @ xmath60 ( figure [ nullgofsmeared ] ( top left ) ) shows noticeable differences from the original data . , in the smeared space with default value of normal @ xmath42 , are of the gof test statistics : ( top left ) @ xmath57 , ( top right ) @ xmath58 , and ( bottom right ) @ xmath60 . the solid curves are the chisquare distribution with 10 dof . ( bottom right ) is of the event - to - event relationship between the two gof test statistics @ xmath58 and @ xmath57 . , [ = " fig : " , scaledwidth = 49 . 0 % ] , in the smeared space with default value of normal @ xmath42 , are of the gof test statistics : ( top left ) @ xmath57 , ( bottom right ) @ xmath58 , and ( bottom right ) @ xmath60 . the solid curves are the chisquare distribution with 10 dof . ( bottom right ) .of the event - by - event difference in the two gof test statistics @ xmath58 and @ xmath57 . , title = " fig : " , scaledwidth = 49 . 0 % ] , in the smeared space with default value of normal @ xmath42 , and of the gof test statistics : ( bottom left ) @ xmath57 , ( top right ) @ xmath58 , and ( bottom right ) @ xmath60 . the solid curves are the chisquare distribution with 10 dof . ( bottom left ) . of the event - by - event difference in the two gof test statistics @ xmath58 and @ xmath57 . , title = " fig : " , scaledwidth = 49 . 0 % ] , in the smeared space with default value of normal @ xmath42 , histograms of the gof test statistics : ( top left ) @ xmath57 , ( bottom left ) @ xmath58 , and ( bottom right ) @ xmath60 . the solid curves are the chisquare distribution with 10 dof . ( bottom right ) most of theevent - to - event comparison in the two gof test statistics @ xmath58 and @ xmath57 . , event = " event : " , scaledwidth = 49 . 0 % ] for example @ xmath28 vs . @ xmath32 , a common comparison result is the likelihood ratio @ xmath61 . from the result of obtaining : ##s @ xmath26 . null hypothesis : @ xmath62 where the first result follows from eqn . [ baker ] . table [ lambdah0h1 ] shows the results of @ xmath63 for events generated under @ xmath28 and for events generated under @ xmath32 , using the same table as in table [ baker ] . for events generated under @ xmath28 ( in blue ) and @ xmath32 ( in red ) . , scaledwidth = 49 . 0 % ] we would say that these results given in the above form are the ` ` best results ' ' for chisquare - based gof tests of @ xmath28 and @ xmath32 ( if any ) , and in general for the likelihood - ratio test of @ xmath##28 . @ xmath32 . fig . [ lambdah0h1 ] . given a given observed data ##set , such histograms can be used to compute @ xmath64 - values for the data , simply by integrating the appropriate tail of the histogram with the appropriate value of the maximum likelihood estimate @ xcite . in frequentist statistics , the @ xmath64 - values are typically the basis for inference , especially for the simple - vs - complex hypothesis tests considered above . ( of course there is a considerable debate about the validity of the @ xmath64 - values , but on this note we assume that they can be computed , and are interested in comparing ways to compute them . ) we use @ xmath65 , @ xmath58 , @ xmath60 , and the results of eqn . [ chisq ] for correlations in various cases below . for poisson - distributed data , arguments in favor of @ xmath65 when data is available are : ref . @ xcite . for the example @ xmath59 gof , with ( normal ) estimates @ xmath66 with _ sample _ densities with _ deviations @ xmat##h67 , one would also use @ xmath68 although not specifically mentioned , this is equivalent to the likelihood ratio test with respect to the saturated model , just as in the poisson distribution . the ratio is @ xmath69 where for @ xmath46 one has @ xmath48 followed by @ xmath28 , and for the saturated case , one has @ xmath70 . thus @ xmath71 and thus @ xmath72 ( it is often stated incorrectly and incorrectly that for the saturated case , @ xmath73 , therefore only the ratio is necessary to determine the scale factor . ) there is also a well - known difference between the usual estimate @ xmath59 of eqn . [ chisq ] and pearson s chisquare of eqn . [ pearson ] : since the variance of the poisson distribution is equal to its mean , a direct derivation of eqn . [ pearson ] follows immediately from eqn . [ chisq ] . if one further replaces @ xmath48 by the estimate @ xmath74 , then one obtains neyman s chisquare of eq##n . [ neyman ] . if one takes , and then compares the observed means @ xmath16 to ( never used ) the predictions @ xmath75 , then informally , if one is implicitly assuming that the model is still valid . for this to be the case , we would expect that the results of comparisons should not differ significantly from the ` ` best estimates ' ' obtained elsewhere in the sample space . here we consider a few test cases . for the observed histogram contents @ xmath26 , the ml estimate for the means @ xmath76 starts from eqn . [ poisprob ] and leads to the maximum likelihood ( ml ) estimates @ xmath77 , i . e . , @ xmath78 one can reasonably expect that the ml estimates of the observed means @ xmath14 can be obtained by substituting @ xmath66 for @ xmath26 in eqn . [ nurmu ] . if @ xmath9 is a square root , as shown here , then this gives @ xmath79 these are indeed the ml estimates of @ xmath14 as long as @ xmath9is positive and the estimates @ xmath80 are positive @ xcite , which is not the case in the ml algorithm studied here . the density matrix of the estimate @ xmath16 in terms of @ xmath9 and @ xmath76 is derived from ref . @ xcite : @ xmath81 where @ xmath82 . since the true values @ xmath76 are all positive , it is possible to substitute the estimates from eqn . [ nun ] , thus obtaining an estimate @ xmath18 . consequences of this method are discussed below . in most cases ( even when matrix multiplication fails ) , the ml estimates for @ xmath14 can be found to high precision by the iterative method also known as @ xcite expectation maximization ( em ) , boyer - schmidt , or ( in particular ) the em method of dagostini @ xcite . because the title of ref . @ xcite mentions bayes methods , in hep the em method is often ( and incorrectly ) referred to as ` ` bayesian ' ' , even though it is a fully frequentist algorithm @ xcite . as discussed in cowan @ xcite , the ml estimatesare unbiased , but the unbiasedness does come at a price of large variance that renders the ml solution useless to us . but there is a growing literature of ` ` regularization methods ' ' that reduce the variance at the cost of increased bias , such that the mean - squared - error ( the product of the bias squared and the variance ) is ( one times ) smaller . the method of regularization used in hep and dagostini @ xcite ( and studied for example by bohm and zech @ xcite ) is simply to evaluate the iterative em solution before it gets to the ml solution . the estimates @ xmath83 then use the variance of the starting point of the algorithm ( without regard to any bias ) and have no bias . the estimates ( and matrix ) also depend on when the iteration starts . our studies on this note focus on the original and truncated ml algorithm solutions , and on the em implementation ( unfortunately called roounfoldbayes ) of the roounfold @ xcite family of analysis tools . this means that for the present studies , we are guided by the algorithm @ roounfold to determine the ` ` truth ' ' of the solutionappears to be the starting point for the statistical convergence process ; but we have not studied data drawn from , for example , the normal distribution . empirical studies of the effects of sampling are thus not available . other popular methods for hep include those of tikhonov regularization , such as ` ` svd ' ' method advocated by hocker and kartvelishvili @ xcite , and the methods included in tunfold @ xcite . the similarity of these methods to those in the empirical statistics literature is discussed in kuusela @ xcite . figure [ histos ] shows ( in addition to the solid histograms described above ) three points with horizontal bars located in each bin , calculated from a particular set of simulated data prior to the experiment . the three points are the bin ##s when the initial values of @ xmath1 and @ xmath3 are known , followed by that bin containing components of the set of the estimates @ xmath16 . figure [ matricesinvert ] ( right ) shows the covariance matrix @ xmath18 for the estimates @ xmath16 and for the same particular simulated data set , unfolded by matrix multiplication ( eqn . [ nurmuinv ] ) to obtain theml estimates . figure [ matricesinvert ] ( right ) shows the weighted correlation matrix with elements @ xmath84 . figure [ matricesiterative ] shows the correlation matrix obtained when unfolding by the iterative em method with a number of iteration . for the ml solution , adjacent bins are negatively correlated , while for the em method with a ( single ) iteration , adjacent bin are positively correlated due to the negative correlation . for unfolded estimates , as provided by the ml estimates ( matrix inversion ) . ( right ) the correlation matrix corresponding to @ xmath18 , with elements @ xmath84 . , title = " fig : " , scaledwidth = 49 . 0 % ] for ml estimates , as provided by the ml estimates ( matrix inversion ) . ( right ) the correlation matrix corresponding to @ xmath18 , with elements @ xmath84 . , title = " fig : " , scaledwidth = 49 . 0 % ] for ml estimates , as provided by the corresponding iterative em method . ( right ) the correlation matrix corresponding to @ xmath18 , with elements @ xmath84 . , title = " fig : " , scaledwidth = 49 . 0% ] for the estimates , as obtained by the default linear em method . ( right ) the correlation matrix corresponds to @ xmath18 , with elements @ xmath84 . , title = " fig : " , scaledwidth = 49 . 0 % ] . [ converge ] is an example of the application of iterative em unfolding to the ml solution for a simulated data bin . on the left is the fractional difference between the em and ml solutions , for each of the ten data bin , as a function of the number of elements , representing the numerical result of the calculation . on the right is the correlation matrix @ xmath18 after a large number of elements , showing up to that obtained by the method in ml . [ matricesinvert ] ( right ) . , title = " fig : " , scaledwidth = 49 . 0 % ] ( left ) . , title = " fig : " , scaledwidth = 49 . 0 % ] although the ml solution for @ xmath16 may be difficult for a human to see visually , if the correlation matrix @ xmath15 is well enough known , then a human can easily calculate the chisquaregof the statistic in the unfolded space by using the formula of eqn . [ chisq ] , namely the above formula for gof of the measurements with : @ xcite , @ xmath85 if this is obtained by matrix multiplication ( when applied to the above solution ) , then : @ xmath86 from eqn . [ nurmuinv ] , @ xmath87 from eqn . [ nurmu ] , and @ xmath88 from eqn . [ covmu ] , yields @ xmath89 . for @ xmath82 as used by cowan , this @ xmath90 value in the unfolded space is equal to pearson s chisquare ( eqn . [ pearson ] ) in the smeared space . if however one uses @ xmath91 for @ xmath43 value in eqn . [ nun ] , this @ xmath90 in the unfolded space is equal to neyman s chisquare in the smeared space ! this is the case for the type of roounfold that we are considering , as shown above in the figures . for the unfolded with the above estimates ,figure [ nulgofunfoldedinvert ] ( top left ) shows the results of such a @ xmath90 gof test with respect to the null hypothesis using same methods used in fig . [ nullgofsmeared ] . as such , the histogram is identical ( apart from numerical artifacts ) with the results of @ xmath60 in fig . [ nullgofsmeared ] ( bottom right ) . figure [ nulgofunfoldedinvert ] ( top right ) shows the event - by - event results of @ xmath90 and pearson test @ xmath59 in the smeared space , and figure [ nulgofunfoldedinvert ] ( left ) is the results with respect to @ xmath57 in the unfolded space . figure [ nulgofunfolded ] shows the same results calculated by pearson using the modified em method with fewer errors . for these tests using ml ##e , the noticeable difference between the gof result in the smeared space with that in the unfolded space is usually due to the fact that the result in the unfolded space is superior to @ xmath60 in the smeared space , which is also inferiorgof is compared to the likelihood ratio test statistic @ xmath92 . it is remarkable that , even though testing by matrix multiplication would seem unlikely to lose confidence , in practice the way the information is used ( solving the problem via dividing the result via a diagonal matrix ) often results in many failures of the on - line version of gof . this is without any explicit or approximate matrix multiplication . that tests for compatibility with @ xmath28 in the unfolded space , for the same events generated under @ xmath28 as those used in the smeared - space test of x . [ nullgofsmeared ] . ( top right ) for these events , average of the difference between @ xmath90 in the unfolded space and @ xmath58 in the unfolded space . ( bottom ) for these events , histogram of the difference between @ xmath90 in the unfolded space and the gof test statistic @ xmath92 in the unfolded space . , [ = " x : " , scaledwidth = 0 . 0 % ] that tests for compatibility with @ xmath28 in the unfolded space , for the same events generated under @ xmath28 as thoseused in the smeared - space test of fig . [ nullgofsmeared ] . ( top right ) for these events , computation of the difference between @ xmath90 in the smeared space and @ xmath58 in the unfolded space . ( bottom ) for these events , computation of the difference between @ xmath90 in the unfolded space and the gof test median @ xmath92 in the unfolded space . , title = " fig : " , scaledwidth = 49 . 0 % ] that allows for compatibility with @ xmath28 in the unfolded space , for the same results generated by @ xmath28 as those generated in the smeared - space test of fig . [ nullgofsmeared ] . ( top right ) for these events , computation of the difference between @ xmath90 in the unfolded space and @ xmath58 in the smeared space . ( bottom ) for these events , computation of the difference between @ xmath90 in the unfolded space and the gof test median @ xmath92 in the unfolded space . , title = " fig : " , scaledwidth = 49 . 0 % ] , .calculated after unfolding using the linear em method with default ( four ) iteration . , title = " fig : " , scaledwidth = 49 . 0 % ] , here calculated after unfolding using the linear em method with default ( four ) iteration . , title = " fig : " , scaledwidth = 49 . 0 % ] , here calculated after ##folding using the ##erative em method with default ( four ) iteration . , title = " fig : " , scaledwidth = 49 . 0 % ] for the result of the simulated experiment , the gof of @ xmath90 is calculated with respect to the prediction of @ xmath28 and then with respect to the prediction of @ xmath32 . the difference of these two values , @ xmath93 , is then the test statistic for both @ xmath28 vs . @ xmath32 , corresponding to the test of @ xmath63 . fig [ delchi ] shows , for the same purposes as those used in fig . [ lambdah0h1 ] , results of the test of @ xmath93 in the unfolded space for experiments generated under @xmath28 and under @ xmath32 , with @ xmath9 calculated using @ xmath28 and under @ xmath32 . for the two plots studied above , the information on @ xmath9 is not clear . ( unless otherwise stated , consider the plot with @ xmath9 and under @ xmath28 . , calculation of the test statistic @ xmath93 in the unfolded space , for events generated under @ xmath28 ( in blue ) and @ xmath32 ( in red ) , with @ xmath9 calculated using @ xmath28 . ( right ) for the second plot , calculation of the test scenario @ xmath93 in the unfolded space , with @ xmath9 calculated using @ xmath32 . , [ = " fig : " , scaledwidth = 0 . 5 % ] , calculation of the test scenario @ xmath93 in the unfolded space , for events generated under @ xmath28 ( in blue ) and @ xmath32 ( in red ) , with @ xmath9 calculated using @ xmath28 . ( left ) for the samethus , histograms of the same statistic @ xmath93 generate the unfolded space , with @ xmath9 calculated under @ xmath32 . , result = " fig : " , scaledwidth = 0 . 0 % ] figure [ deldel ] shows , for the example of figs . [ lambdah0h1 ] and . [ delchi ] , histograms of the event - to - event difference of @ xmath63 and @ xmath93 . the red curves correspond to events generated under @ xmath28 , while the blue curves correspond for events generated under @ xmath32 . the unfolding method uses l on the left and _ r on the right . this is an example of a _ bottom - up test _ : can one get the same result in the unfolded and unfolded space ? there are problems associated with different statistical methods . since the events generated under both @ xmath28 and @ xmath32 are shifted in the same direction , the statistical implications are not always clear . if we turn to green curves , equivalent curves from neyman - pearson hypothesis testing . and in [ delchi ] ( 2 ) , curves of the same- by - event difference of @ xmath63 and @ xmath93 . in the left panel , ml unfolding is used , while in the right histogram , linear em unfolding is used . , title = " fig : " , scaledwidth = 49 . 0 % ] and in [ delchi ] ( 2007 ) , calculation of the event - by - event difference of @ xmath63 and @ xmath93 . in the left panel , ml unfolding is used , while in the right histogram , iterative em ##folding is used . , title = " fig : " , scaledwidth = 49 . 0 % ] we can measure the effect of the effect shown in fig . [ deldel ] by using the language of neyman - pearson hypothesis testing , in which one rejects @ xmath28 if the value of the test variable ( @ xmath63 in the observed space , not @ xmath93 in the observed space ) is below some critical value @ xcite . the type i conditional probability @ xmath94 is the probability of rejecting @ xmath28 when it is true , not knownas the ` ` true positive rate ' ' . the type ii _ quantity @ xmath95 is the probability of accepting ( not rejecting ) @ xmath28 when it is true . the quantity @ xmath96 is the _ probability _ of the test , also known as the ` ` false positive rate ' ' . the quantities @ xmath94 and @ xmath95 both follow from the cumulative distribution functions ( cdfs ) of histograms of the test data . in classification work outside hep is very common to make the roc curve of true positive rate vs . the false positive rate , as shown in fig . 1 [ alphabeta ] shows the above information in a plot of @ xmath95 vs . @ xmath94 , i . e . , with the vertical axis inverted relative to the roc curve . figure [ alphabetaloglog ] shows the same plot as fig . [ alphabeta ] , with the axes on logarithmic axes . the result of this ` ` bottom up plot ' ' does not appear to be true in this first figure , and appear to be caused by the difference between the poisson - distribution @ xmath63 and @ xmath9##3 already appears in the ml em value , other than in the additional confusion caused by truncating the two values . but no general conclusion can be drawn from this example , since as mentioned above the ml em solution here differs from the em value as the first example . it is of course necessary to use other initial values . and [ delchi ] ( right ) , roc curves for classification performed in the smeared space ( blue curve ) and in the unsmeared space ( red curve ) . ( left ) classification by ml , and ( right ) classification by ml em . , title = " fig : " , scaledwidth = 49 . 0 % ] and [ delchi ] ( right ) , roc curves for classification performed in the smeared space ( blue curve ) and in the unsmeared space ( red curve ) . ( left ) unfolding by ml , and ( right ) unfolding by ml em . , title = " fig : " , scaledwidth = 49 . 0 % ] and [ delchi ] ( left ) , plots of @ xmath95 vs . @ xmath94 , for classification performed in the smeared space ( blue curve ) and in the unsmeared space( red curve ) . ( left ) unfolding by ml , and ( right ) folding by linear em . , title = " fig : " , scaledwidth = 49 . 0 % ] and [ delchi ] ( left ) , instead of @ xmath95 vs . @ xmath94 , for calculations performed in the closed space ( blue curve ) and in the unsmeared space ( red curve ) . ( right ) ##folding by ml , and ( right ) folding by linear em . , title = " fig : " , scaledwidth = 49 . 0 % ] vs . @ xmath94 as in fig . [ alphabeta ] , here with linear scale on both axes . , title = " fig : " , scaledwidth = 49 . 0 % ] vs . @ xmath94 as in fig . [ alphabeta ] , here with linear scale on both axes . , title = " fig : " , scaledwidth = 49 . 0 % ] with the above figures as a table , we can ask how many of the above plots are if we include the values in figure [ baseline ] . figure [ sigmaparam ]shows , as a function of the probability distribution of @ xmath42 , the results of the gof tests shown for @ xmath97 in 1d histograms in figs . [ nulgofunfoldedinvert ] ( top right ) and [ nulgofunfoldedinvert ] ( bottom ) . the results are generated under @ xmath28 . used in scattering ( horizontal axis ) . the vertical axes are the same as those in the 1d images in figs . [ nulgofunfoldedinvert ] ( top left ) and [ nulgofunfoldedinvert ] ( bottom ) , and @ xmath90 in the smeared space ; and the results with respect to @ xmath57 in the smeared space ; for gof tests with respect to @ xmath28 using events generated under @ xmath28 . , x = " fig : " , scaledwidth = 49 . 0 % ] used in scattering ( vertical axis ) . the horizontal axes are the same as those in the 1d images in fig . [ nulgofunfoldedinvert ] ( toptop ) and [ nulgofunfoldedinvert ] ( bottom ) , with @ xmath90 in the unfolded space ; and the difference with respect to @ xmath57 in the unfolded space ; for gof difference with respect to @ xmath28 using events generated by @ xmath28 . , title = " fig : " , scaledwidth = 49 . 0 % ] . [ deldelsigma ] : the difference of the 1d histogram in fig [ deldel ] with the one @ xmath42 used in fig , for both ml and em testing . used in peeling ( horizontal axis ) of the 1d figure in fig [ deldel ] of the event - by - event difference of @ xmath63 and @ xmath93 . ( right ) the same used for [ em unfolding . , title = " fig : " , scaledwidth = 49 . 0 % ] used in smearing ( vertical axis ) of the 1d figure in fig [ deldel ] of the event - by - event difference of @ xmath63 and @ xmath93 . ( left ) the same, for the analysis ##folding . , title = " fig : " , scaledwidth = 49 . 0 % ] , [ deldelb ] and [ deldelbiter ] . , for ml and c ##ulation . , the result of the bottom - line analysis of folding . [ deldel ] as a function of the amplitude @ xmath35 of the extra term in @ xmath98 in eqn . [ altp ] . as a function of the amplitude @ xmath35 of the extra term in @ xmath98 in eqn . [ altp ] , for ( left ) @ xmath9 derived from @ xmath28 and ( right ) @ xmath9 derived from @ xmath32 ; for ml folding . , title = " fig : " , scaledwidth = 49 . 0 % ] as a function of the amplitude @ xmath35 of the extra term in @ xmath98 in eqn . [ altp ] , for ( left ) @ xmath9 derived from @ xmath28 and ( right ) @ xmath9 derived from @ xmath32 ; for ml folding . , [= " fig : " , scaledwidth = 49 . 0 % ] , for iterative em folding . , title = " fig : " , scaledwidth = 49 . 0 % ] , for linear em unfolding . , title = " fig : " , scaledwidth = 49 . 0 % ] figure [ deldelnummeas ] shows , for folding and em unfolding , the result of the bottom - line test of folding . [ deldel ] as a function of the mean number of events in the graph of @ xmath26 . as a function of the number of events on the graph of @ xmath26 , for ( left ) ml folding and ( right ) linear em unfolding . , title = " fig : " , scaledwidth = 49 . 0 % ] as a function of the number of events in the graph of @ xmath26 , for ( left ) ml folding and ( right ) iterative em folding . , title = " fig : " , scaledwidth = 49 . 0 % ] figure [ deldelreg ] shows , for linear em folding , the result of the bottom - line test offig . [ deldel ] as a function of the number of iterations . as a function of number of iterations in ( left ) linear horizontal scale and ( right ) linear vertical scale . , title = " fig : " , scaledwidth = 49 . 0 % ] as a function of number of iterations in ( left ) linear vertical scale and ( right ) linear vertical scale . , title = " fig : " , scaledwidth = 49 . 0 % ] this note illustrates in detail some of the problems that can arise with respect to the unfolded space when testing solutions in the unfolded space . since the note focuses on a very simple hypotheses problem , and looks only at the em and em methods , no general conclusions can be drawn , apart from claiming the general usefulness of the ` ` bottom line tests ' ' . even with the limitations of the roounfold method used here ( in particular that the initial estimate for em is the theoretical minimum ) , we have plenty of examples of testing hypotheses after unfolding . perhaps the most important thing to note so far is that testing by no means ( and with no regularization ) is , in the implementation studied here ,a new @ xmath93 test statistic that is identical to @ xmath60 in the smeared data , which is intrinsically inferior to @ xmath63 . the other very important source of error due to regularization under the bottom line test needs to be addressed . these issues should be kept in mind , even in informal comparisons of unfolded data to predictions from theory . for informal comparisons ( including the presumed use of unfolded data to evaluate predictions in the predictions from theory ) , we believe that some caution should be exercised , including performing the bottom - line - tests with some departures from expectations . this applies to both gof comparisons of a single hypothesis , and comparisons of multiple predictions . more research is needed in order to gain insight regarding what sort of experimental problems and unfolding methods produce results that have poor accuracy under the bottom - line - tests , and which cases lead to experimental failures . as often suggested , comparing the density of @ xmath9 ##3 with the smeared data can facilitate comparisons with the predictions of the folded space , in spite of the dependence of @ xmath9 on the true pdfs . we are grateful to pengcheng zhang , yan ru ##i , wang zhang , and renyuan zhang for assistance in theearly stages of this study . rc ##p the cms technical committee and gnter zech for further discussions regarding the on - line project . this project was partially funded by the u . s . department of energy . award : de sc0009937 . louis lyons , ` ` validity : introduction , ' ' in proceedings of the phystat 2011 conference on the issues related to validity claims in physics teaching and unfolding , edited by h . b . prosper and l . lyons , ( cern , zurich , switzerland , 18 - 20 may 2011 ) + https : / / www . cern . ch / record / 1306523 ( see section of discussion below . ) , et al . ( particle physics society ) , s . p * 38 * 090001 ( 2014 ) and 2015 update . the likelihood - ratio gof test with standard model is eqn . the @ xmath59 test for gaussian distribution with mean is eqn . pearson test @ xmath59 is eqn . 38 . 48 . mikael kuusela , ` ` introduction to validity in high energy physics , ' ' lecture at the scientific computing institute , eth zurich ( may 15 , 2014 ) + https :/ / mkuusela . cds . cern . ch / mkuusela / eth _ 11 _ july _ 2014 / 2 . pdf m . adye , ` ` ' search and experiments with roounfold , ' ' , in proceedings of the phystat 2011 conference on technical issues related to discovery , web search engines and experiments , edited by j . b . muller and m . muller , ( cern , geneva , switzerland , 17 - 19 may 2011 ) http : / / cds . cern . ch / 2011 / 1306523 , p . 313 . we have version 1 . 1 . 1 from + http : / / hepunx . rl . co . uk / ~ adye / search / unfold / roounfold . pdf , available here .