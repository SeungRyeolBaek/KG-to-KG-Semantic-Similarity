set theory was proposed with the intended use to the fields of pattern recognition and image processing [ 1 ] . indeed , it has attracted many applications , and their applications to real - life problems are of a great significance . simpson [ 2 ] presented the fuzzy - based neural network ( fmm ) , which makes the soft decision to organize hyperboxes by the degree of belongingness to a certain class , which is known as the membership function . hyperbox is a convex set , usually represented by min and max functions . fmm classification results are completely obtained with the help of the membership function . along with this original idea , [ 2 ] also presented the need for a good classifier , among which , the ordering , overlapping , and similarity parameters have proved to be of a great interest to a scientific community . simpson also presented a similar problem using fmm , [ 3 ] . but many problems in real - life involve both classification and classification . to address this problem , gfmm [ 4 ] brought this generality . for example , the more significant modification was proved to be related to the membership function . the new membership function computes the belongingness to the hyperbox so that the membership function decreases rapidly as we move away from the hyperbox . anotherfocus of fmm was the patterns belonging to this region , where the risk of misclassification was very high . the tuning parameter , theta ( @ xmath0 ) , which represents the size of a hyperbox , has a great impact on this overlapped region . smaller theta values produce more accuracy for high the accuracy , but the efficacy of the network gets reduced , and for larger theta values , accuracy is decreased . several approaches are used to tackle this problem . earlier , the method of learning [ 3 ] [ 4 ] was employed , which used to eliminate all the overlapping regions . this method solved the intrinsic problem of representing patterns not belonging to any of the hyperbox , in turn lessening the accuracy . exclusion / inclusion pattern classification ( hefc ) - was introduced in [ 5 ] , which further reduced the number of hyperboxes and increased the accuracy . inclusion hyperboxes were used to denote patterns belonging to the overlap region , and exclusion hyperboxes were used to denote the overlapped region , just as if it is a hyperbox . this notion is used as it is in almost all the newly introduced models [ 6 ] [ 7 ] [ 8 ] [ 9 ] . fuzzy min - a fuzzy fuzzy classifier with compensatory properties (fmcn ) was acquainted with [ 7 ] . authors categorized the overlap into four categories , overlap , full containment , half containment and full overlap , and then a new membership function to accommodate belongingness based on the compensation value . authors also suggest that not taking care of the region automatically brings the insensitivity to the hyperbox extinction factor , @ xmath0 . the core based distributed min - random neural network ( dcfmn ) [ 8 ] further builds upon fmcn . authors solved the problem of overlap ##s . they also suggest a new membership function based on size , data density and the cores of the hyperbox . wherein dcfmn improves the accuracy in all cases , there are some serious drawbacks . * * dcfmn introduces two new user controlled variables , @ xmath1 and @ xmath2 . @ xmath1 is used to suppress the influence of the noise and @ xmath2 is used to control the computation speed of the membership function . these two variables greatly affect the performance of the model and therefore , defining their values is a difficult job . * there is an implicit assumption that behavior in all the hyperboxes is identical , which may not be true . however , the sequence ofthe training error plays a role as well . * mlf shows that this membership function is not universally preferred , and that , it does not work well for the percentage of samples belonging to overlapped region . multi - level fuzzy - based learning network ( mlf ) [ 9 ] addresses the problem of overlapped region with an elegant approach . it creates separate levels for overlapping regions , and monotonically decreases the hyperbox size ( @ xmath0 ) . for most cases , mlf produces 100 % training accuracy . although mlf achieves a significant improvement , entertaining the results is rather more important than training accuracy , and it greatly sways the usage of the algorithm in practical applications . in this case , we identify and define a new training class , where misclassification rate is substantial . to the best of our knowledge , this kind of algorithm was used for the first time , at which we did not come across any similar training algorithms . hence we propose a method , based on random sampling , to evidentially show that handling this newly introduced problem of competition between hyperboxes of different classes significantly improves the testing accuracy . the paper is summarized as follows . mlf is reviewed in section 1 . we describe non - mlf algorithms in section2 . an illustrative example and comparative analysis of m - mlf with mlf model are given in sections iv and v , respectively . finally , it is given in section vi . multi - level - min - neural network ( mlf ) is a classifier which efficiently caters misclassification of nodes belonging to overlapped networks by maintaining the tree structure , which is a homogeneous tree [ 1 ] . in mlf - model , weights are continuously updated to all the hyperboxes and layers , and recursion results in one level . this update process is repeated until the predefined maximum , and till overlap ##s . hyperbox expansion , based on hyperbox size and parameter ( @ xmath0 ) , is performed using equation ( 1 ) and multiplication is carried out by equation ( 2 ) . @ xmath3 @ xmath4 where , @ xmath5 and @ xmath6 are min point and max point of hyperbox _ c _ respectively , @ xmath7 is the @ xmath8 dimension of node _ c _ and _ d _ and the number of layers . also , according to each recursion , @ xmath0 is .in equation ( 3 ) @ xmath9 where , @ xmath10 and @ xmath11 stand for current level and previous level , respectively and @ xmath12 , with the value between 0 and 1 , ensures that size of hyperbox in the region is less than its previous size . in the testing phase , overlap ##ped are then traversed recursively , to discover appropriate subnet to which a given pattern belongs to . finally , in that phase , the class of hyperbox with highest membership , with the hyperboxes in the discovered subnet , is selected as the candidate class . mlf is able to achieve higher performance , than previous fmm algorithms . this is due to an elegant solution to the boundary of a confusion area . but , after testing , there is a search for yet another class . the region where it only generates very close data sets , it is difficult to assign this class with high degree of accuracy . as per our results , mlf , and all the previous classifiers , did not perform well in this area . hence , a solution of this confusion region , and a method to solve it are proposed . in this section , we give information about a newly proposed algorithm , namely , we define a new boundary that generateddue to trained learning and as a method to correctly classify the patterns belonging to it . _ figure 1 _ in the d - mlf structure , each node of s @ xmath13net has two segments , hyperboxes segment ( bs ) and overlapped segment ( ols ) . hbs represents hyperboxes generated at that level , whereas ols represents nodes at that level . along with hyperbox information , the classification ( s ) . _ figure 2 _ describes the area of problem considered by mlf and d - mlf . we introduce a boundary region that exists between the two hyperboxes , where , according to our model , the risk of misclassification is comparatively low . in the dc method , the nodes of mlf are intact , in addition to this , we conduct experiments with the data points to solve a classification problem in the anew boundary region . according to the mlf learning procedure , d - mlf s @ xmath14 ##net hbs and ols algorithm . first , all the patterns are filtered through , resulting in identification and detection of hyperboxes using equations ( 1 ) and ( 2 ) . then each hyperbox is checked with the creation of hyperboxes to identify the pattern using equations (( ) . @ xmath15 where @ xmath16 and @ xmath17 are the min points and @ xmath18 and @ xmath19 are the min points of the two hyperboxes , among which overlap is found . finally , d - mlf adds a further step to the learning phase , known as data correlation ( dc ) computation , where dc of all input patterns belonging to each hyperbox is maintained in the database . dc is computed as follows : @ xmath20 where @ xmath21 is the data centroid of the @ xmath22 hyperbox , @ xmath23 is number of patterns belonging to @ xmath22 hyperbox and @ xmath24 is the @ xmath8 pattern in @ xmath22 hyperbox . if there exists an overlap , patterns belonging to the overlapped pattern are then sent to learning phase , where cas and ols computation takes place for the next level . this process of recursion is repeated again to find all the overlap . due to lack of ols and difficulty of finding patterns belonging to ols , d - mlf and mlf are not multiple access algorithms . in fact , given the n overlaps ,the first level , the same region has to be visited multiple times . then , in the second level , data belonging to each region is sorted in order of magnitude of number of overlaps in that region . this is a novel finding , and contradictory to what mlf authors have stated [ 1 ] . note that , the patterns belonging to this region are not part of the dc ##s . this step makes sure that training patterns belonging for more than one class are omitted from the final decision making . + train = d - mlf - train ( s , @ xmath0 ) + @ xmath25 @ xmath26 = h . centroid / h . membercount return : h . centroid + = s ; h . membercount + = 1 ; create a hyperbox h ; h . sample = s ; h . membercount = sample ; sdata = samples which inhabit in another class ; h . centroid - = s ; h . membercount - = 1 ; create an overlap - box as @ xmath27 and link to ols @ xmath28 = d - mlf - train ( sdata , @ xmath29 ) ; link @ xmath27 to @ x##math28 with link @ xmath30 ; @ xmath31 the original mlf is a decision tree based on the subnets chosen . the selected subnet will not be a new node in the network . we do not change this decision , but alter the process of how subnet marks the choice . a function described in the equation ( 5 ) is used against the boxes . by recursively applying the ols the appropriate subnet is selected , to which the box belongs to . the membership function explained in the equation ( 6 ) is used , this time , to compare the boxes with the hyperboxes of the selected subnet . @ xmath32 \ \ [ ( - ( ( v _ i ^ i - v _ i ^ j , \ gamma _ j ) ] ) ) \ \ f ( x , \ gamma ) = \ \ { 0 } { 0 } \ ; \ ; \ ; \ ; if \ ; x \ ; \ ; > \ ; > \ \ { 0 } \ ; \ ; \ ; \ ; if \ ; 1 \ ; \ leq \ ; 1 \ ; \ ; \ leq \ ; > \ \ { 0 } \ ; \ ; \ ; \ ; if \ ; \\ ; \ ; < \ ; 0 \ begin { cases } \ end { cases } \ ] ] where @ xmath33 represents belongingness of sample @ xmath34 with @ xmath35 hyperbox . @ xmath36 is the distance between min and the point with sample @ xmath34 and @ xmath37 is a boundary parameter to check fuzziness . using these membership values , hyperboxes with these membership values are selected to form a pattern . medial area of these hyperboxes , controlled by @ xmath38 , is treated as a boundary region . @ xmath38 is a well defined variable , mentioned in the membership value . at this point , it is necessary to determine if the pattern belongs to the boundary region . we use @ xmath391 and @ xmath392 as incident points between test pattern and the hyperboxes , respectively . inclusion value is computed as follows : @ xmath40 further , based on the inclusion value , a pattern is created . if it is in the area outside of the defined boundary , we can create a pattern of mlf , and classify the pattern based on the associated membership value , which is already computed . if the pattern belongs to the boundarytherefore , the distance [ 10 ] between test sample and the total set of the selected hyperboxes is zero . thus , depending on the inclusion value , the output of the algorithm is denoted as either the sum of distances @ xmath41 among all the hyperboxes , or as the minimum of the distances of the topmost selected hyperboxes @ xmath42 where @ xmath43 is given by ; @ xmath44 where @ xmath45 is the @ xmath8 is output for the test sample in @ xmath46 subnet , @ xmath47 the output of subnet @ xmath46 and the corresponding overlap box that enables the subnet if test sample is in this overlap box . and @ xmath48 is the output of ols , which is given by equation ( 10 ) @ xmath49 where @ xmath50 is number of overlap boxes in ols and @ xmath51 is a maximum of the @ xmath35 overlap box for test sample @ xmath34 , given by equation ( 11 ) @ xmath52 and @ xmath53 , given by equation ( 12 ) @ xmath54 where @ xmath55calculate the average data distance amongst samples @ xmath46 and the square root of the sample @ xmath8 hyperbox using equation ( 1 ) @ xmath56 out = d - mlf - test ( net , sample ) + @ xmath25 out = d - mlf - test ( @ xmath28 , sample ) ; = 0 ; mv = [ ] ; mv + = max ( sample , @ xmath57 ) ; = [ max ( sample ) , max ( ) ( ) @ xmath58 max ( mv ) ) ) ] mv = eudistance ( sample , h1 . dc , h2 . dc ) ; d = min ( d ) . class ; out = min ( d ) . class ; @ xmath31 in this figure , we describe the effectiveness of the proposed model , clearly pointing out the identification and resolution of the fundamental problem of learning . _ _ _ _ illustrates the 2 - diamentional data model . we have 14 data samples for training and 12 data samples for testing . hyperbox boundary parameter ( @ xmath0 ) is fixed at 0 . 3 and the boundary parameter ( @ xmath38 ) is fixed at 100 % . the mlf andd - mlf creates two hyperboxes at @ xmath59 ##0 . d - mlf also creates two centroids ( clusters ) for each hyperbox , @ xmath60 and @ xmath61 . therefore , the centroids of @ xmath60 and @ xmath61 become @ xmath62 and @ xmath63 , respectively . patterns which do not belong to boundary region are classified correctly by mlf . but when it comes to boundary region , it fails to correctly classify the patterns . whereas the proposed d - mlf works better in the boundary region as well , because its decision making is not only based on the membership value , but it also considers data structure . it can be seen that the patterns in the above example are not uniformly spread out . which is a very difficult problem in real - world data . this is because of the dominance of the variables such as outliers , temporal nature of the data , etc . due to them , most of the times , the patterns within the overall data , and in case of the # ##kowski hierarchy , the hyperboxes , will not be uniformly distributed across all the layers . as mentioned above , our proposed model treats them elegantly , withoutsome of the modifications to the state of the art . performance of the method ( d - mlf ) is measured on the basis of the classification rate . several experiments were carried out to test d - mlf using different standard datasets . several datasets such as iris , glass , iris , wisconsin prostate cancer ( wbc ) , wisconsin breast breast cancer ( wdbc ) and ionosphere were used . these datasets were obtained from the uci repository of machine learning databases [ 9 ] . for these experiments , hyperbox size data ( @ xmath0 ) was chosen as 0 . 2 , 0 . 3 and 0 . 9 . this is to test the classification of the data . as we increase the size of the hyperbox , the number of overlaps increases , and so does the misclassification rate . we compare the data sets for training and testing . the average results are compared for both experiments . for each experiment , training and testing data are selected randomly . _ table _ _ _ here , we compare our method to mlf method , and it has been successfully proven to perform better than the previously proposed fmm method [ 9 ] . . results [ cols = " ^ , ^ , ^ , ^ " , header = " ^ ", ] in this paper , we propose a data boundary region and region based mlf modeling method to model data belonging to that boundary region . the data centroid based approach , d - mlf , reduces number of errors and other errors in decision making . it has been evidentially shown that the proposal incorporates all the previously proposed fmm methods . more importantly , we have proposed a model language for applications in the real world , extending the state of the art . d - mlf will work in many areas such as security , natural language processing , automated reasoning , etc . l . a . zadeh , fuzzy sets , information and control , vol . 4 , pp . 338 - 353 , 1965 . p . k . simpson , fuzzy min - max neural networks . classification , ieee trans . neural networks , vol . 10 , pp . 776786 , sep . 1992 . simpson , p . k . , fuzzy min - max neural networks - part 1 : clustering , ieee trans . systems research , 3245 1993 . b . gabrys and a . bargiela , on fuzzy min - max neural networks for clustering and classification , ieee trans . neural networks , vol . 11 , pp . 76978##3 , 2000 . bargiela , m . pedrycz , and y . tanaka , an inclusion / exclusion - hyperbox design , int . based intell . , vol . 8 , no . 3 , pp . 9198 , 2004 . m . rizzi , m . panella , and m . a . m . mascioli , high performance min - max classifiers , ieee trans . neural netw . 2 , pp . 402414 , dec . 2006 . s . nandedkar and s . k . biswas , a fuzzy min - max neural network design with compensatory neuron models , ieee trans . neural netw . 2 , pp . 4254 , dec . 2007 . y . zhang , y . liu , y . li , and y . wang , multi - multi - level fuzzy min - neural networks for object recognition , ieee trans . neural netw . 3 , pp . 23392352 , dec . 2007 . davtalab , m . a . dezfoulian and m . mansourizade , multi - level fuzzy min - max neural network design , ieee trans . neural netw . 3 , pp . 470 - 481 , dec . 2007 . bezd##el and d . j . smith , results of an analysis and learning of words by humans using line - based data , proc . 2060 - 2066 , feb . 2013 . bache and a . lichman . , uci online learning repository , uci inf . center , irvine , california , usa . , 2013 . [ not available ] http : / / www . ics . uci . edu / .