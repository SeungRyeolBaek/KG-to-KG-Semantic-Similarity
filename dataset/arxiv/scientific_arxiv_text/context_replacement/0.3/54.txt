in adaptive control and unknown parameter estimation one often needs to adjust recursively to estimate @ xmath0 of a vector @ xmath1 , which contains @ xmath2 , but unknown data , or measurements of a vector @ xmath3 where @ xmath4 is a vector of unknown data , sometimes called the regressor , and @ xmath5 is a measurement error signal . the goal of optimization is to keep both the measurement error @ xmath6 and the parameter error @ xmath7 as small as possible . there are many different methods for dealing with the problem above , for example least - squares . perhaps the most straightforward is minimizing the estimation error via gradient - type algorithms of the form : @ xmath8 where @ xmath9 is a real , positive , positive - definite definite matrix . let us take @ xmath10 and analyze the equations and , which under the assumption that @ xmath11 is not zero read : @ xmath12 the positive derivative @ xmath13 the zero derivative @ xmath14 and @ xmath15 inspection of the equation above shows that @ xmath16 is limited in time , and @xmath17 , and note that the error @ xmath18 ( norms are based on the interval @ xmath19 where all parameters are defined ) . these are the main properties an algorithm needs in order to be considered a suitable algorithm for the use of a tuner in an adaptive tuning system . often @ xmath20 or something similar is considered a desirable approximation . to achieve the latter , normalized tuning can be used ; however , the relative merits of normalized versus unnormalized tuners are still somewhat unclear . another alternative is to use a time - varying @ xmath9 , which is done in least - squares regression . in [ sec : variance ] we present a tuner that measures the first derivative of @ xmath0 , and in [ sec : variance ] the effects of the white noise @ xmath5 on the performance of the two algorithms are discussed . next we present some examples and make concluding remarks . the tuners are such that the _ acceleration _ of adaptation ( the first derivative of the parameters ) is set equal to the regressor and to the prediction error @ xmath21 . we propose to set the _ acceleration _ of the parameters : @ xmath22 notice that thethe formula above is implementable ( using @ xmath23 integrators ) if measurement error is present , because the error @ xmath24 appears only in the calculation with @ xmath25 . as another example of lyapunovian inspiration : @ xmath26 taking all of the trajectories of gives @ xmath27 and @ xmath28 we obtain @ xmath29 which leads us to the desired properties : @ xmath30 the slow variation of @ xmath31 follows without the need for correction , and thus we obtain @ xmath32 instead of @ xmath33 as expected . we also obtain @ xmath34 as a modified error , which can be used in the error analysis of a detectable or ` ` tunable ' ' optical system via the error - correction method ; see @ xcite . a variant of is @ xmath35 with @ xmath36 and @ xmath37 orthogonal , symmetric , positive - definite @ xmath38 , such that @ xmath39 and @ xmath40 . the property of stability , which can be obtained from the positive - definite error @ xmath41 in thesame manner as before , for @ xmath42 we will consider the effects on the squared variance and covariance of @ xmath43 of the presence of a measurement error . the results are that @ xmath11 is a random noise with noise , and that @ xmath44 and that @ xmath45 are random , deterministic data . for comparison purposes , we consider what happens when the fm ##f is applied to in the presence of measurement error @ xmath5 : @ xmath46 the solution to the equation above can be written in terms of @ xmath47 and the transition matrix @ xmath48 as : @ xmath49 hence @ xmath50 because @ xmath51 has noise . here the notation @ xmath52 , denoting the derivative with respect to the random variable @ xmath5 , is used to indicate that the statistical properties of @ xmath25 are also under consideration . the conclusion is that @ xmath43 will converge to zero in time as soon as @ xmath53 does . the well - defined set of boundary conditions of @ xmath54 are sufficient for the latter to converge . toconsider the first term of the parameter . , since @ xmath55 the covariance of @ xmath43 can be written as the sum of three terms . the first is zero . the second term @ xmath56 because @ xmath11 has zero mean , and the third term is not zero . the result is @ xmath57 where fubini convergence theorem and the gain @ xmath58 were proved . repeating the process and studying the first and third terms results in @ xmath59 this equation can be given the following form : for example @ xmath60 , when @ xmath53 is close to the identity , the gain of @ xmath43 is close to @ xmath61 , the dot product of the factors in the initial guess of the parameters with itself . for @ xmath62 , which can happen if @ xmath54 is more accurate , @ xmath63 close to @ xmath64 . this leads to a compromise between higher computational speeds and lower steady - state parameter error , which require both larger and smaller values of the gain @ xmath9 . , that allows for the best of computing the parameter error .the least - squares sense may utilize time - varying , variable values ; an example of the least - squares sense . we shall now do a similar analysis for the acceleration function from to , which results in the differential equation @ xmath65 , @ xmath66 where @ xmath67 , @ xmath68 , and @ xmath69 is a function of @ xmath70 unless otherwise noted , and the remainder is convergence with respect to the second term . if @ xmath71 , @ xmath72 follow the same formula used for the acceleration function , one gets that @ xmath73 and that @ xmath74 however the properties of the acceleration and acceleration functions are not yet considered comparable because the right - hand side of does not lend itself to immediate convergence . to obtain comparable results , we use the simple but easily understood formula , @ xmath75 ' ' ' ' ' valid for the scalars @ xmath76 and @ xmath77 , and use the [ [ simplifying - assumption ] ] - formula : + + + + + + + + + + + + + + + ++ + + + + + + + for @ xmath78 , and finally , @ xmath79 , where @ xmath80 are integers and @ xmath81 is the @ xmath82 density matrix . premultiplying [ @ xmath83 $ ] , postmultiplying [ @ xmath83 ^ \ top $ ] , moving from [ to @ xmath60 , and applying the above algorithm and setting . @ xmath84 ' ' ' ' ' , @ xmath85 ' , @ xmath86 is positive - semidefinite , and @ xmath87 the combination of and means that @ xmath88 can be increased without affecting @ xmath24 ##90 steady - state performance . on the other hand , to correct the error we need to increase @ xmath89 , which strictly speaking means a decrease in . since @ xmath88 and @ xmath89 can be increased without affecting the performance properties shown above [ see : below ] , a better than @ xmath90 steady - state performance improvement may be achievablewith the velocity greater than with the right tuner , at least in the case when @ xmath91 , @ xmath92 , and @ xmath37 are ` ` equal . ' ' notice that @ xmath93 by construction . [ [ approximate - analysis ] ] approximate analysis : + + + + + + + + + + + + + + + + + + + + + + the derivation of inequality does not require any approximations , and it gives an upper bound for @ xmath94 , valid instead of @ xmath54 . a less accurate derivation of the inequality in can be obtained by replacing @ xmath95 by the new value @ xmath96 in the interval of @ xmath86 . . this method is useful because @ xmath86 is in the interval , which calls for more rigorous theoretical analysis . to obtain a new inequality , we use @ xmath97 ; or , using the schur approximation @ xmath98 or , using the above assumption and replacing @ xmath95 by the value @ xmath96 @ xmath99 suppose ##s that @ xmath100 . .for the least squares estimate , we use @ xmath101 , the least value of @ xmath76 that exceeds @ xmath97 . replace @ xmath102 with @ xmath103 \ bar { m } _ 1 \ left [ \ begin { smallmatrix } { \ phi } ^ \ top _ { 12 } ( t , 0 ) \ \ { \ phi } ^ \ top _ { 12 } ( t , 0 ) \ begin { smallmatrix } \ right ] } { 4m _ 1 ^ { ##m _ 2m _ 3r ( t + \ top _ 12 ) - 1 } . $ ] for @ xmath104 we use the previous , simpler formula . for the larger values of @ xmath77 the first term of the right - hand side of reduces to @ xmath105 , which means that the steady - state value of the phase factor decreases when the signal @ xmath25 increases in amplitude , and that it can be made smaller via the choices of the parameters @ xmath88 and @ xmath106 . the result for the fm amplifier is not much more accurate than for the conventional amplifier . theexamples in this article compare the behavior of the accelerating signal with those of the gradient one and of a normalized gradient one . the experiments were performed in open - loop , with the regressor a two - dimensional signal , and without measurement noise . figure [ fig : step ] shows the values of @ xmath107 and @ xmath108 , when @ xmath25 is a two - dimensional step signal . in figure [ fig : sin ] the regressor is a vector , in figure [ fig : sia ] an infinitely increasing sinusoid , and in figure [ fig : prb ] a pseudorandom graph , using matlab . an effort was made to improve the choice of input signals ( @ xmath91 , @ xmath92 , and @ xmath37 were all chosen according to the experiment ) , and the effect of measurement noise was not considered . the performance of the accelerating tuner is comparable , and therefore better , to that of the other tuners . = 2 . 5 in = 2 . 5 in = 2 . 5 in = 2 . 5 in = 2 . 5 in = 2 . 5 in = 2 . 5 in = 2 . 5in other ideas related to the above one are replacing the function in with a positive - definite transfer function @ xcite , and using high - order tuning ( @ xcite ) . high - order tuning requires the outputs @ xmath0 as well as its derivatives up to that given order ( in this sense we can call the accelerating algorithm a second - order algorithm ) , and also the accelerating tuner requires derivatives of @ xmath25 up to that given order . we hope that accelerating tuners will find application in the control of nonlinear systems and also in dealing with the nonlinear incompatibility known as the ` ` loss of stabilizability problem ' ' in the adaptive filtering literature . the stochastic ##ity in [ see : covariance ] shows that the performance and convergence properties of the accelerating tuner , together with its moderate computational cost , do indeed make it a desirable solution for adaptive filtering applications . it seems that a better transient @ xmath90 steady - state noise compromise is possible with the accelerating algorithm than with the accelerating algorithm . to verify this conjecture , a study of the properties of the accelerating algorithms and their performance with the persistence of initial conditions , in order , as well as thecontinuous training in the presence of measurement noise .