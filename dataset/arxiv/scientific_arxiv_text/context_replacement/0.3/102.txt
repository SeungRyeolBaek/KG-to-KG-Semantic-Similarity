model selection is an important problem in many areas including machine learning . if a proper model is not selected , any method for the estimation or prediction of the ' s outcome is impossible . given a set of observed models , the goal of model selection is to select the model that best approximates the observed data and captures its expected properties . model selection criteria are defined such that they strike a balance between the _ goodness - of - fit ( gof ) _ , and the _ generalizability _ or _ complexity _ of the models . goodness - of - fit measures how well the model captures the properties of the data . generalizability / complexity is the measure of the dependence of the model on the data and how accurately the model fits / captures the observed data . models with higher complexity than necessary will suffer from overfitting and poor generalization , and models that are too complex will underfit and have poor gof @ xcite . cross - sampling @ xcite , bootstrapping @ xcite , akaike information criterion ( aic ) @ xcite , and robust information criterion ( bic ) @ xcite , are well known examples of robust model selection . in cross - sampling methods such as cross - validation and regression , the model##ization error of the model is estimated using monte carlo simulation . in contrast with random - sampling methods , the error estimation methods of aic and bic do not require validation to estimate the model error , and are highly efficient . in these methods an _ _ criterion _ is defined such that the model error is reduced by penalizing the model for error from observed data . a large number of information criteria have been developed with different motivations that lead to different statistical results . for instance , the small penalization parameter of bic produces better models , whereas aic works better when the dataset has a very small sample size . kernel methods are simple , computationally efficient statistical methods that are capable of operating on low dimensional data with arbitrarily complex structure . they have been successfully applied in a range of fields such as classification , and regression . in kernel methods , the data are transformed from their original space to a higher dimensional feature space , the sparse kernel feature space ( rkhs ) . the idea behind this method is to transform the nonlinear relationship between data points in the kernel space into an easy - to - compute linear learning model in the feature space . for example , in linear regression the response variable is described as a linear function of the observed data .every model that can be represented through inner products has a kernel evaluation . this operation , called kernelization , makes it possible to transform traditional , well known , model selection methods into stronger , corresponding kernel methods . the work in kernel analysis has , historically , mostly focused on kernel selection and on tuning the ridge parameters , but some recent work is done on kernel - based model selection @ xcite . in this study , we investigate a kernel - based information criterion for ridge regression models . in kernel ridge regression ( krr ) , tuning the ridge parameters to find the most appropriate subspace with respect to the data at hand and the observed data is the goal of the kernel model selection criterion . in classical model selection , the validity of the model selection criterion is evaluated both by constructing a consistency proof where the sample size tends to infinity and empirically by empirical testing for finite sample sizes . these methods investigate a probabilistic upper bound of the prediction error @ xcite . evaluating the consistency properties of the model selection criterion _ _ model selection _ is difficult . the evaluation mechanism of the kernel methods does not work well . the reasons for that are : the ability of the model to solve problems such as under / overfitting @ xcite is not known ( for @ xmath##1 ( points of dimension @ xmath2 , the result of @ xmath3 , which is independent of @ xmath2 ) and the results of generalization are . estimators are hard to use in rkhs . researchers have kernelized the traditional model selection methods and demonstrated the advantages of their new model selection methods . kobayashi and komaki @ xcite used the kernel - based regularization information criterion ( kric ) from the eigenvalue analysis to calculate the selection criteria for kernel logistic regression and support vector model ( svm ) . rosipal et al . @ xcite used central information criterion ( cic ) for model selection in the principal component analysis , because of its better results compared to aic and bic for generalized linear regression . demyanov et al . @ xcite , used alternative way of calculating the likelihood function in akaike information criterion ( aic , @ xcite and bayesian information criterion ( bic , @ xcite ) , and used it for model selection in svms using the gaussian kernel . as pointed out by van emden @ xcite , a desirable model is the one with the least dependent variables . .finding a complexity term that increases the number of model parameters enables us to select the most efficient model . in this study , we define a novel step - wise model and obtain the complexity term from the additive distribution of variance based on model parameters . using the complexity term in this way we measures the interdependency of each parameter of the model . we call this optimization method _ kernel - based optimization criterion ( kic ) _ . model selection criterion in general process optimization ( gpr ; @ xcite ) , and kernel - based information criterion ( icomp ; @ xcite ) in kic in using a kernel - based complexity measure . however , the methods differ because these complexity measures measure the interdependency between the data ##set rather than the model parameters . although we can not establish the exact properties of kic theoretically , we can evaluate the efficiency of kic both in synthetic and real datasets using state - of - the - art results compared to check - it - out - cross - validation ( loocv ) , kernel - based icomp , and linear log - likelihood in gpr . the paper is summarized as follows . in section [ sec : krr ] , we give an exampleof kernel ridge regression . kic is described in detail in section [ sec : kic ] . section [ sec : om ] also provides a detailed description of the methods to which kic is applied , and in section [ sec : exp ] we describe the performance of kic through series of tests . in regression analysis , the regression is of the form : @ xmath4 where @ xmath5 can be either a linear or non - linear function . in linear regression we assume , @ xmath6 , where @ xmath7 is an observation vector ( response variable ) of size @ xmath8 , @ xmath9 is a full dimensional data matrix of response variables of size @ xmath10 , and @ xmath11 , is an observation vector of unknown parameters , where @ xmath12 is the transposition . we also assume that the error ( variance ) of @ xmath13 is an @ xmath1 - dimensional vector whose elements are the i . i . d , @ xmath14 , where @ xmath15 is an @ xmath1 - dimensional data matrix and @ xmath16 is the observation vector . the regression coefficients are the observed variables , @ x##math17 , with estimated function @ xmath18 , and estimated function @ xmath5 . when @ xmath19 , the problem is ill - posed , meaning that some form of optimization , such as tikhanov regularization ( ridge regression ) is required , and the results are the following optimization : @ xmath20 where @ xmath21 is the regression parameter . the estimated regression coefficients in ridge regression @ xmath22 are : @ xmath23 k _ r _ based regression ( krr ) , the kernel of @ xmath9 is non - explicitly defined in rkhs using a feature of @ xmath24 . the estimated regression coefficients based on @ xmath25 are : @ xmath26 where @ xmath27 is the kernel matrix . # [ 1 : 1 ] does not have an explicit solution for @ xmath28 because of @ xmath24 ( the kernel matrix allows one to avoid explicitly defining @ xmath25 that could be considered intractable if transformed in rkhs , if possible ) , so a ridge estimator is used ( e . g . @ xcite ) that excludes @ xmath24 : @xmath29 and @ xmath30 in the case of krr is equivalent to using the objective function function of the regression coefficients , where the kernel function is : @ xmath31 and @ xmath32 are the relevant rkhs . for @ xmath33 , and @ xmath34 we have : @ xmath35 where @ xmath36 is the kernel function , and @ xmath37 . the main idea of this paper was to introduce a new kernel - based optimization problem ( kic ) for the model used in kernel - based regression . according to this kic measures both the goodness - of - fit and the complexity of the model . gof is calculated using a log - likelihood - generating function ( we call the log likelihood ) and the complexity measure is a function based on the likelihood function of the kernel of the model . in the following subsections we expand on these results . the method of van emden @ xcite for the complexity measure of a random variable is based on the interactions among the variables in the estimated covariance function . the desirable model is the one with the least random ##ness . this reduces the estimated entropy and gives lower complexity . in this paperwe rely on this one of the complexity measures . for the @ xmath2 - dimensional normal distribution @ xmath38 , the complexity of the covariance matrix , @ xmath39 , is given by the ' joint entropy @ xcite , @ xmath40 where @ xmath41 , @ xmath42 are the mean and the joint entropy , and @ xmath43 is the @ xmath44 - entropy of @ xmath39 . @ xmath45 if and only if the covariates are independent . the complexity measure in equation deals with orthonormal statistics because it is based on the complexity of the orthogonal basis of @ xmath46 @ xcite . to overcome these problems , bozodgan and haughton @ xcite introduced icomp information along with a complexity measure based on the maximal computational complexity , which gives an upper bound on the complexity measure in equation : @ xmath47 this complexity measure is equal to the estimated variance ( @ xmath48 ) and the mean ( @ xmath49 ) of the eigenvalues of the density matrix . less than of @ xmath50 , indicates the complexity ofindependent variables , and vice versa . zhang @ xcite has a kernel version of this complexity measure @ xmath50 , that is based on kernel - based covariance of the ridge function : @ xmath51 the complexity measure in general linear regression ( gpr ; @ xcite ) is defined as @ xmath52 , a concept from the joint paper @ xmath42 ( as shown in the [ eq : 1 ] ) . in contrast to icomp and gpr , the complexity measure in kic is defined using the state - space ( hs ) version of the covariance matrix , @ xmath53 . using this complexity measure obtains a model with only independent variables . in the next section , we explain in detail how to find the independent variable - based parameters in the complexity measure , and the definition of the complexity measure . + in kernel - based model selection methods such as icomp , and gpr , the complexity measure is defined using a random matrix that is of size @ xmath54 for @ xmath9 of size @ xmath10 . the idea of this method is to find the difference between the model parameters , which independent of thedefinition of the model class @ xmath2 . in the other words , the determination of the kernel of the model is important because of the presence of a parameter . to define a model class that depends on @ xmath2 , we define component - wise kernel as an additive combination of coefficients for each component of the model . let @ xmath55 denote the parameter vector of the model . regression : @ xmath56 where @ xmath57 and @ xmath58 , and @ xmath59 the solution of krr is given by @ xmath60 . the quantity @ xmath61 = \ alpha ^ { \ operatorname { tr } [ [ ( 1 + \ alpha } ) ^ { - 2 } ] $ ] can be interpreted as the sum of coefficients for the component - wise parameter vectors , if the additive sum of component - wise coefficients is used : @ xmath62 where @ xmath63 and @ xmath64 are the k - wise parameters of , @ xmath65 and @ xmath66 . with this additive combination , the quantity @ xmath67 can be written as : @ xmath68 where @ xmath6##9 is the parameter of @ xmath70 , the rkhs given by @ xmath71 . the parameter @ xmath28 in this model is given by @ xmath72 where @ xmath73 , and since @ xmath69 in r [ 0 : 1 ] is related to @ xmath74 . let @ xmath75 be the conditional expectation of @ xmath74 , @ xmath69 or @ xmath76 . we have @ xmath77 , \ end { aligned } \ ] ] where @ xmath78 is the gram associated with @ xmath71 . since @ xmath79 , we have @ xmath80 = \ operatorname { tr } [ \ theta _ { \ theta } ] . \ end { aligned } \ ] ] formalizing the parameter class with term - length estimates that measures the contribution of the kernel of the model ( or the significance of the contribution of the parameter ) . . + gretton et al . @ xcite ##s the gram - based independence criterion , namely the gram - based independence criterion ( hsic ) , which issee below . suppose @ xmath81 , and @ xmath82 are unit vectors with linear maps @ xmath83 , and @ xmath84 , where @ xmath85 , and @ xmath86 are rkhss . the cross - correlation operator associated to the joint joint distribution @ xmath87 is a linear map , @ xmath88 such that : @ xmath89 , \ end { aligned } \ ] ] where @ xmath90 is the inner product , @ xmath91 = e [ k ( \ cdot , x ) ] $ ] , and @ xmath92 = e [ k ( \ cdot , x ) ] $ ] , for @ xmath93 , and linear map , @ xmath36 . the hsic measure for the rkhs @ xmath85 , and @ xmath86 is the squared hs - measure of the cross - correlation matrix and is given by : @ xmath94 \ end { aligned } \ ] ] * = 0 . * suppose @ xmath95 , and @ xmath96 are vectors , forfor @ xmath97 , and @ xmath98 , @ xmath99 , and @ xmath100 , @ xmath101 if and only if @ xmath102 , and @ xmath7 are independent ( figure 1 , @ xcite ) . + by considering the hsic and covariance matrix associated with each of the @ xmath103 we can determine the independence of the parameters . since @ xmath104 is a symmetric , positive - definite matrix , @ xmath105 , and the inverse of the # ##ic of the symmetric matrix is equal to : @ xmath106 = \ sum _ { i = 1 } ^ { v _ i ^ 2 \ nonumber \ \ ~ ~ & = \ alpha ^ { \ operatorname { tr } [ [ ( k + \ alpha i ) ^ { - 1 } k ( k + \ alpha i ) ^ { - 1 } ] \ begin { tr } \ ] ] kic is defined as : @ xmath107 where @ xmath108 is the equivalence class based on : [ 1 : 1 ] . the inverse of @ xmath109 ##8##s a complexity measure that is sensitive to changes in data ( similar to icomp ##s ) . the parameter kic is the best estimate . ] . the maximum log - width ( pll ) of krr for normally distributed data is given by : @ xmath110 the unknown ##s @ xmath22 , and @ xmath111 are calculated by minimizing the kic hash function . @ xmath112 we have considered the possibility of using @ xmath113 $ ] , and @ xmath61 $ ] as complexity measures . the best results are in the [ subsec : realdata ] on the datasets , and also with kic . we define these complexity measures as : @ xmath114 , \ end { aligned } \ ] ] @ xmath115 . \ end { aligned } \ ] ] in both kic _ 1 , and kic _ 2 , similarly to kic , @ xmath116 , while because the complexity measure is based on @ xmath16 , @ xmath111 for kic _ 1 , : @ xmath117 = 0 . \ end { aligned } \ ] ] if wefor @ xmath118 , @ xmath111 is the solution of the linear optimization problem , @ xmath119 , where @ xmath120 . in the case of kic _ 1 , the @ xmath111 is the square root of the following characteristic polynomial : @ xmath121 where @ xmath122 $ ] . we compare kic with loocv @ xcite , model - based icomp @ xcite , and an estimate of marginal likelihood using gpr ( written as gpr ) @ xcite to find the regression ridge regressors . the reason to compare kic with icomp and gpr is that in all of these cases the likelihood function measures the interdependency of the parameters as a function of covariance matrix in different ways . loocv is a popular and widely used methods for model selection . * loocv : * random - ##ized model selection methods for cross - validation are time efficient @ xcite . for example , the pick - one - out - cross - validation ( loocv ) : the computational time of @ xmath123 the number of possible combinations ( @ xmath124 ) the computational timeof the model selection algorithm @ xmath125 ) for @ xmath126 random variables . to support cross - validation and with faster processing times , the closed form criterion for the best estimators of the data under special conditions is provided . we use the kernel - based regression form of loocv for the regression introduced by @ xcite : @ xmath127 ^ { - 1 } [ i - h ] [ \ | _ 2 ^ { } { n } \ end { i } \ ] ] where @ xmath128 is the hat function . * * the log of marginal likelihood ( gpr ) * is a kernel - based regression method . for a given data set @ xmath129 , and @ xmath130 , a uniform gaussian distribution is defined as a function @ xmath5 such that , @ xmath131 , where @ xmath39 is a constant . marginal likelihood is used as the model selection criterion for gpr , since it distinguishes between the goodness - of - fit and utility of a model . finding the log of marginal likelihood obtains the optimal parameters for model selection . the log of marginal likelihood is defined as : @ x##math132 where @ xmath133 denotes the parameter , ##n , @ xmath134 , denotes the complexity , and @ xmath135 denotes the quality factor . without loss of generality in this section gpr , the model selection method is used in gpr . * icomp : * the information - based icomp algorithm for @ xcite is the selection criterion to select the model and parameters defined by @ xmath136 , where @ xmath50 , and @ xmath39 translate into equations [ q : cicomp ] , and [ eq : sigmaicomp ] . in this section we compare the performance of kic on experimental , and real datasets , and compare with other model selection methods . kic was first used on the problem of approximating @ xmath137 from a set of data points sampled at regular intervals in @ xmath138 $ ] . to improve sensitivity to noise , the random noise was applied to the @ xmath139 data with two mean - to - variance ( nsr ) ratios : @ xmath140 , and @ xmath141 . the [ sinc ] shows the sinc function andthe following datasets . the following experiments were performed : ( 1 ) shows how kic balances between gof and complexity , ( 2 ) shows how kic and mse in training data change when the sample size and the level of noise affect the parameters , ( 3 ) investigates the effect of using multiple parameters , and ( 4 ) tests the consistency of kic with parameter estimation . the experiments were run 100 times on randomly generated datasets , and using test sets of over ##flow . * table 1 . * the effect of @ xmath21 on complexity , lack - of - fit and kic , was measured by setting @ xmath142 , with krr , randomly generated and a gaussian kernel with different standard deviation , @ xmath143 , selected from the 100 data ##set . the results are summarized in table [ la _ la _ kic ] . the model associated with @ xmath144 overfits , because it is overly complex , while @ xmath145 is a simple model that underfits . as the ridge parameter @ xmath21 increases , the model complexity decreases and the lack - of - fit is adversely affected . kic balances between thesetwo terms , which yields a way to select a model that has high reliability , as well as goodness of fit to the parameters . * squared error . * the effect of the sample size was measured by the sample sizes , @ xmath1 , of 50 , and 100 , for a total of four sets of experiments : ( @ xmath146 ) : ( @ xmath147 ) , ( @ xmath148 ) , ( @ xmath149 ) , ( @ xmath150 ) . the gaussian kernel was measured with @ xmath151 . the kic , and mean squared error ( mse , @ xmath152 ) , for different @ xmath153 @ xmath154 are shown in figure [ kic - mse ] . the data with nsr = @ xmath141 have larger mse values , and lower error rates , and consequently higher kic values compared to those with nsr = @ xmath140 . in both cases , kic and mse change with increasing frequency with respect to @ xmath21 . the parameters and the sample size have an effect on kic for selecting the best model ( x@ xmath21 ) . * experiment 2 . * the effect of using the convex kernel , @ xmath155 , versus the cauchy kernel , @ xmath156 , was investigated , where @ xmath157 , and @ xmath158 allowed the computation of the model - specific model selection of icomp , kic , gpr , and loocv . the results are shown in figures [ convex kernel ] and [ cauchy kernel ] . the figures are the plots with values at @ xmath159 , and @ xmath160 of the empirical distribution of mse values . as expected , the mse of both methods is larger when nsr is used , @ xmath161 , and smaller for the larger of the two data sets ( 50 % ) . loocv , icomp , and kic performed better , and better than gpr using the convex kernel for data with nsr @ xmath162 . in the remaining cases , the same results ( for mse ) were obtained with kic . all methods have smaller mse values than the convex kernel versus the cauchy kernel . gpr withthe cauchy test gave results comparable with kic , but with a standard deviation closer to 1 . * + 4 . * we assessed the frequency of tuning / tuning the parameters of the model in comparison with loocv . we considered an average of two sizes , @ xmath163 , and nsr @ xmath164 . the parameters to tune / tune were @ xmath165 @ xmath166 , and @ xmath167 for the gaussian model . the results of selecting the parameters are shown in figure [ loocv ] for loocv , and in figure [ kic _ frequency ] for kic . the more consistent criterion is the more reliable quality criterion . the results show that kic is more reliable for selecting the parameters , than loocv . loocv is also related to sample size . it provides a more reliable basis for benchmarks with @ xmath168 samples . + we used the samples taken from the following datasets ( www . cs . toronto . edu / ~ / / ~ ) : ( 1 ) core dataset ( 4177 samples , 3 dimensions ) , ( 2 ) c - set of datasets ( 41datasets ; 8192 instances , 8 dimensions ) , and ( 2 ) pu - family of datasets ( 8 datasets ; 8192 instances , 8 dimensions ) . for the first dataset , the goal was to determine the number of participants . we used 1000 samples for each [ 0 , 1 ] . the experiment was repeated 100 times to determine the confidence level . in addition , 1000 samples were selected randomly as the data set and the remaining 4077 samples as the test set . the kin - family and pu - family datasets are computer simulations of the human arm taking into account combinations of attributes such as whether the arm movement is nonlinear ( n ) or nearly linear ( f ) , and whether the degree of movement ( unpredictability ) of the arm is : medium ( m ) , or high ( n ) . the pu - family includes : kin - 8fm , kin - 8fh , kin - 8 nm , kin - 8nh datasets , and the puma - family contains : puma - 8fm , kin - 8fh , kin - 8 nm , and kin - 8nh datasets . in the kin - family of datasets , thethe angular position of the off - axis robot arm , the distance of the end ##point of the robot arm from its starting point are predicted . the angular position of the link of the robot arm is determined by the angular positions , the distances , and the torques of the link . we compare kic _ 1 ( [ q : kic1 ] ) , kic _ 2 ( [ eq : kic2 ] ) , and kic with loocv , icomp , and gpr across the three datasets . the results are shown in box - shapes in figures [ family ] , [ kin - family ] , and [ e - family ] for q , kin - family , and e - family datasets , respectively . the best results across the three datasets were obtained using kic , and the second best results were for loocv . for the q dataset , comparable results were obtained for kic and loocv , that were better than icomp , and the smallest mse value obtained using sgpr . kic _ 1 , and kic _ 2 had larger mse values , which are better than for the other two . for the kin - family datasets, except for kin - 8fm , kic obtained better results than gpr , icomp , and loocv . kic _ 1 , and kic _ 2 obtain better results than gpr , and loocv for kin - 8fm , and kin - 8 nm , which are datasets with low levels of noise , and higher mse values for datasets with low noise ( kin - 8fh , and kin - 8nh ) . for the kin - 8 datasets , kic got the best results for all datasets except for the puma - 8 ##fm , where the highest mse was achieved by loocv . the result of kic is comparable to icomp and better than gpr for kin - 8 ##fm dataset . for kin - 8fm , kin - 8fh , and kin - 8nh , although the result of mse for loocv and gpr is comparable to kic , kic obtained a less high mse ( smaller interquartile between the two samples ) . the median mse value for kic _ 1 , and kic _ 2 is comparable to the median mse values of the two samples on kin - 8fm, and puma - 8 ##f , where the noise level is moderate compared to alpha - 8fh , and alpha - 8nh , where the noise level is high . the sensitivity of kic _ 1 , and kic _ 2 to noise is due to the existence of variance in their formula . kic _ 2 has a higher interquartile of mse than kic _ 1 in datasets with low noise , which highlights the importance of @ xmath109 in the formula ( see [ ref : kic2 ] ) rather than @ xmath16 in equation . we introduced a new kernel - based information criterion ( kic ) for model complexity in regression analysis . the complexity factor in kic is defined on a variable - valued variance which explicitly measures the variance of each parameter involved in the model ; whereas in methods such as kernel - based icomp and gpr , this variance is based on a random matrix , which measures the total contribution of the individual parameters . we provided empirical data showing how kic , loocv ( with kernel - based closed form formula of the variance ) , kernel - based icomp , and gpr ,on different artificial intelligence and real time datasets : abalon , kin ##ect , and puma + . in these cases , kic efficiently balances the degree of fit and uncertainty of the data , is robust to noise ( although for higher noise we have larger confidence intervals than expected ) and sample size , is efficient in tuning / selecting the ridge and valley parameters , and has significantly smaller or comparable mean error rates with respect to competing methods , even using stronger regressors . the effect of using different methods was also investigated since the lack of a proper kernel plays an important role in competing methods . kic had superior results for different kernels and for the proper kernel a higher mse . this method was supported by fnsnf ##2 ( p1tip2 _ 148352 , pbtip2 _ 140015 ) . we want to thank john gretton , and zoltn szab for the original discussions .