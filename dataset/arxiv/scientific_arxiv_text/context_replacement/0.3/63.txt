given suitable mathematical models to model a system , we need to know whether our data is compatible with the candidate model . often selecting models involves collecting and fitting time series data to the parameter values and then applying an information criterion to select a ` best ' model @ xcite . however sometimes it is not feasible to determine the value of these unknown ##s ( e . g . , phase space , nonlinear wave function , nonidentifiable etc ) . the same problem has motivated the growth of fields that have a parameter - free approach such as chemical reaction network theory and stoichiometric methods @ xcite . however many of these fields are limited to studying the behavior of models in steady - state @ xcite . motivated by techniques commonly used in applied algebraic geometry @ xcite and algebraic statistics @ xcite , methods for comparing between models without estimating parameters have been developed for steady - state models @ xcite , applied to models in wnt signaling @ xcite , and recently extended to only include the data point @ xcite . specifically , these methods construct a model @ xmath0 from the state space @ xmath1 using techniques from applied algebraic geometry and tests whether the steady - state variables are compatible with this particular model of thethus , called a _ steady - state model _ @ xcite . } the method does not require parameter ##ization , and instead includes a parameter trade - off for _ compatibility with course data . thus , we present a method for comparing models with _ _ _ data _ via computing the _ differential invariant _ . we present equations of the form @ xmath2 and @ xmath3 where @ xmath4 is a known input into the system , @ xmath5 , @ xmath6 is the known output ( s ) from the system , @ xmath7 , @ xmath8 are species parameters , @ xmath9 , @ xmath10 is the unknown @ xmath11dimensional parameter value , and the parameters @ xmath12 are rational functions of their values . the parameters of the system can be expressed in terms of discrete time series where @ xmath13 is the input at discrete time and @ xmath14 is the output . in this setting , we aim to improve our mathematical understanding by eliminating parameters we could not measure by differential equations from linear algebra . from the equations , we form a differential equation , where the associated monomials have coefficients that are independent of the parameters @xmath15 . we obtain a system of equations in 0 , 1 , and their partial derivatives and we write this implicit system of equations as @ xmath16 , @ xmath7 , and call these the input - output equations our _ _ equations _ . finally , we obtain equations of the form : @ xmath17 where @ xmath18 are linear combinations of the variables and @ xmath19 are the monomials , i . e . monomials in @ xmath20 . we will see here that in the linear case , @ xmath21 is a linear differential equation . for non - linear models , @ xmath21 is nonlinear . if we substitute into the differential invariant available data into the linear monomials for each of the time points , we can obtain a linear system of equations ( each point is a different time point ) . then we get : if there exist a @ xmath22 such that @ xmath23 . if @ xmath24 of then we are guaranteed a non trivial solution and the non - linear case can be determined via a rank criterion ( i . e . , svd ) and we replace the rank criterion developed in @ xcite with thewe improved in @ xcite , but for @ xmath23 there may be no solution . therefore , we must test if the linear system of in @ xmath23 is complete , i . e . has zero or infinitely many solutions . when measurement noise is zero , we have a statistical cut - off for when the model is consistent with the data . however suppose that we does not have data points for the higher order derivative of , then these have to be estimated . we present a method using gaussian process regression ( gpr ) to estimate the time complexity of performing the gpr . since the variance of a gp is also gp , then we can take the higher order derivative of the data as well as the measurement noise , and estimate the error observed during the gpr ( so we can remove points with too much gpr measurement noise ) . this allows us to input the data into the differential invariant and test model , using the same method with the statistical cut - off we present . we illustrate our method throughout with examples from linear and nonlinear regression . we also have some background in linear algebra since a crucial step in our method is to perform differential elimination to obtain estimates , in terms of input variables , dependent variables , and parameters . for this reason, we will also provide information on the algorithms from differential algebra , to understand the algorithm evaluation ##s . for a more detailed description of differential ideals and the algorithms described above , see @ xcite . in what follows , we hope the reader is familiar with concepts such as _ ideals _ and _ ideals _ , which are covered in great detail in @ xcite . a ring @ xmath25 is said to be a _ differential ideal _ if there is a ring _ in @ xmath25 and @ xmath25 is closed under differentiation . differential ideal _ is an ideal which is closed under differentiation . a finite description of a differential ideal is called a _ differential characteristic set _ , which is a finite description of a possibly infinite set of differential polynomials . we get the following definition from @ xcite : let @ xmath26 be a set of differential polynomials , not necessarily finite . if @ xmath27 is an auto - reduced set , such that no lower - auto - reduced set can be found in @ xmath26 , then @ xmath28 is called a _ differential characteristic set _ . a well - known fact in differential algebra is that differential ideals need not be finitely generated @ xcite . however , a _ differential ideal _is generated by the _ ritt - raudenbush function of _ @ xcite . this result gives rise to ritt - pseudodivision ##s ( see below ) , allowing us to find the differential characteristic set of a given differential equation . we then describe various ways to find the differential characteristic set and other related notions , and we show how they are relevant to our problem , namely , they can be used to solve the _ input - output equations _ . consider an ode system of the form @ xmath29 and @ xmath30 for @ xmath7 with @ xmath31 and @ xmath32 as functions of their arguments . let our differential ideal be generated by the two polynomials obtained by taking the right - hand - side from the ode system to obtain @ xmath33 and @ xmath34 for @ xmath7 . then the differential characteristic set is of the form @ xcite : @ xmath35 the _ @ xmath36 terms of the differential characteristic set , @ xmath37 , are those terms independent of the state variables and when set to zero in the _ input - output equations _ : @ xmath38 . , the @ xmath36 term -output equations @ xmath39 are polynomial equations in the field @ xmath40 with rational coefficients and the identity vector @ xmath10 . note that the differential characteristic set is in general non - unique , but the coefficients of the input - output equations can be fixed only by normalizing the equations to make them monic . we then discuss several methods to find the input - output equations . the first method ( ritt s pseudodivision algorithm ) can be used to find a differential characteristic set for a radical differential ideal . the second method ( rosenfeldgroebner ) gives a representation of the radical of the differential ideal as the intersection of two differential ideals and can also be used to find a differential characteristic set under certain conditions @ xcite . next , we discuss grbner basis methods to find the _ input - output equations _ . the differential characteristic set of a prime differential ideal is a set of coefficients for the equation @ xcite . the algorithm to find a differential characteristic set of a radical ( in particular , prime ) differential ideal generated by a finite number of rational polynomals is called ritt s pseudodivision algorithm . we discuss the process in detail below , which comes from the description of @ xcite . note that our differential equations areconsider above as a partial differential equation @ xcite . let @ xmath41 be the root of the polynomial @ xmath42 , which is the highest degree polynomial of the variables appearing in that equation . a polynomial @ xmath43 is said to be of _ higher rank _ than @ xmath42 if @ xmath44 or , whenever @ xmath45 , the algebraic degree of the leader of @ xmath43 is less than the algebraic degree of the leader of @ xmath42 . a polynomial @ xmath43 is _ reduced with respect to the polynomial _ @ xmath42 if @ xmath43 contains neither the root of @ xmath42 with equal or greater algebraic degree , nor its derivative . if @ xmath43 is _ reduced with respect to @ xmath42 , it can be solved by using the pseudodivision algorithm above . 1 . if @ xmath43 contains the @ xmath46 and @ xmath47 of the leader of @ xmath42 , @ xmath42 is reduced @ xmath48 , so the leader is @ xmath47 . 2 . reduce the polynomial @ xmath43 by the root ofthe highest degree of @ xmath47 ; let @ xmath49 be the result of the division of this new polynomial by @ xmath50 with respect to the variable @ xmath47 . then @ xmath49 is reduced with respect to @ xmath50 . the polynomial @ xmath49 is called the _ pseudoremainder _ of the pseudodivision . the polynomial @ xmath43 is reduced by the pseudoremainder @ xmath49 and the division is done by @ xmath51 in terms of @ xmath50 and so on , until the pseudoremainder is reduced with respect to @ xmath42 . this algorithm is applied to a set of differential ideals , such that each ideal is reduced with respect to each other , to form an ideal - reduced polynomial . the result is the polynomial reduced polynomial . using the differentialalgebra package in maple , one can construct a representation of the radical of a differential ideal generated by two polynomials , as an intersection of radical differential ideals with respect to a given variable and as a prime differential ideal using a different ranking @ xcite . similarly , the rosenfeldgroebner package in maple takes two functions : sys andr , where sys is a type of set of differential equations or inequations which are all rational in the independent and dependent variables and their values and , is a regular polynomial ring built with the command differentialring for the independent and dependent variables and a ranking for them @ xcite . then rosenfeldgroebner gives a representation of the radical of the differential ring generated by sys , and the intersection of radical differential ideals saturated in the polynomial family generated by the inequations . in sys this basis consists of the set of rational polynomial chains with respect to the ranking of r . note that rosenfeldgroebner returns a rational characteristic set if the differential ideal is prime @ xcite . thus , both algebraic and rational grbner bases can be used to represent the input - model equations . to construct an algebraic grbner basis , one can take a finite number of values of the model equations and then treat the derivatives of the variables as indeterminates in the polynomial ring in @ xmath52 , @ xmath53 , @ xmath54 , . . . , @ xmath55 , @ xmath56 , @ xmath57 , . . . , @ xmath58 , @ x##math59 , @ xmath60 , . . . , etc . } a grbner basis of the equations generated by this linear system of ( differential ) equations with an additional step where the state variables and their derivatives are eliminated , can be found . details of this approach can be found in @ xcite . other grbner bases have been developed by carr ferro @ xcite , ollivier @ xcite , and mansfield @ xcite , but currently there are no implementations for computer algebra in @ xcite . we now know how to use the differential invariants arising from model elimination ( using ritt s pseudodivision , the groebner basis , or some other method ) for model design / optimization . recall our input - output relations , for differential equations , are of the form : @ xmath17 the coefficients @ xmath19 are differential monomials , i . e . monomials in the input / output vector @ xmath61 , @ xmath62 , @ xmath63 , etc , and the coefficients @ xmath18 are rational coefficients in the input parameter vector @ xmath10 . in order to precisely fix the rational coefficients @ xmath18 to the differential mono##mials @ xmath19 , we normalize the input / output variable to make it monic . in other words , we can re - write our input - output relations as : @ xmath64 by @ xmath65 obtain a linear equation in the input / output variables @ xmath61 , @ xmath62 , @ xmath63 , etc . if the values of @ xmath61 , @ xmath62 , @ xmath63 , etc , are known at a sufficient number of time instances @ xmath66 , then we could write in terms of @ xmath19 and @ xmath65 at each of these time instances to obtain a linear system of equations in the variable @ xmath67 . we write the solution of a linear input - output equation . if there are @ xmath68 unknown ##s @ xmath67 , we write the equation : @ xmath69 we write this linear system as @ xmath23 , where @ xmath28 is the @ xmath70 by @ xmath68 solution of the form : @ xmath71 @ xmath22 is theis of unknown coefficients @ xmath72 ^ t $ ] , and @ xmath73 is of the form @ xmath74 ^ t $ ] . for the case of two input - output equations , we have the following double diagonal system of equations @ xmath23 : @ xmath75 where @ xmath28 is a @ xmath76 ##7 @ xmath77 matrix . for noise - free ( noisy ) models , this system @ xmath23 should have a unique solution for @ xmath22 @ xcite . in other words , the solution @ xmath67 of the input - output equation can be easily found from enough input / output data @ xcite . the basic idea of this method is the following . given a set of candidate models , we find their corresponding differential invariants and then factor the values of @ xmath20 , etc , at the time points @ xmath78 , thus setting up the corresponding system @ xmath23 for each model . the solution to @ xmath23 should be unique for the correct model , and there should be a solution for each of the other models . thus under certain conditions , one should be able toselect the correct model since the input / output data corresponding to that model should satisfy its differential invariant . thus , one should be able to select the incorrect model since the input / output data should not satisfy their differential invariant . however , with imperfect data , there could be no solution to @ xmath23 except for the correct model . thus , with imperfect data , one should be able to select the correct model . on the other hand , if there is no solution to @ xmath23 for each of the incorrect models , then the problem is to see how ` ` if ' ' each of the models fail and reject models altogether . we now describe how to reject models . take @ xmath80 and consider the model system @ xmath81 where @ xmath82 . then , in our case , @ xmath83 , and @ xmath84 is just the system @ xmath73 . here , we study the behavior of under ( a special form of ) rejection of both @ xmath28 and @ xmath84 . let @ xmath85 and @ xmath86 be the perturbed versions of @ xmath28 and @ xmath##84 , respectively , and note that @ xmath87 and @ xmath88 depend only on @ xmath85 and @ xmath86 , respectively . our goal is to determine the _ value _ of the optimal solution from those of @ xmath85 and @ xmath86 respectively . we can show how to detect the presence of the null matrix , but first use notation . the singular value of the matrix @ xmath80 will be denoted by @ xmath89 ( note that we have already calculated the number of singular values of @ xmath28 from @ xmath90 to @ xmath68 . ) the rank of @ xmath28 is denoted @ xmath91 . the rank of @ xmath28 is denoted @ xmath92 . here , @ xmath93 refers to the maximum value . the basic idea will be to prove that a null matrix that has a solution , i . e . , @ xmath94 , and then to express its consequences in terms of @ xmath85 and @ xmath86 . if these consequences are not satisfied , then we replace by usingthat is unsolvable . in other words , we can provide _ necessary but _ sufficient _ conditions for to find a solution , i . e . , we can always prove ( but not test ) the null hypothesis . we will refer to this procedure as _ test _ the _ hypothesis . we first obtain two simple results . the first , weyl s inequality , is quite simple . let @ xmath95 . then @ xmath96 weyl s inequality can be used to prove @ xmath91 using knowledge of the @ xmath85 . let @ xmath97 and assume that @ xmath98 . let @ xmath99 [ cor : weyl - rank ] then , if is not true , let @ xmath100 . test the null hypothesis . let @ xmath94 , ( @ xmath101 ) = \ operatorname { [ } ( n ) \ leq \ ( ( m , n ) $ ] . similarly , @ xmath102 ) = [ $ ] . but we do not have access to @ xmath103 $ ] and so we use instead the notation [ matrix @ xmath104 $ ]. by the null hypothesis , @ xmath105 ) \ leq \ | [ \ tilde { a } - a , \ begin { b } - a ] \ | \ leq \ | \ begin { a } - a \ | + \ | \ begin { b } - a \ | . \ begin { eqn : augmented - rank } \ begin { a } \ ] ] [ s : sparse - matrix ] equal to [ cor : weyl - matrix ] . in other words , if does not exist , it has no value . this statement will fail to completely satisfy the null hypothesis if @ xmath28 is ( numerically ) sparse - rank . as an example , suppose that @ xmath106 and @ @ xmath107 consist of a single element ( @ xmath108 ) . ( @ xmath101 ) \ leq [ $ ] , ( @ xmath102 ) = 0 $ ] ( or is small ) . assuming that @ xmath109 and @ xmath110 are small , @ xmath111 ) $ ] will therefore also be small . in addition , we should consider that theassertion that @ xmath101 ) = \ operatorname { n } ( 1 ) $ ] . however , we can only establish lower bounds on the singular rank ( we can only check if the singular rank is ` ` sufficiently large ' ' ) , so this is not feasible in practice . an alternative approach is to use the _ _ _ value obtained by thresholding . how to achieve such a threshold , however , is not at all straightforward and can be a very difficult task especially if the algorithms have high dynamic range . the algorithm is uninformative if @ xmath112 since then @ xmath102 ) = \ operator _ { n + 1 } ( \ tilde { a } , \ tilde { b } ) = 0 $ ] $ . however , this is not a significant improvement over that stated above since if @ xmath28 is full - dimensional , then it must be known that is solvable . as a demonstration of this , we first apply theorem [ thm : n - matrix ] to a simple linear model . we start by taking the input and output data and then add a specific amount of noise to the output data and attempt to correct the linear model . in the subsequent sections , we will show howto prove : [ ex : augmented - x ] statistically find a particular ` ` noise ' ' model for the model . here , we take data from the linear 3 - compartment model , add it , and try to find the general form of the linear 3 - compartment model with the same input / output equation . [ ex : mainex ] let our model be a 3 - compartment model of the general form : @ xmath113 @ xmath114 . we have an ode to the first compartment of the matrix @ xmath115 and the second compartment is empty , so that @ xmath116 is the output . the solution to this system of equations can be easily found of the form : @ xmath117 so that @ xmath118 . the input - output equation for : @ xmath119 the model with a single input / output to the first compartment has the form : @ xmath120 where @ xmath121 are the coefficients of the characteristic polynomial of the matrix @ xmath28 and @ xmath122 are the coefficients of the characteristic polynomial of the matrix @ xmath123 which represents the first row and first column of@ xmath28 removed . we then substitute values of @ xmath124 at time instances @ xmath125 into our input - output equation and solved the resulting linear system of equations for @ xmath126 . we find that @ xmath127 , which agrees with the coefficients of the characteristic polynomials of @ xmath28 and @ xmath123 . we now attempt to solve the 2 - compartment model using 1 - compartment model only . we find the input - output equations for the @ xmath128 - model with a different input / output to the same compartment , which have the form : @ xmath129 where and @ xmath130 are the coefficients of the characteristic polynomial of the matrix @ xmath28 and @ xmath131 are the coefficients of the characteristic polynomial of the matrix @ xmath123 which has the first row and first column of @ xmath28 removed . we substitute values of @ xmath132 at time instances @ xmath133 into our input - output equation and attempt to solve the resulting linear system of equations for @ xmath134 . the expression ##ity for the matrix @ xmath28 with thethe values of @ xmath135 at time instances @ xmath133 are : @ xmath136 the singular values of the matrix @ xmath137 with the substituted values of @ xmath132 at time instances @ xmath133 are : @ xmath138 we add noise to our vector matrix in the following way . to each entry @ xmath139 , and @ xmath140 , we add @ xmath141 where @ xmath142 is a random real number between @ xmath143 and @ xmath144 , and @ xmath145 equals @ xmath146 . then the noisy matrix @ xmath85 has the following singular values : @ xmath147 we also add noise to our vector @ xmath73 in the following way . to each entry @ xmath148 , we add @ xmath141 where @ xmath142 is a random real number between @ xmath143 and @ xmath144 , and @ xmath145 equals @ xmath146 . then the noisy matrix @ xmath149 isthe following are proved : @ xmath150 we find the matrix @ xmath151 and apply the norm of this matrix to the smallest singular value of @ xmath149 . since the frobenius norm of @ xmath151 is @ xmath152 , which is _ less than _ the smallest singular value @ xmath153 , we can reject this model . thus , given the 3 - compartment model , , we are able to reject the 2 - compartment model . we can perform the statistical inference of the solvability of . here , we have a noise model . if the parameters @ xmath109 and @ xmath110 are bounded , e . g . , @ xmath154 and @ xmath155 for some @ xmath156 ( with a maximum accuracy of @ xmath145 in the ` ` ' ' ' @ xmath85 and @ xmath86 ) , the theorem [ 1 : 1 - 1 ] can be proved at most . however , it is impossible to model the data with normal random variables , which are not discrete . therefore , we can assume a noise model of the form@ xmath157 where @ xmath158 is a ( computable ) matrix that depends on @ xmath85 and agrees with @ xmath159 , @ xmath160 denotes the hadamard ( entrywise ) inner product @ xmath161 , and @ xmath162 is a real - valued random matrix whose entries @ xmath163 are independent standard vectors . in our case of interest , the entries of @ xmath158 depend on those of @ xmath85 as well . consider @ xmath164 for the input vector @ xmath165 . note that we can only observe the ` ` dominates ' ' of @ xmath166 . then the only perturbed matrix entries are @ xmath167 using the additivity formula @ xmath168 for the gaussians . however , the statistical conclusion is still wrong since @ xmath169 ` ` dominates ' ' @ xmath170 in the sense that the former has variance @ xmath171 , and the latter has variance at @ xmath172 . in other words , we are looking here in the statistical conclusion .this is taken into account in @ xcite . ] @ xmath173 therefore , @ xmath174 therefore , to first order in @ xmath145 , @ xmath175 an analogous result holds for @ xmath159 . both of the results of the proof above are equivalent to @ xmath109 and @ xmath110 ( for example [ sec : augmented - matrix ] , the limit is simply the product of these bounds ) and so can be expressed as @ xmath176 with absorbing constants . the following proof is given as follows . let @ xmath177 be a special case , i . e . , @ xmath111 ) $ ] in [ sec : augmented - matrix ] . then since @ xmath178 where we have made explicit the dependence of both sides on the same underlying random mechanism @ xmath179 , the ( probability ) distribution function of @ xmath177 will be that of @ xmath176 , i . e . , @ xmath180 + , @ xmath181 [ eqn : prob - 1 ] note that if , e . g ., @ xmath182 ( i . e . , if @ xmath84 is known , ) , then simplifies to , @ xmath183 . alternatively , we can associate a @ xmath184 - value to a given value of @ xmath177 by referencing upper - bounds for values of the form @ xmath185 . recall that @ xmath186 under the null hypothesis . in the standard statistical significance testing procedure , we can therefore reject the null hypothesis if for at least @ xmath187 , where @ xmath187 is the statistical significance of ( e . g . , @ xmath188 ) . we then proceed to , @ xmath189 , where we can assume that @ xmath190 . this can be done in several ways . an easy way is to assume that @ xmath191 where @ xmath192 has the frobenius norm , ( @ xmath193 but @ xmath194 has the normal distribution ) . ] with @ xmath195 degrees of freedom . therefore , @ xmath196 if , each term in can be made explicit : thefirst is loose in the sense that @ xmath197 . the second is that @ xmath198 but @ xmath199 a slightly better solution is to use the inequality @ xcite @ xmath200 where @ xmath201 and @ xmath202 are the @ xmath203th row and @ xmath204th column , respectively , of @ xmath205 . the @ xmath206 bound can also be expressed using a probability distribution via @ xmath207 as well or directly using the lower bound ( see below ) . variants of this undoubtedly exist . first , we can refer to the bound as tropp @ xcite . the bound follows from 4 . 3 for @ xcite . let @ xmath190 , where and @ xmath163 . then for any @ xmath208 , @ xmath209 [ th : hadamard - normal ] the bound for @ xmath210 can then be expressed as follows . let @ xmath211 so that @ xmath212 . then by theorem [ th : hadamard - gaussian ] , @ x##math213 \ , dt , \ text { t } \ ] ] where @ xmath214 and @ xmath215 are the ` ` ' ' ' terms of the equation for @ xmath158 and @ xmath159 , respectively . the term in dt corresponds to @ xmath216 \ \ & = \ frac { 1 } { \ sigma _ { a } ^ { 2 } \ sigma _ { b } ^ { 2 } } \ left [ ( \ sigma _ { a } ^ { 2 } + \ sigma _ { b } ^ { 2 } ) \ left ( t - \ frac { \ sigma _ { a } ^ { 2 } } { \ sigma _ { a } ^ { 2 } + \ sigma _ { b } ^ { 2 } } } \ right ) ^ { 2 } + \ sigma _ { b } ^ { 2 } \ left ( t - \ frac { \ sigma _ { b } ^ { 2 } } { \ sigma _ { a } ^ { 2 } + \ sigma _ { b } ^ { 2 } } \ right ) ] ^ { 2 } \ right ] \ \ & = \ frac { 1 }{ \ sigma _ { a } ^ { 2 } \ sigma _ { a } ^ { 2 } } \ left [ ( \ sigma _ { a } ^ { 2 } + \ sigma _ { b } ^ { 2 } ) \ right ( t - \ frac { \ sigma _ { a } ^ { 2 } } { \ sigma _ { a } ^ { 2 } + \ sigma _ { b } ^ { 2 } } x \ right ) ^ { 2 } + \ frac { \ sigma _ { a } ^ { 2 } \ sigma _ { a } ^ { 2 } } { \ sigma _ { a } ^ { 2 } + \ sigma _ { b } ^ { 2 } } x ^ { 2 } \ right ] \ \ & = \ frac { \ sigma _ { a } ^ { 2 } + \ sigma _ { b } ^ { 2 } } { \ sigma _ { a } ^ { 2 } \ sigma _ { b } ^ { 2 } } \ left ( t - \ frac { \ sigma _ { a } ^ { 2 } } { \ sigma _ { a } ^ { 2 } + \ sigma _ { b } ^ { 2 } } x \ right ) ^{ align } + \ frac { x ^ { 2 } } { \ sigma _ { a } ^ { 2 } + \ sigma _ { b } ^ { 2 } } \ text { align } \ ] ] \ to the right . now , @ xmath217 \ int _ { 0 } ^ { 2 } \ exp \ left [ - \ frac { a } { align } \ right ( \ frac { \ sigma _ { a } ^ { 2 } + \ sigma _ { b } ^ { 2 } } { \ sigma _ { a } ^ { 2 } \ int _ { b } ^ { 2 } } \ right ) \ left ( [ - \ frac { \ sigma _ { a } ^ { 2 } } { \ sigma _ { a } ^ { 2 } + \ sigma _ { b } ^ { 2 } } } \ right ) ^ { 2 } \ right ] dt . \ begin { align } \ ] ] . , @ xmath218 so that the integral is @ xmath219 dt = \ int _ { 0 } ^ { 2 } \ exp \ left [ - \ frac { ( - - \ , - ) ^ { 2 } }{ - \ alpha ^ { 2 } } \ right ] dt . \ end { aligned } \ ] ] the following is @ xmath220 dt : @ xmath221 dt = \ phi \ int _ { - \ alpha x / \ sigma } ^ { ( 1 - \ alpha ) x / \ sigma } x ^ { - x ^ { 2 } / 2 } \ , du = \ sqrt { - \ right } \ , \ left [ \ phi \ left ( \ frac { ( 1 - \ alpha ) x } { \ sigma } \ right ) - \ phi \ left ( - \ frac { \ alpha x } { \ sigma } \ right ) \ right ] , \ end { aligned } \ ] ] where @ xmath222 is the joint cumulative distribution function . \ , @ xmath223 \ exp \ left [ - \ frac { a } { 1 } \ left ( \ frac { x ^ { 2 } } { \ sigma _ { a } ^ { 2 } + \ sigma _ { x } ^ { 2 } } \ right ) \ right ] . \ end { eqn : p1 } \ end { aligned } \ ] ] dt =( see much simpler ) instead of @ xmath224 we can develop a method for the partial partial derivatives and the mean function using random process , and then apply the above # ##s to both linear and nonlinear models in the subsequent section . a random process ( gps ) is a random process @ xmath225 , where @ xmath226 is the mean function and @ xmath227 a covariance function . it is often used for regression / estimation as follows . suppose that there is an unknown random process @ xmath228 that we can easily estimate with no measurement error : @ xmath229 , where @ xmath230 for @ xmath231 the posterior delta . we consider the problem of finding @ xmath228 in a bayesian setting , defining it to be a process with posterior delta and covariance functions @ xmath232 and @ xmath233 , respectively . then the probability distribution of @ xmath234 ^ { { \ mathsf { t } } } $ ] and the observation points @ xmath235 ^ { { \ mathsf { t } } } $ ] and@ xmath236 ^ { { \ mathsf { t } } } $ ] at the initial point @ xmath237 ^ { { \ mathsf { t } } } $ ] is @ xmath238 the posterior distribution of @ xmath239 and @ xmath240 is the same : @ xmath241 where @ xmath242 is the posterior mean and variance , respectively . this allows us to test @ xmath239 on the basis of testing @ xmath243 . the two components of @ xmath244 are the posterior variance and are the uncertainty associated with this test procedure . here is an expression for the variance of @ xmath239 . what if we want to find posterior variance ? test @ xmath245 for the likelihood function @ xmath48 . suppose @ xmath246 has linearity of coefficients . now , @ xmath247 \ to { x } ( \ boldsymbol { t } ) \ cr \ - x ( \ boldsymbol { t } ) \ - x ' ( \ boldsymbol { t } ) \ -\ vdots \ - i ^ { ( 1 ) } ( \ boldsymbol { s } ) \ left \ begin { pmat } \ sim { \ mathcal { t } } \ left ( \ begin { pmat } [ { . } ] \ mu _ { { \ text { prior } } } ( \ boldsymbol { s } ) \ left \ - \ mu _ { { \ text { prior } } } ( \ boldsymbol { s } ) \ left \ mu _ { { \ text { prior } } } ^ { ( 1 ) } ( \ boldsymbol { s } ) \ cr \ vdots \ - \ mu _ { { \ text { prior } } } ^ { ( 1 ) } ( \ boldsymbol { s } ) \ left \ begin { pmat } , \ begin { pmat } [ { | . . . } ] \ mu _ { { \ text { prior } } } ( \ boldsymbol { s } , \ boldsymbol { s } ) + \ begin ^ { prior } ( \ boldsymbol { s } ) } & \ mu _ { { \ text { prior } } } ^ {{ \ mathsf { t } } } ( \ boldsymbol { s } , \ boldsymbol { t } ) & \ sigma _ { { \ text { prior } } } ^ { ( n , 0 ) , { \ mathsf { t } } } ( \ boldsymbol { s } , \ boldsymbol { t } ) & \ cdots & \ sigma _ { { \ text { prior } } } ^ { ( n , 0 ) , { \ mathsf { t } } } ( \ boldsymbol { s } , \ boldsymbol { t } ) \ - \ - \ sigma _ { { \ text { prior } } } ( \ boldsymbol { s } , \ boldsymbol { t } ) & \ sigma _ { { \ text { prior } } } ( \ boldsymbol { prior } , \ boldsymbol { t } ) & \ sigma _ { { \ text { prior } } } ^ { ( n , 0 ) , { \ mathsf { t } } } ( \ boldsymbol { s } , \ boldsymbol { t } ) & \ cdots & \ sigma _ { {\ text { prior } } } ^ { ( n , 1 ) , { \ mathsf { t } } } ( \ boldsymbol { s } , \ boldsymbol { s } ) \ cr \ sigma _ { { \ text { prior } } } ^ { ( 1 , 0 ) } ( \ boldsymbol { prior } , \ boldsymbol { s } ) & \ sigma _ { { \ text { prior } } } ^ { ( 1 , 0 ) } ( \ boldsymbol { s } , \ boldsymbol { s } ) & \ sigma _ { { \ text { prior } } } ^ { ( 1 , 0 ) } ( \ boldsymbol { s } , \ boldsymbol { s } ) & \ cdots & \ sigma _ { { \ text { prior } } } ^ { ( n , 1 ) , { \ mathsf { t } } } ( \ boldsymbol { prior } , \ boldsymbol { s } ) \ cr \ vdots & \ vdots & \ vdots & \ ddots & \ vdots \ cr \ sigma _ { { \ text { prior } } }^ { ( n , 0 ) } ( \ boldsymbol { s } , \ boldsymbol { prior } ) & \ sigma _ { { \ text { prior } } } ^ { ( n , 0 ) } ( \ boldsymbol { t } , \ boldsymbol { prior } ) & \ sigma _ { { \ text { prior } } } ^ { ( n , 0 ) } ( \ boldsymbol { t } , \ boldsymbol { prior } ) & \ cdots & \ sigma _ { ( n , 0 ) } ( \ boldsymbol { s } , \ boldsymbol { prior } ) \ , \ begin { pmat } \ , ) , \ begin { prior } \ ] ] where @ xmath248 is the posterior variance for @ xmath249 and @ xmath250 . this posterior variance is usually of the form . an exact version of this gives the same variance of @ xmath251 for each @ xmath252 . however , if we are interested only in the posterior variance of each @ xmath253 , then it follows to estimate each @ xmath254and independently : @ xmath255 the cost of computing @ xmath256 can then be calculated by computing @ xmath203 . we can consider the special case of the standard error ( se ) distribution function @ xmath257 , \ { { aligned } \ ] ] where @ xmath258 is the squared error and @ xmath90 is the probability density . the se function is one of the most widely used se functions in statistics . its derivative can be expressed in terms of the ( probabilists ) hermite polynomials @ xmath259 ( these are also sometimes denoted @ xmath260 ) . the first three hermite polynomials are @ xmath261 , @ xmath262 , and @ xmath263 . we need to compute the derivative @ xmath264 . then @ xmath265 so that @ xmath266 . then @ xmath267 and @ xmath268 . then , @ xmath269 the # ##a allows us to compute the derivatives of the hyperparameters @ xmath270 , @ xmath271 , and @ xmat##h90 . in practice , however , these are hardly ever used . in the examples below , we deal with this by estimating the hyperparameters from the data by maximizing the likelihood . we do this by using a nonlinear - gradient algorithm , which can be quite close to the initial data set , where we do multiple tests over a large area of hyperparameter space and return the best results found . this improves the quality of the resulting hyperparameters but can still sometimes fail . we base our method on competing models : linear compartment models ( 2 and 3 species ) , lotka - volterra models ( 2 and 3 species ) and others . as the linear compartment differential equations are described in an earlier section , we compute the differential invariants of the lotka - volterra and models using rosenfeldgroebner . we use each of these models to generate the course estimates , with varying levels of noise , and estimate the necessary higher order derivatives using gp fit . as described in the earlier section , we expect the values of the higher order derivatives to satisfy a negative - likelihood value , because the gp fit is not ` good ' . in some cases , this can be remedied by increasing the number of data points. using the . . ##a data , we model each of the models using the differential invariant ##s of the models . [ ex : lv2 ] the two species lotka - volterra model is : @ xmath272 where @ xmath273 and @ xmath274 are variables , and @ xmath275 are parameters . we assume only @ xmath273 is known and perform differential elimination and obtain our differential invariant in terms of : @ xmath276 : @ xmath277 [ ex : lv3 ] . with the additional variable @ xmath278 , the three species lotka - volterra model is : @ xmath279 where only @ xmath116 is known . performing differential elimination , the differential invariant is : @ xmath280 [ ex : lor ] the two species model , the linear model , is described by the system of equations : @ xmath281 we assume only @ xmath116 is known , perform differential elimination , and obtain the differential invariant : @ xmath282 [ ex : lc2 ] a linear 2 - dimensional model without input can be writtenexample : @ xmath283 where @ xmath273 and @ xmath274 are variables , and @ xmath284 are parameters . we assume only @ xmath273 is observable and perform differential elimination and obtain our differential invariant in terms of only @ xmath276 : @ xmath285 [ x : lc3 ] the linear 3 - dimensional model without parameters example : @ xmath286 where @ xmath287 are variables , and @ xmath288 are parameters . we assume only @ xmath273 is constant and perform differential elimination and obtain our differential invariant in terms of only @ xmath276 : @ xmath289 by assuming @ xmath116 in examples 1 . 16 . 1 represents the same observable model , we apply our analysis to data taken from each model and perform model fitting . the data are simulated and 100 data points are obtained from @ xmath165 for each model . we add different levels of random ##ness to the simulated data , and then extract the higher order parameters from the data . for example , during our study we found that for the parameters of the lotka -volterra three species model , e . g . @ xmath290 $ ] , we obtained a positive log - correlation , which meant that we could only estimate the higher order parameters of the data . once the correlation is obtained and derivative data is obtained from the linear regression , each model parameter value is compared against the corresponding differential invariants . these are shown in figure [ fig - 1 ] , where a score of 1 , means model 0 , and the means model is used . we find that we can reject the two species lotka - volterra model and lorenz model for data simulated from the lotka - volterra three species ; thus the linear compartment models are compatible . for data from the two species lotka - volterra model , the linear compartment model and two - species lotka - volterra can be used until the correlation increases and then the method can no longer reject both models . for data simulated from the lorenz model can be reject the two species linear compartment and two species lotka - volterra model . $ ] and initial condition @ xmath291 $ ] . ( b ) data simulated from three species lotka - volterra model with parameter values @ xmath292 $ ] and thecondition @ xmath293 $ ] . ( c ) data simulated from the lorenz model with parameter values @ xmath294 $ ] and initial condition @ xmath293 $ ] . ( d ) data simulated from the linear ##ized three dimensional model with parameter values @ xmath295 $ ] and initial condition @ xmath296 $ ] . ] we have demonstrated our model rejection algorithm for various models . in this paper , we make some important important points regarding differential equations . note that we have assumed that the coefficients are all unknown and we have not taken the possible linear relationships among the coefficients into account . this theoretical problem is another reason our work only involves model rejection and not model selection . thus , the unknown coefficient is treated only as an independent unknown variable in our linear system of equations . however , there may be instances where we d like to consider all this additional information . we first investigated the effect of incorporating the unknown values . in @ xcite , an explicit formula for the input - output equations for linear models was found . in particular , it was shown that the condition @ xmath297compartment ##s corresponding to two connected graphs with at least one leak and with the same input and output compartments , havethe model has polynomial form of the input - output equations . for example , a linear 2 - compartment model with a single input and output in the same compartment and corresponding to a strongly connected graph with at least one leak has the form : @ xmath298 thus , our leak ##age problem would not hold for two different linear 2 - compartment models with the above - mentioned form . in order to discriminate between two such models , we need to take other factors into account , e . g . , parameter values . consider the following two linear 2 - compartment models : @ xmath299 @ xmath300 whose corresponding input - output equations are of the form : @ xmath301 assume that both of these models are of the above - mentioned form , i . e . linear 2 - compartment models with a single input and output in the same compartment and corresponding to strongly connected graphs with at least one leak . in the first model , there is a leak from the first compartment and an exchange between compartments @ xmath144 and @ xmath128 . in the second model , there is a leak from the second compartment and an exchange between compartments @ xmath144 and @ xmath128 . assume that thethen @ xmath302 is true . in the first model , this reduces our invariant to : @ xmath303 in the second model , our invariant is : @ xmath304 in this case , the right - hand sides of the two equations are the same , but the first equation has two variables ( coefficients ) and the second equation has three variables ( coefficients ) . thus , if we had data from the second model , we could try to reject the first model ( much like the 1 - compartment versus 2 - compartment model discrimination in the examples below ) . in other words , the vector in the span of @ xmath305 and @ xmath306 for @ xmath307 may not be in the span of @ xmath139 and @ xmath140 respectively . we also have the possibility of incorporating polynomial dependency relationships . while we can only include the linear algebraic dependency relationships among the coefficients in our linear algebraic approach to model selection , we can include polynomial dependency relationships , such as the coefficients being polynomial constants . we have already seen the way in which this can happen in the first model ( from the nonzero parameter values ) . we can explore the case where the coefficients go to zero .from the above expression for input - output equations from @ xcite , we get that a linear model without any leaks has a zero coefficient for the value of @ xmath140 . thus a linear 2 - compartment model with a linear input and output in the first compartment and corresponding to a simply connected graph without any leaks has the invariant : @ xmath308 . to discriminate between two different linear 2 - compartment models , one with leaks and one without any leaks , we should introduce this zero coefficient into our equation . consider the following two linear 2 - compartment models : @ xmath309 @ xmath310 whose linear input - output equations are of the form : @ xmath311 in the first model , there is a leak from the first compartment and an exchange between compartments @ xmath144 and @ xmath128 . in the second model , there is an exchange between compartments @ xmath144 and @ xmath128 and no leaks . thus , our invariant can be written as : @ xmath312 again , the right - hand sides of the two equations are the same , but the first equation has three variables ( and ) while the second equation has two variables (coefficients ) . thus , if we reject vectors from the first model , we could choose to reject the second model . in other words , any vector in the span of @ xmath305 and @ xmath306 for @ xmath307 could not be in the span of @ xmath139 and @ xmath306 for . finally , we consider the identifiability properties of our model . if the number of parameters is greater than the number of coefficients , then the model is unidentifiable . on the other hand , if the number of parameters is less than or equal to the number of coefficients , then the model could not be identifiable . clearly , an identifiable model is equivalent to an unidentifiable model . we note that , in our method of forming the linear system @ xmath23 from the input - output model , we could in principle solve for the coefficients @ xmath22 and then solve for the coefficients from these two coefficient sets if the model is identifiable @ xcite . however , this is not a commonly used method to estimate coefficient sets in general . as noted above , the linear algebraic dependency relationships among the coefficients are not taken into account in our linear system method . this isthat there could be many different models with the same differential algebraic form of the input - output relations . if such a model can not be rejected , we note that an identifiable model satisfying a certain input - output relation is preferred over an identifiable model satisying the same form of the input - output relations , as we see in the following example . consider the following two linear two - compartment models : @ xmath299 @ xmath313 whose corresponding input - output relations are of the form : @ xmath314 in the first model , there is a leak from the first compartment and an exchange between compartments @ xmath144 and @ xmath128 . in the second model , there are leaks from both compartments and an exchange between compartments @ xmath144 and @ xmath128 . therefore , both models have equations of the form : @ xmath298 since the first model is identifiable and the second model is identifiable , we prefer to use the equations of the first model if the model s ##1 can not be rejected . by performing this differential algebraic method of rejection , one has already obtained the input - output relations to obtain the identifiability @ x##cite . in a sense , our method extends the full range of potential methods for comparing models with time course data , in that one method must reject incompatible models , then test structural identifiability of the models with input - output equations derived from the incompatible equations , infer parameter values of the incompatible models , and use an # ##al ##bedo selection technique to assert the best model . notably the presented differential invariant comparison method does not check for model consistency , unlike traditional model selection techniques . rather , we reject when a model may not , for some parameter value , be compatible with the best model . we found that simpler models , such as the simple single compartment model could be rejected when data is simulated from a more complex model , such as the three species lotka - volterra model , which elicits a wider range of behavior . on the other hand , more complex models , such as the linear model , are often not rejected , from data simulated from more complex models . in addition it would be useful to better understand the relationship between differential invariants and behavior . we also believe it would be beneficial to investigate the properties of sloppiness @ xcite . we believe there is large scope for additional error - free coplanarity model selection methods . itwould be beneficial to know which algorithms for solving equations can have larger numbers , and whether this problem could be explored . the authors acknowledge funding from the american institute of mathematics ( aim ) where this research commenced . the authors thank mauricio barahona , david cohen , and david sullivant for the funding . we are also grateful to paul green for writing about nm and for his gp ##a , which serves as an easy template to get started . nm was initially funded by the david and lucille packard foundation . hah received funding from ams packard foundation grant , epsrc fellowship ep / k041096 / 2009 and the stumpf leverhulme trust grant . a . aistleitner , _ relationships between grbner functions , differential grbner functions , and _ characteristic functions _ , masters thesis , johannes kepler universitt , 2010 . h . akaike , _ a new look at the biological function identification _ , ieee trans . automat . math , * 1 * ( 1974 ) , pp . 716723 . f . boulier , _ differential elimination and biological functions _ , radon . opt . math . , * 2 * ( 2007 ) , pp . 111 - 139 . f . boulier , j . k##zard , f . ollivier , m . petitot , _ expression for the solution of a previously presented differential equation _ , in : issac ##s : proceedings of the 1995 international symposium on symbolic and symbolic computation , pp 158 - 166 . acm press , 1995 . g . carr ferro , _ grbner polynomials and differential equations , in m . huguet and m . poli , editors , proceedings of the 6th international conference on linear algebra , algebraic , and error - correction codes , volume 356 of lecture notes in computer science , pp . 131 - 142 . springer , 1987 . clarke , _ _ polynomial analysis _ , mathematical biophys . , 2 ( 1988 ) , pp . 34 . clarke , m . smith , and donal oshea , _ methods , polynomials , and equations _ , springer , new york , 2007 . m . conradi , m . saez - rodriguez , j . d . gilles , j . raisch , _ using _ reaction control methods to discard a reaction _ _ _ , iee proc . 152 ( 2005 ) , pp . s . diop , _ differential equation decision trees and their applications to control systems _ , * , * ( 1992 ) , pp . 137 - 161 .j . drton , b . sturmfels , j . sullivant , _ lectures on mathematical statistics _ , oberwolfach verlag ( munich , germany ) vol . 1 . 2009 . m . feinberg , _ chemical reaction network structure and the stability of complex isothermal reactors i . the deficiency zero and deficiency one models _ , chem . , * 43 * ( 1987 ) , pp . 22292268 . m . feinberg , _ chemical reaction network structure and the stability of complex thermal reactors ii . the transition states for networks of deficiency zero _ , chem . , * 43 * ( 1988 ) , pp . 125 . a . forsman , _ on linear algebra and quantum field theory _ , doctoral thesis , linkping ##er , 1991 . a . golubitsky , a . kondratieva , m . a . maza , and v . ovchinnikov , _ a test for the rosenfeld - grbner algorithm _ , j . science comput . , * 43 * ( 2008 ) , pp . 582 - 610 . e . gross , m . j . harrington , a . rosen , b . sturmfels , _ computational number theory : a case study for the application##nt ##h _ , j . biol . , * 109 * ( 1 ) ( 2016 ) , pp . 23 - 29 . e . gross , j . brown , k . l . ho , j . myers , m . harrington , _ using computational geometry for model selection _ , s . s . gunawardena , _ distributivity and processivity of multisite phosphorylation can be determined by steady - state simulation _ , biophys . rev . , 93 ( 2007 ) , pp . gutenkunst , m . j . waterfall , j . p . casey , j . l . brown , j . a . myers , s . s . sethna , _ model - determined sensitivities in molecular dynamics _ _ , plos comput . biol . , 93 ( 2007 ) , harrington , k . l . ho , t . thorne , m . j . h . stumpf , _ parameter - based model selection _ based on steady - state coplanarity _ , proc . , * 109 * ( 1 ) ( 2012 ) , pp . 1574615751 . i . kaplansky , _ an introduction to linear programming _ , springer , berlin , 1957 . e . gross. kolchin , _ abstract groups and algebraic geometry _ , j appl . math . , * 1 * ( 1973 ) . j . ljung and m . smith , _ _ _ identifiability for multi - parameterization _ , automatica , * 1 * ( 1 ) ( 1994 ) , pp . 265 - 276 . maclean , m . rosen , j . j . byrne , m . j . harrington , _ error - correction to model wnt - models and the design of systems _ , proc . , * 112 * ( 1 ) ( 2015 ) , pp . 26522657 . j . anderson , _ _ grbner ##s _ , phd thesis , university of michigan , 1991 . j . s . manrai , s . gunawardena , _ the theory of multisite phosphorylation _ , ann . , * 1 * ( 2008 ) , pp . 55335543 . # ##s . , http : / / www . maplesoft . com / view / view / view / view . aspx ? view = differentialalgebra s . meshkat , j . anderson , and a . a . distefano * , _ introduction to ritt _ pseudodivisionfor all the input - output relations of multi - compartment models _ , nature biosci . , * 239 * ( 2012 ) , pp . 117 - 123 . a . meshkat , a . sullivant , and a . eisenberg , _ identifiability relations for certain classes of linear regression models _ , j . opt . biol . , * 34 * ( 2015 ) , pp . 1620 - 1651 . f . ollivier , _ le probleme de lidentifiabilite structurelle globale : solution theoretique , methodes effectives and bornes de complexite _ , doctoral thesis , ecole polytechnique , 1990 . f . ollivier , _ a system of differential equations _ . in t . sakata , editor , proceedings of the 6th international symposium on applied mathematics , computing , and error - correction systems , volume 508 of lecture notes in computer science , pp . 304 - 321 . springer , 1991 . orth , i . thiele , j . . palsson , _ what is optimal control theory ? _ nature biotechnol . , * 34 * ( 2010 ) , pp . 1 , 2 . j . a . hansen , _ optimal control for the_ _ . the mit press : cambridge , 2006 . j . h . ritt , _ differential equations _ , cambridge ( 1950 ) . m . a . saccomani , m . audoly , and g . dangi , _ _ identifiability of differential equations : the case of boundary conditions _ , automatica * 39 * ( 2003 ) , pp . 619 - 632 . user - defined correlation functions for sums of random variables . found . 1 : 389434 , 2012 . formulas for the absolute value of hadamard numbers . siam j . applied math . 51 ( 10 ) : 10931095 , 1997 .