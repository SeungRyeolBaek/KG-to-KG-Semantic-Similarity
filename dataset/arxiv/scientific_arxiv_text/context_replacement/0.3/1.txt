additive models @ xcite are an important family of models for semiparametric inference and classification . the reasons for the success of additive models are their increased stability when compared to linear or generalized linear models and their increased interpretability when compared to other nonparametric models . it is well - known that good models in additive models are in general less prone to the curse of high dimensionality than good models in fully linear models . some examples of additive estimators belong to the general class of additive kernel based methods over a reproducing complex hilbert space @ xmath0 , see e . g . @ xcite . in the last decade , important results on learning rates of regularized kernel based models for additive models have been obtained when the focus is on learning and when the classical least squares loss function is compact , see e . g . @ xcite , @ xcite , @ xcite , @ xcite , @ xcite , @ xcite and the references therein . of course , the least squares loss function is compact and has many nice mathematical properties , but it is only locally lipschitz , and therefore , kernel based methods based on this loss function typically rely on bad statistical robustness issues , .if the kernel is convex . this is in sharp contrast to kernel methods based on a lipschitz continuous loss function and on a bounded influence function , where bounds on upper bounds for the maxbias bias and on a bounded influence function are known , see e . g . @ xcite for the general case and @ xcite for additive models . therefore , we will here consider the case of the kernel based methods based on a general convex and lipschitz continuous loss function , on a general kernel , and on the classical regularizing term @ xmath1 for and @ xmath2 which has a smoothness penalty but not a sparsity penalty , see e . g . @ xcite . such regularized kernel based methods are now usually called support vector methods ( svms ) , although the term was originally used for kernel methods based on the special hinge loss function and for special kernels instead , we refer to @ xcite . in this paper we address the fundamental question , whether an svm with an additive kernel can provide a much better learning rate in high dimensions than an svm with a general kernel , either a convex or rbf kernel , if the kernel of an additive model is satisfied . our leading example of learning rates forquantile regression based on the lipschitz , a non - differentiable pinball loss function , which is always under check . in the literature , see e . g . @ xcite and @ xcite for kernel quantile regression and @ xcite , @ xcite , and @ xcite for kernel based quantile regression . we will not address the question how to check whether the kernel of an additive model is satisfied because this would be a topic of a paper of its own . of course , a better approach would be to fit both models and calculate their risks separately for test purposes . for the same reason we will probably not cover it . consistency of support vector ##ization driven by the kernel for additive models is considered in @ xcite . in this paper we establish learning problems for these models . let us consider the framework with a complete additive metric space @ xmath3 as the input space and a complete subset @ xmath4 of @ xmath5 as the output space . a borel probability measure @ xmath6 and @ xmath7 is used to model the learning problem and an independent and uniformly distributed sample @ xmath8 is used according to @ xmath6 for learning . the kernel function@ xmath9 is used to check the quality of a loss function @ xmath10 with the local error @ xmath11 . _ using the algorithm we see that @ xmath12 is smooth , @ xmath13 , linear with respect to the random variable , and @ lipschitz , satisfying @ xmath14 with a finite error @ xmath15 . _ support vector machines ( svms ) considered here are vector - based regularization schemes for the sparse kernel vector space ( rkhs ) @ xmath0 generated by the mercer algorithm @ xmath16 . with the same loss function @ xmath17 , for dealing even with heavy - tailed errors as @ xmath18 , they take the form @ xmath19 where for the general borel function @ xmath20 on @ xmath21 , the function @ xmath22 is given by @ xmath23 where @ xmath24 is a constant parameter . the attempt to define a loss function has a long history , see e . g . @ xcite in the context of m - estimators . it was shown in @ xcite that @ xmath22 is not aminimizer of the following optimization problem involving the original additive function @ xmath12 if a minimizer exists : @ xmath25 the additive function we consider consists of the _ hypothesis space decomposition _ @ xmath26 with _ @ xmath27 a _ _ input space and the _ hypothesis space _ @ xmath28 where @ xmath29 is the set of quantities @ xmath30 each of which is also defined as a map @ xmath31 from @ xmath3 to @ xmath5 . thus the functions from @ xmath32 take the general form @ xmath33 . we note , that there is clearly not a notational problem here , because in the above formula each quantity @ xmath34 is an element of the sample @ xmath35 which is a subset of the full input space @ xmath36 , @ xmath37 , whereas in the case of _ @ xmath8 each quantity @ xmath38 is an element of the full input space @ xmath36 , where @ xmath39 . because these formulas can all be used in different contexts and because we do not expect any confusion , we say this formula is notand more intuitive than specifying these functions with different names . the additive kernel @ xmath40 is written in terms of the kernels @ xmath41 on @ xmath27 and @ xmath42 it is the rkhs @ xmath0 which can be written in terms of the rkhs @ xmath43 given by @ xmath41 on @ xmath27 according to the form ( [ additive ] ) let @ xmath44 with kernel given by @ xmath45 the kernel of @ xmath46 on @ xmath47 to illustrate properties of additive models , we provide two ways of comparing them with additive kernels . the first example deals with gaussian rbf models . all examples can be given in the [ proofsection ] . [ gaussadd ] let @ xmath48 , @ xmath49 $ ] and @ xmath50 ^ 2 . $ ] let @ xmath51 and @ xmath52 . \ ] ] the additive kernel @ xmath53 is given by @ xmath54 . , the additive kernel @ xmath55 is the additive gaussian kernel given by @ xmath##56 defines a continuous function @ xmath57 on @ xmath58 ^ [ $ ] and one on another . let @ xmath59 and @ xmath60 then @ xmath61 where @ xmath62 denotes the rkhs generated by the standard mercer rbf kernel @ xmath63 . the following section is about sobolev kernels . [ sobolvadd ] [ @ xmath64 , @ xmath65 $ ] and @ xmath58 ^ [ . $ ] let @ xmath66 : = \ bigl \ { u \ in l _ 2 ( [ 0 , 1 ] ) ; } ^ \ alpha u \ in l _ 2 ( [ 0 , 1 ] ) \ mbox { ~ for ~ { ~ } | \ alpha | \ { ##q \ bigr \ } \ ] ] is the sobolev kernel consisting of all square integrable univariate functions whose derivative is also square integrable . there is an rkhs with a mercer kernel @ xmath67 generated by @ xmath68 ^ [ $ ] . if we go from the mercer kernel @ xmath69 toif @ xmath67 , then @ xmath70 $ ] for each @ xmath71 . the additive function @ xmath72 is also a mercer kernel and is an rkhs @ xmath73 \ { \ } . \ ] ] however , the [ sobolev space @ xmath74 ^ { ) $ ] , consisting of all square integrable functions whose partial derivatives are also square integrable , contains these functions and is also an rkhs . consider the marginal distribution of @ xmath6 , @ xmath27 and @ xmath75 . under the assumption that @ xmath76 for each @ xmath71 and that @ xmath43 is included with @ xmath29 in the @ xmath77 - family , it was proved by @ xcite that @ xmath78 in general as long as @ xmath79 satisfies @ xmath80 and @ xmath81 . the rest of the article gives the same result . the [ ratessection ] presents our recent results on learning rates for svms based on mercer kernels . learning rates for quan##tile problems are treated as the special cases . section [ comparisonsection ] is a comparison of our results with the learning algorithms published above . section [ proofsection ] contains all the results and some examples which may be useful on their application . in this section we have calculated learning rates for the support vector machines generated by the formula for the polynomials which helps in the quantitative analysis presented in @ xcite . the results are about asymptotic reduction of the excess risk @ xmath82 and take the form @ xmath83 with @ xmath84 . they can be computed under four types of conditions : the hypothesis space @ xmath0 , the measure @ xmath6 , the loss @ xmath12 , and the choice of the regularization function @ xmath85 . the first condition is about the approximation ability of the hypothesis space @ xmath0 . since the output function @ xmath19 comes from the hypothesis space , the learning rates of the learning algorithms depend on the approximation ability of the hypothesis space @ xmath0 with respect to the probability distribution @ xmath86 given by the following : ##s . [ defapprox ] the squared error of the triple @ xmath8##7 is defined by @ xmath88 to estimate the kernel error , we make an estimate on the minimizer of the kernel @ xmath89 for each @ xmath90 , define the kernel of @ xmath91 together with the kernel @ xmath41 by @ xmath92 we assume that @ xmath93 is a bounded and positive operator of @ xmath94 . hence we can find the positive eigenpairs @ xmath95 and that @ xmath96 is an orthonormal operator of @ xmath94 and @ xmath97 by @ xmath98 . then @ xmath99 . then we can find the @ xmath100 - th power @ xmath101 of @ xmath93 by @ xmath102 this is a positive and bounded operator and its range is well - defined . the kernel @ xmath103 and @ xmath104 are in this range . [ assumption1 ] we define @ xmath105 and @ xmath106 where for each @ xmath107 and each @ xmath10##8 , @ xmath109 is a function of the form @ xmath110 with some @ xmath111 . the expression @ xmath112 of assumption [ assumption1 ] for each @ xmath113 is in the rkhs @ xmath43 . the equivalent formula in the literature ( e . g . , @ xcite ) for achieving equality of the function @ xmath114 for the approximation error ( [ approxerrordef ] ) is @ xmath115 with some @ xmath116 . since the function @ xmath117 is defined as @ xmath118 in general , this can also be written in the additive form . however , the function [UNK] ( [ additive ] ) has an additive form @ xmath119 . so it is possible for us to impose an additive expression @ xmath120 for the target function @ xmath121 with the component of @ xmath113 satisfying the additive expression @ xmath110 . the above natural assumption leads to a technical problem in calculating the approximation error : the function @ xmath113 has no direct relationship to the marginal function @ xmat##h122 maps onto @ xmath27 , but existing methods in the literature ( e . g . , @ xcite ) can not be applied directly . note that on the parameter space @ xmath123 , there is no natural probability measure projected from @ xmath6 , and the distribution of @ xmath124 is not known . our idea to overcome the difficulty is to use the auxiliary function @ xmath125 . this may not minimize the risk ( which is not always possible ) . however , it approximates the component function @ xmath113 exactly . when we add two such functions @ xmath126 , we get a good approximation of the auxiliary function @ xmath121 , and thereby a good estimate of the approximation error . this is the main novelty of the paper . [ approxerrorthm ] under the [ assumption1 ] , we have @ xmath127 where @ xmath128 is the probability measured by @ xmath129 the second problem for our first problem is finding the capacity of the set , measured by @ xmath130 - the natural numbers . let @ xmath131 be the set of points on @ xmath##21 and @ xmath132 for some @ xmath133 the * covering number of @ xmath131 * with respect to the associated metric @ xmath134 , such that @ xmath135 is defined as @ xmath136 and the * @ xmath130 - empirical covering number * of @ xmath137 is defined as @ xmath138 [ assumption2 ] we assume @ xmath139 and that for some @ xmath140 , @ xmath141 and some @ xmath142 , the @ xmath130 - empirical covering number of the center ball of @ xmath43 is @ xmath143 the main goal of this paper is to show that the additive expansion of the hypothesis space has the following integral formula with the thirty - one maximum value for the covering numbers of the points of the hypothesis space @ xmath0 , to be found in the [ samplesection ] . [ capacitythm ] in the [ assumption2 ] , for some @ xmath144 and @ xmath145 , we assume @ xmath146 the formula for the covering numbers ofthe theorem [ capacitythm ] is special : the power @ xmath147 is independent of the number @ xmath148 of the balls of the additive model . it is well - known @ xcite in the field of function theory that the covering numbers of balls of the sobolev space @ xmath149 [ the cube @ xmath150 ^ s $ ] of the hypothesis space @ xmath151 with the index @ xmath152 have the following linear relationship with @ xmath153 : @ xmath154 here the index @ xmath155 depends linearly on the index @ xmath148 . similar dimension - dependent results for the covering numbers of the rkhss associated with gaussian rbf - kernels can be found in @ xcite . the second condition in theorem [ capacitythm ] gives an advantage of the additive model in terms of capacity of the corresponding hypothesis space . the third condition for our learning problem is that the noise is in the ball @ xmath6 with respect to the hypothesis space . before stating the third condition , we consider the special case for quantile regression , to illustrate our general problem . let @xmath156 is the quantile of . the quantile loss function @ xmath157 is defined by the definition @ xmath158 to be the @ xmath159 - quantile of @ xmath160 , i . e . , the function @ xmath161 = @ xmath162 the regularization scheme for quantile regression defined here takes the form ( [ algor ] ) with the loss function @ xmath12 replaced by the exponential loss function @ xmath163 a sufficient condition on @ xmath6 for quantile regression is given by @ xcite as follows . to this end , let @ xmath164 be the similarity measure of @ xmath165 and @ xmath166 . then the real number @ xmath167 is called @ xmath159 - quantile of @ xmath164 , if and only if @ xmath167 belongs to the ( @ xmath168 \ bigr ) \ le \ le \ mbox { ~ ~ and ~ ~ } q \ bigl ( [ 0 , \ infty ) \ bigr ) \ le ( - \ le\ bigr \ } \ , . \ ] ] it is well - known that @ xmath169 has a confidence level . [ noisecond ] . @ xmath166 . let . a probability measure @ xmath164 on @ xmath165 is said to have a * @ xmath159 - quantile of type @ xmath170 * , if there is an @ xmath159 - quantile @ xmath171 and a * @ xmath172 such that , for [ @ xmath173 $ ] , we have @ xmath174 $ . [ @ xmath175 $ ] . we say that a probability measure @ xmath20 on @ xmath176 has a * @ xmath159 - quantile of @ xmath177 - of type @ xmath170 * if the same probability measure @ xmath178 on @ xmath179 - also has an @ xmath159 - quantile of type @ xmath170 and the function @ xmath180 where @ xmath181 is the function defined by equation ( 1) , and @ xmath182 . we can show that every distribution @ xmath164 has an @ xmath159 - quantile of type @ xmath170 has a corresponding @ xmath159 - quantile @ xmath183 . similarly , if @ xmath164 has a lebesgue density @ xmath184 then @ xmath164 has an @ xmath159 - quantile of type @ xmath170 if @ xmath184 is far away from [ s @ xmath185 $ ] since we can use @ xmath186 \ } $ ] = ( [ tauquantileoftype2formula ] ) . this theorem is strong enough to test many distributions used in computational statistics such as normal , with s @ xmath187 , and logistic distributions ( with @ xmath188 ) , gamma and log - normal distributions ( with @ xmath189 ) , and gamma and normal distributions ( with @ xmath190 $ ] ) . the following theorem , to be stated in the [ proofsection ] , gives the learning rate for the following##ization of ( [ algor ] ) is the special case of quantile ##s . [ quantilethm ] implies that @ xmath191 always matches for any kernel @ xmath192 , and that the kernel @ xmath41 matches @ xmath193 with @ xmath194 for any @ xmath195 . if then [ assumption1 ] , with @ xmath112 and @ xmath6 [ matching @ xmath159 - quantile of @ xmath177 - data of @ xmath170 for some @ xmath196 $ ] , then , setting @ xmath197 , for any @ xmath198 and @ xmath199 , with probability at most @ xmath200 we get @ xmath201 where @ xmath202 is a constant multiple of @ xmath203 and @ xmath204 and @ xmath205 . note that the kernel @ xmath206 given by ( [ quantilerates2 ] ) for the decision problem of ( [ quantilerates ] ) is independent of the quantile of @ xmath159 ,of the dimension @ xmath148 of large values of @ xmath207 , and of the dimension @ xmath208 and @ xmath209 . suppose that @ xmath210 , if @ xmath211 , and @ xmath212 if @ xmath213 . because @ xmath214 will be very close to @ xmath215 , the learning rate , which is independent of the dimension @ xmath216 and , by theorem [ quantilethm ] , is close to @ xmath217 for large values of @ xmath177 and is close to @ xmath218 for even , if @ xmath211 . to obtain our maximum learning rate , we make an assumption _ _ _ _ - expectation _ _ which is equivalent to a [ noisecond ] for the special case of quantile regression . [ assumption3 ] we assume that there is an exponent @ xmath219 $ ] and a positive constant @ xmath220 such that @ xmath221 assumption [ assumption3 ] also holds true for @ xmath222 . if the triple @ x##math223 under some conditions , the value @ xmath224 can be calculated . for example , when @ xmath12 is the maximum of ( [ pinloss ] ) and @ xmath6 is the @ xmath159 - quantile of @ xmath177 - of [ @ xmath225 for any @ xmath196 $ ] and @ xmath226 [ defined in @ xcite , then @ xmath227 . [ mainratesthm ] implies that @ xmath228 is bounded by the constant @ xmath229 almost surely . from assumptions [ assumption1 ] to [ assumption3 ] , if we have @ xmath198 and @ xmath230 for some @ xmath231 , and for any @ xmath232 , with probability at most @ xmath200 we get @ xmath233 where @ xmath234 is bounded by @ xmath235 and @ xmath202 is also independent of @ xmath203 and @ xmath204 ( to be discussed later in the article ) . we also have a certainand numerical simulations on the basis of our learning rates with those from the literature . as already mentioned in the paper , the reasons for the popularity of additive models are simplicity , increased interpretability , and ( possibly ) a reduced proneness of the problem of high dimensions . hence it is important to note , whether the learning rate described in theorem [ mainratesthm ] with the assumption of an additive model actually compares to ( essentially ) the learning rates without this assumption . in other words , we need to show that the main goal of this paper is satisfied by theorem [ quantilethm ] and theorem [ mainratesthm ] , i . e . that an svm based on an additive model will provide a much better learning rate in high dimensions than an svm with a regular kernel , say a classical gaussian rbf kernel , provided the assumption of an additive model is satisfied . our learning rate in theorem [ quantilethm ] is new and optimal in the literature of svm for quantile regression . most learning rates in the literature of svm for quantile regression are optimal for the projection operators @ xmath236 , since it is well known that projections are learning rates @ xcite . here the projection operator @ xmath237is defined for a continuous function @ xmath10 , @ xmath238 . this is called integration . continuous functions are defined in @ xcite . for example , under the assumptions that @ xmath6 is an @ xmath159 - quantile of @ xmath177 - average of @ xmath170 , the # ##al condition ( [ approxerrorb ] ) is that for some @ xmath239 , and that for some constants @ xmath240 , the sequence of eigenvalues @ xmath241 of the differential ##s @ xmath117 and @ xmath242 for some @ xmath243 , it is shown in @ xcite that with values at least @ xmath200 , @ xmath244 where @ xmath245 , the parameter @ xmath246 is the sum of the rkhs @ xmath247 and thus plays a similar role as that of the parameter @ xmath147 under step 1 . for a @ xmath193 , and @ xmath112 , we can define @ xmath246 and @ x##math147 to be sufficiently large and the approximation error index @ xmath248 can be expressed as @ xmath249 . the error rate of the [ quantilethm ] may be reduced by applying step 1 to the sobolev convergence condition for @ xmath121 and a similar condition for the marginal distribution @ xmath250 . for example , we can consider a gaussian kernel @ xmath251 based on the sample functions @ xmath203 and @ xcite with the approximation error of ( [ approxerrorb ] ) for some @ xmath252 . this is true for quantile ##s ##n @ xcite . since we are mainly interested in linear models , we shall not discuss such an assumption . [ gaussmore ] with @ xmath48 , @ xmath49 $ ] and @ xmath50 ^ [ . $ ] as @ xmath51 and the error function @ xmath72 is replaced by ( [ gaussaddform ] ) with @ xmath253 in [ [ gaussadd ] , @ xmath52 . \ ] ] if the function @ xmath##121 ##5 given by ( [ gaussfcn ] ) , @ xmath191 , surely for some constant @ xmath192 , and @ xmath6 for some @ xmath159 - quantile of @ xmath177 - and of @ xmath170 for some @ xmath196 $ ] , and , setting @ xmath197 , for some @ xmath145 and @ xmath199 , ( [ quantilerates ] ) , with probability at most @ xmath200 . it is unclear whether the above learning rate can be derived from other estimates in the literature ( e . g . @ xcite ) or with projection . note that the expression in the above example is independent of the sample size . it would be useful to know whether there exists some @ xmath99 such that the operator @ xmath57 given by ( [ gaussfcn ] ) is in the range of the operator @ xmath254 . the existence of such a positive index would lead to the squared error of ( [ approxerrorb ] ) , for @ xcite . let us also do some numerical comparisons . the estimates of our learning ratecompare our theorem [ mainratesthm ] with those proved by @ xcite . their corollary 4 . 12 is ( essentially ) minmax machine learning algorithm for ( essentially ) svms in the form of nonparametric quantile regression using a gaussian rbf distribution on the target input space with the boundary conditions of the input space . let us consider the case that the distribution @ xmath6 is an @ xmath159 - quantile of @ xmath177 - of type @ xmath170 , where @ xmath255 , and assume that the theorem 4 . 12 in @ xcite and our theorem [ mainratesthm ] are equivalent . i . e . , we have in general that @ xmath6 is a probability measure on @ xmath256 $ ] and that the probability distribution @ xmath257 has a lebesgue structure @ xmath258 for each @ xmath259 . similarly , assume that the linear decision function @ xmath260 has ( to be [ [ mainratesthm ] associated with @ xmath261 $ ] ) the additive structure @ xmath207 with the @ x##math104 is defined in # [ assumption1 ] , where @ xmath262 and @ xmath263 , with the parameter @ xmath86 and additionally , ( to make sense 1 . 1 , @ xcite ##r ) @ xmath264 where @ xmath265 $ ] and @ xmath266 is the besov space with the parameter @ xmath267 . the intuitive property of @ xmath248 is , that decreasing values of @ xmath248 lead to increased values . we refer to ( * ? ? ? * and p . 2 ) for details on besov spaces . it is well - known that the besov space @ xmath268 is the sobolev space @ xmath269 for @ xmath270 , @ xmath271 , and @ xmath272 , and that @ xmath273 . we note that if the @ xmath41 are the defined wendland spaces , their corresponding dual vector spaces @ xmath43 are sobolev spaces , and ( * ? ? ? * thm . ) . 2 , .. . ) . here , we use the same set of regularizing operators as in ( * ? ? ? 4 . 9 , cor . 4 . 12 ) , i . e . , @ xmath274 where @ xmath275 , @ xmath276 , @ xmath277 $ ] , and @ xmath278 is a well - known time constant ##s of @ xmath279 . for sake of simplicity , let us use @ xmath280 . example ( * ? ? ? 4 . 12 ) the learning rates for the set of svms for @ xmath159 - quantile functions , if a modified - rbf - version of @ xmath281 is used for @ xmath159 - quantile functions of @ xmath177 - - replace @ xmath170 with @ xmath255 , which is of type @ xmath282 . the learning rate of the [ quantilethm ] is better than the one of ( * ? ? ? 4 . 12 ) in this situation , if @ xmath283 then the assumption of the linear approximation is correct . see [ table1 ] liststhe values of @ xmath284 from ( [ explicitratescz2 ] ) for a given value of the are @ xmath216 , where @ xmath285 . all of these values of @ xmath284 are equal with the exceptions if @ xmath286 = @ xmath287 . this is in contrast to the corresponding exponent of the learning rate by ( * ? ? * cor . 1 . 12 ) , because @ xmath288 table [ table2 ] and figures [ figure1 ] to [ figure2 ] give no information about the value @ xmath289 . of course , higher values of the exponent imply higher rate of convergence . it is obvious , that the svm based on an additive model has a significantly faster rate of convergence for higher dimensions @ xmath216 compared to svm based on a pseudo - rbf kernel defined over the whole euclidean space , of course under the assumption that the additive model is valid . the results seem to indicate that our learning rate from the [ mainratesthm ] is probably not comparable for higher dimensions . however , the main focus of the original paper was on higher dimensions . . [ table1 ] theit returns the values of the functions @ xmath290 from ( * ? ? ? * cor . 1 . 0 ) and @ xmath291 from ( [ mainratesthm ] , respectively , if the given function @ xmath292 is chosen in the same way for the given function , i . e . @ xmath293 , with @ xmath294 for @ xmath295 and @ xmath296 . note that @ xmath297 $ ] . [ $ = " > , > , > , > " , ]