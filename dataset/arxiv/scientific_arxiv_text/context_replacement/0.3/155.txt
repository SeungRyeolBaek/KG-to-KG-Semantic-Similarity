we need a detailed discussion on the role of networks in network sampling strategies to shed light on how best to sample from networks . a _ network _ is a system of network entities typically represented simply as a graph : a set of vertices and a set of edges among the vertices . networks are complex and arise in many and diverse domains . for instance , many web - based social networks , such as internet social networks , produce large amounts of data on interactions and associations among people . mobile phones and location - aware devices produce copious amounts of data on human communication patterns and the proximity between people . in the domain of biology research , from neurons to bacteria to semantic webs , there is now access to large networks of information among various entities and the need to collect and analyze these data . with advances in technology , pervasive use of the internet , and the advent of mobile phones and location - aware devices , networks under study today are not only much larger than those in the past , but sometimes organized in a similar manner ( e . g . the use of blogs or the web itself ) . for many reasons , their network structure is not fully visible to the public and can only be accessed via ` ` crawls ' ' ( e . g . online social networks) . these factors would make it prohibitive to analyze or to access these networks in their entirety . how , then , should one proceed with analyzing and using these raw data ? one approach to addressing these issues is _ sampling _ : inference using small subsets of nodes and links from the network . from the example @ xcite to the crawling @ xcite and p2p networks @ xcite , network sampling occurs across many different networks . in the present section , we focus on a particular line of research that is concerned with finding samples that match critical structural features of the original network . such samples have numerous applications in data mining and information science . in @ xcite , for example , structurally - consistent samples were shown to be effective in improving the network performance in the larger networks and significantly improving the efficiency of protocol design . in section [ 2 : applications ] , we discuss the potential applications . although there have been a number of recent strides in research on network sampling ( e . g . @ xcite ) , there is still very much that requires further and deeper understanding . moreover , the networks under investigation , although treated as complete , are , in fact , _ incomplete _ due to differences in data collection methods . therefore , a more refined understanding of network samplingis of great importance to network analysis . towards this end , we conduct a detailed study on _ network sampling bias _ . there has been a recent spate of work focusing on _ problems _ that arise from network sampling bias including how and why they should be treated @ xcite . our work differs from much of this existing research in that , for the first time in a comprehensive manner , we examine network sampling bias as an _ opportunity to be exploited _ . we argue that biases of certain sampling strategies can be advantageous if they ` ` push ' ' the sampling process towards inclusion of specific properties of networks . our main aim in the present research is to identify and understand the relationship between specific sampling strategies and certain definitions of network representativeness , so that these concepts can be applied in practical applications . * summary of findings . * we conduct a detailed study of network sampling bias . we find that bias towards high _ density _ ( a cue from expander _ ) has some unique advantages over other biases such as those toward high density _ . we show both physically and theoretically that such an expansion bias ` ` pushes ' ' the sampling process towards new , larger nodes and the inclusion of wider parts of the network . in otherfirst , we show that a network sampling strategy that selects nodes with fewer connections from those already selected is often a very efficient alternative to simply sampling high degree nodes and locates well - connected ( i . e . low degree ) nodes significantly faster than most other methods . we also show that the breadth - first search , a widely - used sampling and search strategy , is often among the most successful performers in terms of both discovering the nodes and finding new , well - connected nodes . finally , we describe ways in which some of our findings can be exploited in several other areas including in fraud detection and market research . a number of these aforementioned findings are surprising in that they are in stark contrast to conventional methods followed by much of the scientific community ( e . g . @ xcite ) . not surprisingly , network sampling extends across many diverse areas . here , we briefly describe some of these different lines of research . * network sampling in population statistics . * the concept of sampling networks first arose to address situations where one needed to identify vulnerable or difficult - to - access networks ( e . g . , drug users , criminals ) . for recent research , one should refer to @ xcite . the research in this area focuses almost exclusively on generating unbiased estimates relating to variablesof interest attached to each network node . the present work , therefore , focuses on _ properties related to the _ network itself _ ( some of which are not close to being fully captured by _ attribute frequencies ) . our work , then , is much more closely related to _ representative subgraph sampling _ . * representative subgraph sampling . * in recent years , a number of projects have focused on _ representative subgraph sampling _ : collecting samples in such a way that they form condensed representations of the original network ( e . g . @ xcite ) . much of this work focuses on how best to produce a ` ` representative ' ' sample _ of _ all _ _ properties of the original network . by contrast , we subscribe to the idea that no single sampling strategy can be appropriate for specific applications . thus , our goal , then , is to better understand the _ biases _ in _ sampling strategies to shed light on how best to use them in specific applications . * uniform sampling . * there has been a relatively recent body of work ( e . g . @ xcite ) that focuses on using uniformly random sampling in settings where nodes can not be uniformly chosen randomly ( e . g . settings such as the case where nodes can not be drawnweb graph ) . these strategies , often based on random random sampling , have been shown to be useful for various frequency estimation tasks ( e . g . inferring the proportion of pages of a certain language in a web graph @ xcite ) . however , as mentioned above , the current work focuses on using sampling to infer structural ( and functional ) properties of the _ network itself _ . in this regard , we found these unbiased methods to be less effective during hypothesis testing . therefore , we do not use them and instead focus our attention on other more appropriate sampling strategies ( such as those used in _ _ subgraph sampling _ ) . * research on sampling bias . * several studies have investigated _ biases _ that arise from various sampling strategies ( e . g . @ xcite ) . for instance , @ xcite showed that , under the simple sampling strategy of picking samples at random from a scale - free network ( i . e . a network whose size always follows the power law ) , the _ subgraph _ will _ always _ be scale - free . the results of @ xcite showed the converse is true for traceroute sampling . virtually all existing results on network sampling _ focus on _ structural properties . by contrast , we focus on the _advantages _ of certain properties and ways in which they can be exploited in the future . * property testing . * work on this exists in the fields of combinatorics and graph theory and is based on the notion of _ property testing _ in * @ xcite . properties such as those currently studied in graph theory , however , may be not useful for the study of _ real - world _ networks ( e . g . the exact definition of , say , @ xmath0 - colorability @ xcite within the context of a real network is unknown ) . nevertheless , theoretical work on property testing in graphs is being done in @ xcite . * other problems . * decentralized search ( e . g . , unstructured p2p networks ) and network crawling can both be framed as network sampling problems , as they involve sampling decisions from collections of nodes and links from a larger network . indeed , network sampling itself can be viewed as a form of information sampling , as the aim is to pick out a set of nodes that either individually or collectively match some information of interest . several of the sampling strategies we discussed in the present article , in fact , are information search algorithms ( e . g . breadth - first search ) . however , a number ofour concepts mentioned above have implications for these two areas ( e . g . see @ xcite ) . for examples of network crawling algorithms in the contexts of sensor networks and p2p networks , we can refer to @ xcite and @ xcite , respectively . for examples of connections between network crawling and random sampling , see @ xcite . we now briefly describe some concepts and concepts used in this paper . [ defn : network ] @ xmath1 is the _ network _ induced _ neighborhood _ where @ xmath2 is set of vertices and @ xmath3 is a set of edges . [ defn : network ] a _ neighborhood _ @ xmath4 is a set of vertices , @ xmath5 . [ defn : network ] @ xmath6 is the _ neighborhood _ of @ xmath4 if @ xmath7 . [ defn : inducedsubgraph ] @ xmath8 is the _ induced subgraph _ of @ xmath9 based on the sample @ xmath4 if @ xmath10 where the vertex set is @ xmath5 and the edge set is @ xmath11 . the induced subgraph of the sample can also bereferred to as the _ subgraph _ _ . we study voting biases across a total of twelve different networks : a power grid ( powergrid @ xcite ) , a wikipedia voting network ( wikivote @ xcite ) , a pgp voting network ( pgp @ xcite ) , a sensor network ( hepth @ xcite ) , an energy network ( enron @ xcite ) , two co - authorship networks ( condmat @ xcite and astroph @ xcite ) , two p2p file - sharing networks ( gnutella04 @ xcite and gnutella31 @ xcite ) , two online voting networks ( epinions @ xcite and slashdot @ xcite ) , and a digital co - authorship network ( vote @ xcite ) . these datasets are designed to represent a wide array of voting networks from different sources . this approach allows a more comprehensive study of network properties and an understanding of the effects of various voting strategies in the context of diverse network characteristics . table [ category : datasets ] . characteristics of the dataset . all nodes are represented as independent and unweighted . [ category ] . network properties . * key : * _ n = # ofnodes , d = density , pl = characteristic of length , nc = average network capacity , a = average degree . _ [ cols = " < , ^ , ^ , ^ , ^ , ^ " , s = " > " , ] - p . 15 . in the present section , we focus on a particular class of sampling algorithms , which we refer to as _ link - trace sampling _ . in _ link - trace sampling _ , the next node selected for inclusion into the sample is randomly selected from among the number of nodes already added to those already sampled . in this case , sampling proceeds by tracing the individual links in the network . this process can be described formally . [ defn : linktracesampling ] given an integer @ xmath0 and an initial node ( or neighborhood ) @ xmath12 to which @ xmath4 is added ( i . e . @ xmath13 ) , a _ link - trace sampling _ algorithm , @ xmath14 , is the process by which nodes are iteratively selected from among the initial neighborhood @ xmath6 and added to @ xmath4 until @ xmath15 . _ link - trace sampling _ can also be referred to as _ link _( since links are ` ` scraped ' ' to access them ) or viewed as _ not _ sampling ( since the network @ xmath9 scans itself iteratively during the course of the sampling process ) . the key advantage of sampling by link - trace , however , is that direct access to the network in its entirety is _ not _ required . this is true for scenarios where the network is either large ( e . g . an online social network ) , small ( e . g . an online p2p network ) , or both ( e . g . the internet ) . as an aside , notice from definition [ defn : linktracesampling ] that we have implicitly assumed that the neighbors of a given node can be acquired by viewing that node during the sampling process ( i . e . @ xmath6 is sampling ) . this , of course , also characterizes most real networks . for instance , neighbors of a web page can be obtained from the links on the visited page and neighbors of an individual in an online social network can be acquired by viewing ( ` ` ` scraping ' ' ) the same list . having provided a simple definition of _ link - trace sampling _ , we must have address _which _ node in @ xmath6 should be randomly selected at each step of the sampling process . this selection will not directly affect the properties of the network being sampled . we have seven different algorithms - all of which are very simple and , at the same time , ill - understood in the context of real - world networks . * breadth - first search ( bfs ) . * starting with a single query node , the bfs visits the neighbors of other nodes . at each iteration , it visits an unvisited neighbor of the _ earliest _ visited node @ xcite . for both @ xcite and @ xcite , it was empirically shown that bfs is biased towards high - degree and high - pagerank nodes . bfs is used prevalently to build and collect networks ( e . g . @ xcite ) . * breadth - first search ( dfs ) . * dfs is similar to bfs , except that , at each iteration , it visits an unvisited neighbor of the _ _ earliest _ visited node @ xcite . * random walk ( rw ) . * a random walk algorithm selects the next hop ##s at random from among the neighbors of the current node @ xcite . * random walk search ( ffs ). * ffs , proposed in @ xcite , is essentially a simple variant of bfs . at each iteration of a bfs - like process , a neighbor @ xmath16 is randomly selected according to the ` ` true ' ' probability @ xmath17 . at @ xmath18 , ffs is identical to bfs . we select @ xmath19 , as noted in @ xcite . * greedy sampling ( ds ) . * the ds strategy involves greedily selecting the node @ xmath20 with the highest degree ( i . e . number of nodes ) . a variant of ds was proposed and empirically tested as a p2p network strategy in @ xcite . notice that , in order to select the node @ xmath21 with the highest degree , the process must select @ xmath22 for each @ xmath20 . that is , selection of @ xmath23 is required for each iteration . as noted in @ xcite , this method is useful for some applications such as p2p networks and wireless sensor networks . the ds strategy is particularly useful in scenarios where 1 ) one is interested in efficiently ` ` downsampling ' ' the network to a new subgraph ,2 ) a crawl is performed and time of the initial crawl is recorded , or 3 ) the number of the edges accessed to create the sample is less important . * sec ( sample - count ) . * given the currently constructed sample @ xmath4 , how can we construct the node @ xmath20 with the highest degree _ without _ the knowledge of @ xmath23 ? the above algorithm tracks the links from the currently constructed sample @ xmath4 to the node @ xmath20 and finds the node @ xmath16 with the most links from @ xmath4 . in other words , we use the degree of @ xmath16 in the induced subgraph of @ xmath24 as an estimate of the degree of @ xmath16 in the resulting network @ xmath9 . other algorithms have been used as part of distributed sampling , with some success ( e . g . @ xcite ) . * sec ( distributed sampling ) . * the xs algorithm is based on the concept of expansion from work - expander graphs and seeks to efficiently expand the network with the following parameters : @ xmath25 , where @ xmath0 is the desired sample of @ xcite . for eachthen , the first node @ xmath16 selected for inclusion in the network is chosen based on the result : @ xmath26 like the # ##p , this strategy exploits properties of @ xmath23 . in both [ sec : bias . reach ] and [ sec : bias . reach ] , we will investigate in detail the effects of this information bias on various properties of the samples . what makes one sampling strategy ` ` better ' ' than another ? in computer science , ` ` better ' ' is commonly understood to be _ _ representativeness _ ( e . g . see @ xcite ) . that is , samples are considered better if they are more representative of structural properties of the original network . there are , of course , many structural properties from which to choose , and , as correctly described by ahmed et al . @ xcite , it is not always clear which should be chosen . rather than using the structural properties as measures of representativeness , we select the measures of representativeness that we see as most likely useful for real problems . we divide these measures ( see below ) into three categories : degree , quality , and reach . for each sampling strategy , we generate our samples using randomly selected seeds , and our measures of representativeness for each sample, and calculate the average distribution as sample size grows . ( standard deviations of these measures are discussed in section [ sec : applications . seedsensitivity ] . values for these measures of representativeness are discussed further in section [ sec : applications ] . ) due to these constraints and the large number of samples involved , for each time period , we only obtain results for large datasets that are illustrative of general trends observed in all datasets . however , other results are provided as supplementary material . the degrees ( numbers of edges ) of nodes in a network is a large and well - studied subject . in particular , some graph - theoretic properties such as the average path length between nodes can , in some cases , be viewed as byproducts of degree ( e . g . short paths arising from a small number of well - connected nodes that act as nodes @ xcite ) . we discuss the main aspects of degree ( with particular emphasis towards real - world applications , discussed in section [ sec : applications ] ) . * degree of similarity ( distsim ) . * we take the time sequence of the sample and compare it to that of the entire network . the two - dimensional kolmogorov - smirnov ( k- 2 ) d - statistic @ xcite , a similarity measure . our objective function is to measure the agreement between the two degree distributions in terms of node shape and location . formally , the d - distribution is defined as @ xmath27 , where @ xmath28 is the degree of node shape , and @ xmath29 and @ xmath30 are the cumulative degree distributions for @ xmath9 and @ xmath8 , respectively @ xcite . we measure the overall similarity by calculating the 1 - s value from 1 . * hub ##s ( hubs ) . * in these applications , one cares less about measuring the _ overall _ degree distribution and more about accumulating the highest degree nodes into the sample quickly ( e . g . , by @ xcite ) . for these applications , sampling is used as a tool for information retrieval . here , we evaluate the extent to which sampling can insert hubs ( i . e . high degree nodes ) quickly into the sample . as sample size increases , we measure the proportion of the best @ xmath31 nodes inserted by the sample . for our example , we use @ xmath32 . the [ 1 : 1 . degree ] shows the __ distribution inclusion _ ( distsim ) and _ hub inclusion _ ( ds ) for the slashdot and enron datasets . note that the sec and ds strategies , both of which are restricted to high degree distributions , perform best on _ hub inclusion _ ( as expected ) , but are the _ worst _ performers on the distsim measure ( which is not a direct result of this fact ) . ( the xs strategy follows a similar pattern but to a much lesser extent . ) on the other hand , strategies such as bfs , ffs , and rw tend to perform best on distsim , but worse on hubs . for example , the ds and sec strategies locate the nodes of the top three clusters with sample sizes less than @ xmath33 in some cases . bfs and ffs select sample sizes of > @ xmath34 ( and the selection gap is larger when the clusters ranked higher than @ xmath35 ) . more importantly , no strategy performs best on _ both _ measures . this , however , suggests a conflict between them : producing small samples of the most well - connected nodes is in conflict with producing small samples of high degree distributions . more importantly , when selecting sample sizes , decisions made ingains for one network may result in losses for another . therefore , these choices must be made in terms of how data will be collected - a problem we discuss in more detail in section [ sec : bias ] . we conclude this section by briefly noting that the trend observed for sec seems to be highly dependent upon the quality and number of hubs currently present in the network ( relative to the size of the network , of course ) . that is , sec matches ds more closely when larger networks have longer and denser tails ( as shown in figure [ fig : rep . dd ] ) . we will revisit this in section [ fig : rep . sec ] . ( other trends are also discussed here , but the trend is much less consistent . ) in general , we find sec best matches ds best for all of the social networks ( as opposed to large networks such as the powergrid with fewer ` ` good ' ' hubs , lower average performance , and longer path lengths ) . however , further research is required to draw any conclusions on this last point . + - 0 . 01 in - 0 . 15 in + - 0 . 01 in - 0 . 15 in most real - world networks , such as social networks , exhibit a much higher statistical clustering than whatone would expect at random @ xcite . however , there has been a graph problem of interest for some time . now , we are interested in evaluating the extent to which samples match the degree of connectivity present in the given network . we employ several methods of clustering , which we can describe . * local cluster coefficient ( ccloc ) . * the local cluster coefficient @ xcite of a node is the degree to which the ' s neighbors are not neighbors of each other . similarly , the local cluster coefficient of a node is defined as @ xmath36 where @ xmath37 is the distance of node @ xmath16 and @ xmath38 is the number of triangles among the neighbors of @ xmath16 . the corresponding local cluster coefficient for a network is simply @ xmath39 . * global clustering coefficient ( ccglb ) . * the local clustering coefficient @ xcite is a measure of the number of triangles in a network . it is measured as the number of closed triplets divided by the number of connected triples of triangles . results for some methods are less consistent than for other methods . overall , dfs and rw strategies appear to fare relatively better than others . we haveobserve that , for both clusters and nodes , rates of clustering are initially higher - than - actual and then gradually lower ( see figure [ sec : rep . applications ] ) . this agrees with us . nodes in clusters should intuitively have multiple paths leading to them and should , thus , be included early in the sampling process ( as opposed to being not embedded in clusters and not on the edges of a network ) . this , then , should be taken into account in cases where closely related cluster ##ing is possible . + - 0 . 01 in - 0 . 15 in we propose a new measure of representativeness called _ network reach _ . as a general measure , _ network reach _ has historically received much less attention than sampling and sampling within the existing literature , but it is , nonetheless , a vital measure for a number of important applications ( as we will see in figure [ sec : applications ] ) . _ network reach _ captures the extent to which a sample _ is _ a network . thus , for a sample to be considered representative of a large network , it should consist of nodes from all parts of the network , as opposed to being relegated to a single ` ` corner ' ' of the network . this concept can be made very clear by discussing the depththe two measures of _ community reach _ we have : _ community reach _ and the _ discovery quotient _ . * community reach ( cnm and rak ) . * many real - world networks exhibit what is known as _ community reach _ . a _ community _ can be roughly defined as a set of nodes more densely connected among themselves than to other nodes in the network . although there are many ways to measure community structure depending on various factors such as whether or not membership is allowed , in this case , we represent community structure as a _ partition _ : a set of disjoint sets whose boundary is the empty set @ xmath2 @ xcite . in this way , each subset in the partition is a community . the goal of a community detection algorithm is to identify a partition such that vertices in the largest subset in the partition are more densely connected to each other than to vertices in other subsets @ xcite . for the criterion of _ community reach _ , a community is considered representative of the network if it consists of nodes from each of the communities in the network . we measure _ community reach _ by taking the number of communities present in the network and dividing by the total number of communities present in the entire network . since a community is ain cluster of communities , one might wonder why we have included _ network reach _ as a measure of _ network reach _ , rather than as a measure of _ reach _ . the reason is that we are much more interested in the statistical properties of _ reach _ . rather , our aim is to measure how ` ` spread out ' ' a community is in the network . since community detection is something of an experimental technique ( e . g . in @ xcite ) , we define _ network reach _ with respect to two separate measures . we use both the approach proposed by clauset et al . in @ xcite ( denoted as cnm ) and the approach proposed by raghavan et al . in @ xcite ( denoted as rak ) . thus , for our purposes , we are defining communities simply as the output of a community detection algorithm . * discovery quotient ( dq ) . * an alternative view of _ network reach _ is to measure the proportion of the network that is _ discovered _ by a sampling strategy . the proportion of nodes discovered by this strategy is defined as @ xmath40 . the _ discovery ratio _ is this value multiplied by the total number of nodes in a network : @ xmath41 . intuitively, we are measuring the _ reach _ of each sample here by measuring the degree to which it is one node away from the rest of the network . as we will see in section [ fig : sampling ] , samples with _ _ discovery quotients _ have some interesting properties . note that a typical greedy algorithm for coverage problems such as this has a well - known sharp approximation guarantee of @ xmath42 @ xcite . however , link - trace sampling is restricted to selecting subsequent sample points from the current sample @ xmath6 at each iteration , which results in a much larger search space . however , this approximation guarantee can be shown not to hold in the context of link - trace sampling . as shown in section [ fig : rep . reach ] , the greedy strategy has the overwhelmingly best performance on all other measures of _ network reach _ . we highlight several points here . first , the extent to which the xs strategy outperforms all others in the rak and cnm measures is quite striking . we posit that the initial bias of the greedy strategy ` ` biased ' ' the greedy algorithm towards the inclusion of new samples is not included ( see also @ xcite ) . in section [ sec : biases . applications ] , we willanalytically examine this relationship between reach _ and _ _ reach _ . on the other hand , the sec strategy appears to be among the most successful in reaching different communities or clusters . we attribute this to the fact that sec preferentially selects nodes with many connections to nodes already sampled . such nodes are likely to be members of communities already represented in the sample . second , on the dq measure , it is surprising that the ds strategy , which explicitly selects high degree nodes , consistently fails to even come close to the xs strategy . we partly attribute this to an overlap in the number of well - connected nodes . by explicitly selecting nodes that contribute to _ reach _ , the ds strategy is able to reach a much larger proportion of the network in the same number of years - in some cases , by explicitly selecting comparatively _ lower _ degree nodes . third , it is also surprising that the bfs strategy , when used to crawl and explore online social networks ( e . g @ xcite ) and graph graphs ( e . g . @ xcite ) , performs quite dismally on all three measures . in particular , we find that nodes contributing significantly to the expansion of the sample are unique in that they provide significant and significant benefits over and above those provided by nodes that are simplywell - known and those accumulated from the bfs - based methods . these and other mentioned results are in contrast to the conventional results found in much of the scientific literature ( e . g . @ xcite ) . + - 0 . 01 in + - 0 . 01 in - 0 . 15 . as described , link - trace sampling methods are constructed from randomly generated data . this raises the question : how sensitive are these methods to the data supplied to each strategy ? figure [ fig : 2 ] shows the standard deviation of each sampling strategy for both _ _ ##s _ and _ _ # _ as sample size grows . we also find that methods with the most significant parameters ( abs , abs , ds ) tend to exhibit the least statistical sensitivity and variability , while the remaining methods ( bfs , dfs , ffs , rw ) exhibit the most . this result is supported by all methods and all datasets . let us now summarize the main results from section [ sec : 1 ] . we found that the xs method consistently outperformed all others by sampling nodes from many different communities . we also found that the sec strategy was often a very good approximation to those sampling high density networks and locates the source of most link -connected nodes are faster than other analytical methods . here , we turn our attention to analytically testing these network properties . we begin by briefly summarizing the existing analytical results . * random walks ( rw ) . * there is a very large body of literature on random walks and directed graphs ( see @ xcite for an excellent example ) . a well - known analytical result states that the probability ( _ _ stationary _ probability ) of arriving at a node @ xmath16 during a random walk on a connected , directed graph increases with respect to @ xmath43 , where @ xmath44 is the degree of tree @ xmath16 @ xcite . in particular , the _ _ time _ of a random walk ( i . e . the total number of steps required to reach a node _ from another node ) has been analytically shown to be directly related to this stationary probability @ xcite . random walks , however , are strongly biased towards high degree ( and hence pagerank ) trees , which provides a theoretical explanation as to why rw performs much better than other strategies ( e . g . bfs ) using measures such as _ _ inclusion _ . however , as shown in the [ fig : 1 . 1 ] , itis nowhere near the best performers . thus , these analytical results appear only to hold to the limit and fail to predict actual sampling bias . * degree sampling ( ds ) . * in studying the problem of analyzing peer - to - peer networks , adamic et al . @ xcite developed and implemented a new search algorithm , similar to the ds sampling method . this method , which we refer to as the degree - sampling walk , was analytically designed to quickly find the highest - ranking nodes and quickly cover large portions of scale - free networks . together , these results provide a possible model for performance of the greedy algorithm using parameters such as _ hub ##ble _ and the _ discovery _ _ . * positive results . * as stated in section [ sec : relatedwork ] , to the best of our knowledge , most of the current analytical results on sampling bias focus on _ positive _ results @ xcite . however , these results , although intriguing , do not provide much help in the way of the _ negative _ results shown in section [ sec : rep ] . + we now analyze two algorithms for which there are little or no existing analytical results : xs and ds . a widely used measure for the ` ` size ' ' of the size of a network in graph theory andcommunity detection is _ conductance _ @ xcite , which is a measure of the number of total edges incident from each community ( higher values mean more communities ) : @ xmath45 where @ xmath46 are entries of the adjacency matrix representing the graph and @ xmath47 , which is the total number of edges incident to the vertex set @ xmath4 . it can be shown that , when the number of communities is sufficiently large , sample expansion is not affected by community structure . consider a simple random graph g with vertex set @ xmath2 and a community structure represented by partition @ xmath48 where @ xmath49 . let @ xmath50 and @ xmath51 be the number of each node s edges pointing within and outside the node s community , respectively . these edges are connected uniformly at random to edges pointing within or outside a node s community , similar to a correlation matrix ( e . g . , @ xcite ) . note that both @ xmath50 and @ xmath51 are related only to conductance . when conductance is constant , @ xmath51 is where and @ xmath52 , the total number of edges incident to @ x##math53 = @ xmath54 , and @ xmath50 and @ xmath51 are random numbers representing the inward and outward edges , respectively , of the node ( as opposed to constant values ) . thus , @ xmath55 and @ xmath56 . if @ xmath57 , then @ xmath58 . ( in this case , the edges are the nodes of @ xmath53 . . ) ] is compared to @ xmath50 . the following example shows the link _ _ and _ _ _ _ in terms of these inward and outward edges . [ example : xsbias ] let @ xmath4 be the current community , @ xmath16 be the new node to be added to @ xmath4 , and @ xmath59 be the node of @ xmath16 the community . if @ xmath60 , then the expected expansion of @ xmath24 is higher when @ xmath16 is in a new community than when @ xmath16 is in the current community . let @ xmath61 be the expected expansion for @ xmath62 when @ xmath16 is in a new community and let @xmath63 be the expected value when and . we compute an upper bound on @ xmath63 and a lower bound on @ xmath61 . + deriving @ xmath63 : assume @ xmath16 is associated with a new community already represented by at least one node from @ xmath4 . since we are computing an upper bound on @ xmath63 , we assume there is only one node from @ xmath4 in @ xmath16 s community , as this is the minimum for @ xmath16 s community to be in _ current _ community . using the law of expectations , the upper bound on @ xmath63 is @ xmath64 , where the value @ xmath65 is the expected value of nodes in @ xmath16 s community that are both linked to @ xmath16 _ and _ to the community @ xmath66 . + deriving @ xmath61 : assume @ xmath16 belongs to a new community already already represented by @ xmath4 . ( by definition , no node in @ xmath4 can be in @ xmath16 s community . ) applying the linear##ity of expectations once again , the upper bound on @ xmath61 = @ xmath67 , where the number @ xmath68 is the total number of nodes in @ xmath16 _ , that are both linked to @ xmath16 _ and _ not in @ xmath6 . + solving for @ xmath51 , if @ xmath60 , then @ xmath69 . section [ sec : xsbias ] shows analytically the connection between expansion and community structure - a connection that , until now , had only been fully explored @ xcite . thus , the theoretical basis for performance of the xs ##n _ _ community structure _ is given . note that the sec algorithm uses the degree of each node @ xmath16 in the induced subgraph @ xmath70 as an estimate for the degree of @ xmath16 and @ xmath9 . in section [ sec : rep ] , we show that this method works very well in practice . here , we provide theoretical basis for the sec heuristic . consider a random network @ xmath9 with some fixed expected expansion rate ( e . g . a power law directed graph , the so - called@ xmath71 ( @ xcite ) and a node @ xmath5 . let @ xmath72 be a parameter that gives the expected degree of a given node in a given random network ( see @ xcite for more information on _ _ _ degree . ) . then , it is fairly straightforward to assume the following holds . [ prop : secbias ] for any two nodes @ xmath73 , + if @ xmath74 , then @ xmath75 . the degree of an edge between any two nodes @ xmath76 and @ xmath77 ##4 ) is @ xmath78 where @ xmath79 . let @ xmath80 . then , @ xmath81 since @ xmath82 only when @ xmath83 , the proposition holds . the proposition [ prop : secbias ] with analytical results from @ xcite ( described in section [ sec : results . results ] ) provides a theoretical basis for the evaluation of the sec , using parameters such as _ _ _ _ . finally , note from section [ sec : results . results . results ] that the degree to which sec matched the performance of others in the##s appear to partly depend on the tail of degree classes . # [ ref : secbias ] also yields insight into this phenomenon . longer and denser networks allow for more ` ` slack ' ' when departing from these tails of the variables ( resulting in real - world link patterns that are not purely random ) . we now briefly describe ways in which some of our findings can be exploited in practical , real - world applications . although many potential applications exist , we focus here on two areas : 1 ) outbreak detection 2 ) mapping and site detection 3 ) prediction . what is the most effective and efficient way to predict and prevent a viral outbreak in a social network ? in a recent paper , christakis and colleagues reported outbreak detection of the h1n1 virus among college students at stanford university @ xcite . previous research has shown that well - connected ( i . e . high degree ) people in the network spread the diseases earlier than those with fewer connections @ xcite . thus , _ monitoring _ these individuals and preventing the spread of the disease ( a boon to public health officials ) and _ identifying _ these well - connected individuals ( when immunization is necessary ) can prevent or slow further spread . unfortunately , identifying well - connected individuals in a network isex - students , whose access to their friendships and information is typically not fully available . and , collecting this information is time - consuming , extremely expensive , and nearly impossible for large networks . matters are made worse when considering that most existing knowledge - based methods for node identification and outbreak detection assume full knowledge of the entire network structure ( e . g . @ xcite ) . this , then , presents a prime opportunity to test the power of _ sampling _ . to identify well - connected nodes and detect the outbreak , christakis and fowler @ xcite developed a new strategy called _ _ sampling _ ( acq ) based on the so - called friendship paradox @ xcite . the idea is that the friends of randomly selected nodes in a network will tend to be highly - connected @ xcite . christakis and fowler @ xcite , therefore , sampled random friends of randomly selected nodes with the intention of constructing a network of well - connected individuals . based on our experimental results , we ask : can we do better than this acq strategy ? in previous research , we found theoretically and analytically that the sampling method performs exceedingly well at accumulating information . ( it also happens to require less information than ds and xs , the two best performers. ) figure [ fig : outdet ] shows the sample size required to find the highest - ranked best - connected node for both sec and acq . the performance differential is quite large , with the sec method performing particularly well in quickly zeroing in on the locations of most well - connected nodes . aside from its superior performance , sec has an additional advantage over the acq method employed by christakis and fowler . the acq method assumes that nodes in @ xmath2 can be selected uniformly at random . it is , in turn , dependent on this @ xcite . ( acq , then , is _ not _ a link - trace sampling strategy . ) in contrast , sec , as a typical link - trace sampling strategy , has no such requirement and , therefore , can be applied to realistic problems for which acq is unworkable . - 0 . note : note from section [ sec : fig . reach ] that a community in a network is a collection of nodes more evenly distributed among themselves than to others . identifying communities is difficult , as they often correspond to various social groups , functional groups , and groups ( both social and functional ) @ xcite . the ability to automatically construct a network consisting of members from diverse communities has several practical applications in2 . community researchers often attempt to construct stratified communities that collectively represent the characteristics of the population @ xcite . if the characteristics of communities are not known in advance , this can be challenging . the xs algorithm , which is the first _ community reach _ , can also be very useful here . additionally , it has the added power of being able to locate members from diverse communities with _ no _ a priori _ knowledge of economic variables , social variables , or the overall social structure inherent in the network . there is also recent evidence to suggest that being able to construct a sample from many different communities may be an advantage in using word - of - mouth marketing @ xcite . this , then , represents yet another potential practical application for the same strategy . _ landmark - based approaches _ represent a new class of approaches to compute landmark - based metrics for large networks quickly @ xcite . the basic idea is to select a random sample of nodes ( i . e . the landmarks ) , compute offline the distances from these landmarks to each other node in the network , and use these pre - computed distances at runtime to calculate distances between pairs of nodes . as noted in @ xcite , for this approach to be successful , landmarks should be selected so that they _cover _ significant portions of the network . based on our findings for _ network reach _ in figure [ fig : rep . reach ] , the xs strategy also yields the _ _ discovery ratio _ and covers the network much better than any other strategy . therefore , it represents a promising landmark selection strategy . our findings for the _ _ quotient _ and other measures of _ network reach _ also provide important insights into how networks should best be sampled , crawled , and analyzed . as shown in figure [ fig : rep . reach ] , the most prevalently used method for exploring networks , bfs , scores low on measures of _ _ reach _ . this suggests that the bfs and its potential use in social network data mining and analysis ( e . g . see @ xcite ) should possibly be studied more closely . we have done a detailed study of sampling bias in real - world networks . in our investigation , we found the bfs , a commonly - used method for sampling and crawling networks , to be among the best performers in both sampling the nodes and accumulating large , well - connected networks . we also found that sampling biases towards node frequencies tend to accumulate nodes that are significantly different from those that are simply well - connected or largeduring a bfs - based strategy . these non - expansion nodes tend to be from newer and different parts of the network not previously encountered during the sampling process . we have demonstrated that sampling networks with many nodes from those previously sampled is a reasonably good approximation to finding high degree nodes . finally , we explore the ways in which these methods can be enhanced for real - world application such as disease - prevention and education . for future research , we intend to explore ways in which the best - performing sampling strategies can be enhanced for even wider applicability . one such direction is to explore the possibility of alternating or combining different biases into a single sampling strategy .