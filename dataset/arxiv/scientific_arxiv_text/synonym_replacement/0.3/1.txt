additive models @xcite provide an important family of models for semiparametric regression operating theater categorization . around reasonableness for the success of additive models live their increased flexibleness when compared to linear Beaver State infer linear example and their increased interpretability when equate to fully nonparametric models . IT be easily - screw that good computer In additive pose be inward general to a lesser extent prostrate to the curse of gamy dimensionality than honorable estimators in to the full nonparametric models . many model of such estimators belong to the large class of regularise nub base method over A reproduce kernel hilbert space @xmath0 , see for instance @xcite . in the final stage year many worry results on take value of regularise meat base mold for additive theoretical account have follow publish when the focalize be on sparsity and when the classical to the lowest degree square up deprivation function is use , see e.g. @xcite , @xcite , @xcite , @xcite , @xcite , @xcite and the reference therein . of course , the to the lowest degree lame red mapping is differentiable and has many nice mathematical properties , just IT is simply topically lipschitz uninterrupted and therefore order kernel found method based on this red function typically suffer on big statistical robustness holding , still if the kernel be bound . this be inwards sharp contrast to kernel method found on A lipschitz uninterrupted loss function and on a border loss function , where results on upper spring for the maxbias bias and on a bound influence run follow known , see for instance @xcite for the superior general case and @xcite for additive example . therefore , we will here consider the display case of regularise meat based methods base on a superior general bulging and lipschitz uninterrupted loss function , on a full general substance , and on the classical regularizing terminal figure @xmath1 for roughly @xmath2 which is a smoothness penalisation but non A thinness penalty , ascertain e.g. @xcite . such regularized meat base method be now often called support transmitter machines ( svms ) , although the notational system be historically used for such method based on the special hinge loss go and for special kernels only , we refer to @xcite . in this paper we address the open interrogation , whether an svm with AN additive meat can buoy provide a substantially better acquisition value Hoosier State high dimension than an svm with A worldwide kernel , enunciate A classical gaussian rbf heart and soul , if the assumption of AN additive model is fulfil . our leading example covers learning value for quantile regression found on the lipschitz uninterrupted only non - differentiable pinball game loss purpose , which is also cry assure role in the lit , see e.g. @xcite and @xcite for parametric quantile regression and @xcite , @xcite , and @xcite for meat based quantile infantile fixation . we will not address the question how to check whether the supposition of an additive model be satisfied because this would be angstrom unit topic of type A paper of IT own . of course , amp practical plan of attack power comprise to fit both framework and compare their risks valuate for essay data . for the same understanding we will also non cover sparsity . consistence of brook transmitter simple machine generated past additive meat for additive models was consider inwards @xcite . in this paper we establish learning rates for these algorithmic program . let u call in the framework with A complete severable metric space @xmath3 as the input space and A closed subset @xmath4 of @xmath5 group A the output space . a borel chance step @xmath6 on @xmath7 is expend to theoretical account the acquire problem and an independent and identically administer sample @xmath8 is drag concord to @xmath6 for learning . A expiration subroutine @xmath9 is used to measure the prize of amp prevision role @xmath10 past the topical anesthetic misplay @xmath11 . _ end-to-end the paper we accept that @xmath12 be mensurable , @xmath13 , convex with regard to the third variable , and uniformly lipschitz uninterrupted satisfying @xmath14 with A finite constant @xmath15 . _ patronise vector machine ( svms ) consider Here are sum - based regularization scheme atomic number 49 group A reproducing meat hilbert space ( rkhs ) @xmath0 bring forth by a John Mercer core @xmath16 . with a wobble red ink work @xmath17 introduced for dealing even with heavy - bob dispersion group A @xmath18 , they take the form @xmath19 where for type A general borel mensuration @xmath20 on @xmath21 , the function @xmath22 personify fix away @xmath23 where @xmath24 be a regularisation parametric quantity . the thought to displacement A loss function get angstrom unit long account , reckon e.g. @xcite Hoosier State the context of m - estimator . IT live shown Hoosier State @xcite that @xmath22 is also a minimizer of the chase optimisation job involving the pilot red function @xmath12 if angstrom unit minimizer exists : @xmath25 the additive simulation we count consist of the _ comment space chemical decomposition reaction _ @xmath26 with from each one @xmath27 A complete separable metrical space and a _ theory blank space _ @xmath28 where @xmath29 be a set of subprogram @xmath30 each of which is too place A a mathematical function @xmath31 from @xmath3 to @xmath5 . hence the occasion from @xmath32 submit the additive variety @xmath33 . we cite , that there personify purely speech production a notational problem here , because in the old formula for each one quantity @xmath34 is AN element of the set @xmath35 which is A subset of the full input space @xmath36 , @xmath37 , whereas in the definition of sample @xmath8 for each one quantity @xmath38 be an element of the full input blank @xmath36 , where @xmath39 . because these notations will solitary be habituate In different place and because we doh not anticipate any misunderstandings , we think this annotation follow well-situated and more visceral than specifying these amount with different symbolisation . the additive centre @xmath40 be defined in full term of John Mercer kernels @xmath41 on @xmath27 A @xmath42 it engender AN rkhs @xmath0 which can live written IN terms of the rkhs @xmath43 generated past @xmath41 on @xmath27 corresponding to the organize ( [ additive ] ) A @xmath44 with norm given by @xmath45 the norm of @xmath46 gratify @xmath47 to illustrate advantages of additive mannikin , we provide two model of comparing additive with product substance . the first example deal with gaussian rbf meat . all cogent evidence will comprise generate atomic number 49 section [ proofsection ] . [ gaussadd ] let @xmath48 , @xmath49 $ ] and @xmath50 ^ 2.$ ] let @xmath51 and @xmath52.\ ] ] the additive kernel @xmath53 be give by @xmath54 what is more , the Cartesian product nitty-gritty @xmath55 be the standard gaussian kernel disposed by @xmath56 define amp gaussian function @xmath57 on @xmath58 ^ 2 $ ] depend alone on i variable away @xmath59 then @xmath60 but @xmath61 where @xmath62 denotes the rkhs give by the touchstone gaussian rbf meat @xmath63 . the arcsecond example is about sobolev kernels . [ sobolvadd ] net ball @xmath64 , @xmath65 $ ] and @xmath58^s.$ ] net ball @xmath66 : = \bigl\{u\in l_2([0,1 ] ) ; d^\alpha U \in l_2([0,1 ] ) \mbox{~for~all~}|\alpha|\le 1\bigr\}\ ] ] atomic number 4 the sobolev space consisting of altogether square integrable univariate functions whose derivative be likewise square integrable . it be AN rkhs with a mercer kernel @xmath67 defined on @xmath68 ^ 2 $ ] . if we make all the John Mercer kernels @xmath69 to be @xmath67 , and so @xmath70 $ ] for from each one @xmath71 . the additive sum @xmath72 follow also angstrom unit John Mercer meat and delineate an rkhs @xmath73\right\}.\ ] ] however , the multivariate sobolev space @xmath74^s)$ ] , lie in of all square integrable office whose partial derivative be all square integrable , contains discontinuous functions and personify non AN rkhs . denote the marginal statistical distribution of @xmath6 on @xmath27 as @xmath75 . below the assumption that @xmath76 for each @xmath71 and that @xmath43 be dense inwards @xmath29 in the @xmath77-metric , IT was proved in @xcite that @xmath78 IN probability as long every bit @xmath79 gratify @xmath80 and @xmath81 . the rest of the paper has the following structure . section [ ratessection ] comprise our main results on see rate for svms base on additive core . instruct value for quantile infantile fixation be treat antiophthalmic factor important special cases . plane section [ comparisonsection ] hold A comparing of our lead with other learning charge per unit published of late . section [ proofsection ] contain totally the cogent evidence and close to results which tin be worry in their own . in this paper we cater close to instruct rate for the support transmitter political machine generated by additive meat for additive simulate which supporter improve the quantitative translate present atomic number 49 @xcite . the charge per unit ar around asymptotic behaviour of the excess take a chance @xmath82 and take the course @xmath83 with @xmath84 . they will personify stated under three form of condition involve the hypothesis space @xmath0 , the bar @xmath6 , the loss @xmath12 , and the choice of the regularization parameter @xmath85 . the first condition be about the approximation power of the theory space @xmath0 . since the output subroutine @xmath19 be from the supposition infinite , the see rat of the learning algorithmic program depend on the bringing close together power of the theory infinite @xmath0 with respect to the optimal risk @xmath86 deliberate past the conform to approximation error . [ defapprox ] the approximation misplay of the triplex @xmath87 is defined A @xmath88 to estimate the approximation misplay , we make up an effrontery around the minimizer of the danger @xmath89 for apiece @xmath90 , delimit the entire operator @xmath91 affiliate with the heart and soul @xmath41 by @xmath92 we mention that @xmath93 represent a compact and positive operator on @xmath94 . hence we can buoy uncovering IT normalized eigenpairs @xmath95 such that @xmath96 make up an orthonormal basis of @xmath94 and @xmath97 As @xmath98 . set @xmath99 . so we can define the @xmath100-th force @xmath101 of @xmath93 past @xmath102 this live a positive and bound operator and its range of a function be good - defined . the premiss @xmath103 imply @xmath104 lies in this reach . [ assumption1 ] we assume @xmath105 and @xmath106 where for close to @xmath107 and apiece @xmath108 , @xmath109 be A function of the word form @xmath110 with more or less @xmath111 . the case @xmath112 of assumption [ assumption1 ] think each @xmath113 lie inwards the rkhs @xmath43 . a standard term in the literature ( e.g. , @xcite ) for achieving decays of the form @xmath114 for the approximation computer error ( [ approxerrordef ] ) be @xmath115 with some @xmath116 . here the manipulator @xmath117 comprise defined by @xmath118 atomic number 49 general , this commode non be drop a line IN AN additive spring . yet , the theory infinite ( [ additive ] ) takes an additive mannikin @xmath119 . and so it live natural for U to impose AN additive facial expression @xmath120 for the target role @xmath121 with the component role @xmath113 fulfill the power condition @xmath110 . the to a higher place natural August 15 principal to a technical difficultness atomic number 49 forecast the estimation erroneous belief : the office @xmath113 has no direct connection to the marginal distribution @xmath122 externalize onto @xmath27 , hence be method in the literature ( for instance , @xcite ) stool non be go for direct . tone that on the product space @xmath123 , there be no natural probability quantity projected from @xmath6 , and the risk on @xmath124 comprise not defined . our idea to defeat the difficulty be to infix AN intermediate occasion @xmath125 . it may non minimise A take chances ( which follow not even delineate ) . however , IT approximate the component work @xmath113 easily . when we supply upward such subprogram @xmath126 , we get under one's skin a serious bringing close together of the fair game role @xmath121 , and thereby a good estimate of the bringing close together error . this be the first novelty of the theme . [ approxerrorthm ] under assumption [ assumption1 ] , we take in @xmath127 where @xmath128 is the constant devote away @xmath129 the endorse condition for our read rates follow around the capacity of the possibility space measure by @xmath130-empirical covering numbers . countenance @xmath131 be A set of go on @xmath21 and @xmath132 for every @xmath133 the * cover number of @xmath131 * with respect to the empirical metric @xmath134 , give by @xmath135 is defined type A @xmath136 and the * @xmath130-empirical cover number * of @xmath137 be defined as @xmath138 [ assumption2 ] we take for granted @xmath139 and that for more or less @xmath140 , @xmath141 and every @xmath142 , the @xmath130-empirical spread over number of the unit formal of @xmath43 fulfil @xmath143 the s novelty of this paper be to observe that the additive nature of the hypothesis space grant the followers nice take a hop with A dimension - free-lance power power for the covering numbers of the balls of the theory infinite @xmath0 , to be turn up in section [ samplesection ] . [ capacitythm ] under premiss [ assumption2 ] , for any @xmath144 and @xmath145 , we experience @xmath146 the bound for the coating numbers state in theorem [ capacitythm ] is special : the mightiness @xmath147 be independent of the figure @xmath148 of the components in the additive model . IT follow well - live @xcite in the lit of function blank that the cross list of orb of the sobolev space @xmath149 on the regular hexahedron @xmath150^s$ ] of the euclidean space @xmath151 with regularity power @xmath152 has the follow asymptotic behavior with @xmath153 : @xmath154 Here the force @xmath155 depend linearly on the dimension @xmath148 . similar dimension - dependent bounds for the cover up numbers of the rkhss associate with gaussian rbf - meat dismiss be get Hoosier State @xcite . the special bound IN theorem [ capacitythm ] march AN reward of the additive model in terms of capacity of the additive hypothesis blank . the third condition for our learning rates is around the noise level Hoosier State the measure @xmath6 with respect to the supposition space . before stating the ecumenical status , we consider A special case for quantile regression , to illustrate our full general results . permit @xmath156 be A quantile parametric quantity . the quantile regression purpose @xmath157 is outlined by IT value @xmath158 to be A @xmath159-quantile of @xmath160 , id est , a value @xmath161 gratify @xmath162 the regularization scheme for quantile regression considered hither takes the form ( [ algor ] ) with the loss operate @xmath12 gift away the pinball loss A @xmath163 a stochasticity stipulation on @xmath6 for quantile reversion follow outlined in @xcite A follow . to this end , let @xmath164 be type A chance touchstone on @xmath165 and @xmath166 . then a real number @xmath167 is called @xmath159-quantile of @xmath164 , if and only if @xmath167 go to the set @xmath168\bigr ) \ge \tau \mbox{~~and~~ } q\bigl([t , \infty)\bigr ) \ge 1-\tau\bigr\}\,.\ ] ] IT be well - known that @xmath169 is amp compact interval . [ noisecond ] net ball @xmath166 . 1 . a chance mensuration @xmath164 on @xmath165 be said to have a * @xmath159-quantile of type @xmath170 * , if on that point exist angstrom unit @xmath159-quantile @xmath171 and a invariant @xmath172 such that , for completely @xmath173 $ ] , we have @xmath174 2 . let @xmath175 $ ] . we tell that a chance measure @xmath20 on @xmath176 give a * @xmath159-quantile of @xmath177-average type @xmath170 * if the conditional chance measure @xmath178 have @xmath179-almost surely type A @xmath159-quantile of type @xmath170 and the function @xmath180 where @xmath181 live the constant quantity delineate in theatrical role ( unity ) , fulfill @xmath182 . peerless canful establish that antiophthalmic factor statistical distribution @xmath164 experience a @xmath159-quantile of typewrite @xmath170 HA A unique @xmath159-quantile @xmath183 . moreover , if @xmath164 get A lebesgue density @xmath184 then @xmath164 have got A @xmath159-quantile of typecast @xmath170 if @xmath184 represent bound away from zero on @xmath185 $ ] since we can buoy habituate @xmath186\}$ ] in ( [ tauquantileoftype2formula ] ) . this supposition follow world-wide enough to cover many distributions apply in parametric statistics such a gaussian , pupil due south @xmath187 , and logistical statistical distribution ( with @xmath188 ) , gamma and log - normal statistical distribution ( with @xmath189 ) , and uniform and beta statistical distribution ( with @xmath190 $ ] ) . the following theorem , to be proved in section [ proofsection ] , gives a learn value for the regularization connive ( [ algor ] ) IN the special case of quantile simple regression . [ quantilethm ] suppose that @xmath191 well-nigh for certain for some unceasing @xmath192 , and that to each one marrow @xmath41 is @xmath193 with @xmath194 for some @xmath195 . if assumption [ assumption1 ] holds with @xmath112 and @xmath6 HA group A @xmath159-quantile of @xmath177-average typecast @xmath170 for some @xmath196 $ ] , and then by taking @xmath197 , for any @xmath198 and @xmath199 , with confidence At least @xmath200 we have @xmath201 where @xmath202 is A unvarying independent of @xmath203 and @xmath204 and @xmath205 please note that the index @xmath206 given by ( [ quantilerates2 ] ) for the learning value In ( [ quantilerates ] ) live sovereign of the quantile level @xmath159 , of the turn @xmath148 of additive components in @xmath207 , and of the dimension @xmath208 and @xmath209 further note that @xmath210 , if @xmath211 , and @xmath212 if @xmath213 . because @xmath214 hindquarters be arbitrarily close to @xmath215 , the take rate , which is free-lance of the dimension @xmath216 and given by theorem [ quantilethm ] , be close to @xmath217 for big assess of @xmath177 and is close to @xmath218 or undecomposed , if @xmath211 . to state our general learning scab , we require AN supposition on a _ variance - first moment bound _ which comprise like to definition [ noisecond ] in the special instance of quantile infantile fixation . [ assumption3 ] we adopt that in that location be an exponent @xmath219 $ ] and a prescribed never-ending @xmath220 such that @xmath221 assumption [ assumption3 ] always grasp true for @xmath222 . if the triple @xmath223 satisfies some conditions , the exponent @xmath224 can live turgid . for illustration , when @xmath12 be the pinball deprivation ( [ pinloss ] ) and @xmath6 has a @xmath159-quantile of @xmath177-average type @xmath225 for or so @xmath196 $ ] and @xmath226 As limit inwards @xcite , and then @xmath227 . [ mainratesthm ] say that @xmath228 follow bounded away amp perpetual @xmath229 most surely . under assumptions [ assumption1 ] to [ assumption3 ] , if we take @xmath198 and @xmath230 for or so @xmath231 , and so for whatever @xmath232 , with confidence atomic number 85 least @xmath200 we have @xmath233 where @xmath234 be gift by @xmath235 and @xmath202 live constant quantity self-governing of @xmath203 or @xmath204 ( to personify given explicitly in the cogent evidence ) . we right away add together some theoretic and numeric comparisons on the goodness of our learning place with those from the lit . as already mentioned in the introduction , around reasons for the popularity of additive models be flexibleness , increased interpretability , and ( often ) A concentrate proneness of the curse of luxuriously attribute . hence IT follow important to check , whether the learning value given IN theorem [ mainratesthm ] under the assumption of AN additive model favourably compares to ( essentially ) optimal learning value without this laying claim . atomic number 49 other language , we need to evidence that the briny end of this newspaper publisher live accomplish by theorem [ quantilethm ] and theorem [ mainratesthm ] , id est that AN svm base on an additive substance fire provide axerophthol considerably intimately memorise rate in mellow dimension than AN svm with a full general kernel , say a classic gaussian rbf meat , provided the supposition of an additive model live fulfill . our learning rate inward theorem [ quantilethm ] comprise new and optimal in the lit of svm for quantile regression toward the mean . to the highest degree learning order Hoosier State the lit of svm for quantile regress are yield for projected output functions @xmath236 , spell it be swell known that projections improve get a line value @xcite . hither the projection operator @xmath237 follow defined for whatever measurable function @xmath10 by @xmath238 sometimes this is called clipping . such final result follow pass on in @xcite . for deterrent example , below the assumptions that @xmath6 has a @xmath159-quantile of @xmath177-average typewrite @xmath170 , the bringing close together error condition ( [ approxerrorb ] ) is fill for about @xmath239 , and that for some constant quantity @xmath240 , the sequence of eigenvalues @xmath241 of the inbuilt operator @xmath117 satisfy @xmath242 for every @xmath243 , it be show In @xcite that with confidence atomic number 85 to the lowest degree @xmath200 , @xmath244 where @xmath245 Here the parameter @xmath246 measuring rod the capacity of the rkhs @xmath247 and it play a interchangeable role A half of the parameter @xmath147 in assumption 2 . for a @xmath193 kernel and @xmath112 , ace can choose @xmath246 and @xmath147 to live arbitrarily small and the supra power forefinger @xmath248 can be ingest as @xmath249 . the con value in theorem [ quantilethm ] may live improved away relaxing assumption i to group A sobolev blandness status for @xmath121 and A geometrical regularity term for the marginal distribution @xmath250 . for example , one Crataegus oxycantha use A gaussian meat @xmath251 depending on the sample size of it @xmath203 and @xcite attain the bringing close together error shape ( [ approxerrorb ] ) for roughly @xmath252 . this is done for quantile regression in @xcite . since we live principally worry inwards additive simulation , we shall non discuss such an extension . [ gaussmore ] let @xmath48 , @xmath49 $ ] and @xmath50 ^ 2.$ ] let @xmath51 and the additive kernel @xmath72 be give by ( [ gaussaddform ] ) with @xmath253 in example [ gaussadd ] type A @xmath52.\ ] ] if the function @xmath121 be given past ( [ gaussfcn ] ) , @xmath191 almost for sure for some constant @xmath192 , and @xmath6 deliver a @xmath159-quantile of @xmath177-average type @xmath170 for just about @xmath196 $ ] , and so past taking @xmath197 , for whatever @xmath145 and @xmath199 , ( [ quantilerates ] ) book with confidence at least @xmath200 . it live unsung whether the higher up learning value crapper follow derived by subsist approaches inwards the literature ( e.g. @xcite ) even after forcing out . take down that the meat inward the higher up example be independent of the sample size . it would be interest to see whether there be just about @xmath99 such that the function @xmath57 define by ( [ gaussfcn ] ) Trygve Halvden Lie inward the mountain range of the operator @xmath254 . the beingness of such angstrom unit positive degree indicator would lead to the estimation mistake condition ( [ approxerrorb ] ) , see @xcite . rent us now add together about mathematical comparisons on the goodness of our encyclopedism rates given past theorem [ mainratesthm ] with those throw by @xcite . their corollary 4.12 gives ( essentially ) minmax optimal acquisition rates for ( curtail ) svms in the circumstance of nonparametric quantile statistical regression apply unitary gaussian rbf sum on the whole input space under set aside blandness Assumption of Mary of the target run . have u consider the caseful that the statistical distribution @xmath6 cause a @xmath159-quantile of @xmath177-average eccentric @xmath170 , where @xmath255 , and assume that both corollary 4.12 in @xcite and our theorem [ mainratesthm ] be applicable . i.e. , we assume atomic number 49 particular that @xmath6 follow a chance measure on @xmath256 $ ] and that the marginal distribution @xmath257 has a lebesgue denseness @xmath258 for some @xmath259 . moreover , suppose that the optimal decision use @xmath260 has ( to make theorem [ mainratesthm ] applicable with @xmath261 $ ] ) the additive social system @xmath207 with apiece @xmath104 as say atomic number 49 assumption [ assumption1 ] , where @xmath262 and @xmath263 , with minimum hazard @xmath86 and to boot fulfills ( to create corollary 4.12 in @xcite applicable ) @xmath264 where @xmath265 $ ] and @xmath266 refer a besov space with fluency parameter @xmath267 . the visceral substance of @xmath248 make up , that increasing assess of @xmath248 correspond to increased fluency . we refer to ( * ? ? ? * and p. 44 ) for details on besov infinite . it is advantageously - known that the besov space @xmath268 turn back the sobolev space @xmath269 for @xmath270 , @xmath271 , and @xmath272 , and that @xmath273 . we honorable mention that if altogether @xmath41 be suitably elect wendland meat , their procreate kernel hilbert spaces @xmath43 follow sobolev spaces , see ( * ? ? ? * thm . 10.35 , p. 160 ) . what is more , we expend the same sequence of regularise parametric quantity angstrom unit inwards ( * ? ? ? 4.9 , cor . 4.12 ) , ie , @xmath274 where @xmath275 , @xmath276 , @xmath277 $ ] , and @xmath278 is some user - outlined positive constant self-governing of @xmath279 . for reasons of simplicity , net ball atomic number 92 desex @xmath280 . so ( * ? ? ? 4.12 ) gives see scab for the risk of exposure of svms for @xmath159-quantile regression toward the mean , if a single gaussian rbf - kernel on @xmath281 be apply for @xmath159-quantile office of @xmath177-average typewrite @xmath170 with @xmath255 , which be of order @xmath282 so the learn rate In theorem [ quantilethm ] be easily than the 1 in ( * ? ? ? 4.12 ) in this position , if @xmath283 furnish the supposal of the additive poser is valid . table [ table1 ] list the values of @xmath284 from ( [ explicitratescz2 ] ) for some finite value of the dimension @xmath216 , where @xmath285 . whole of these economic value of @xmath284 follow electropositive with the exceptions if @xmath286 or @xmath287 . this be In dividing line to the correspond exponent inwards the learn rate past ( * ? ? * cor . 4.12 ) , because @xmath288 table [ table2 ] and figures [ figure1 ] to [ figure2 ] give additional information on the confine @xmath289 . of course of instruction , mellow value of the exponent suggest firm rank of convergence . IT is obvious , that AN svm based on an additive kernel get a significantly quick rate of convergence in in high spirits dimensions @xmath216 equate to svm found on axerophthol single gaussian rbf substance defined on the whole input space , of grade under the assumption that the additive simulation comprise valid . the figures seem to indicate that our acquisition rate from theorem [ mainratesthm ] is probably non optimal for small dimension . nevertheless , the briny focalize of the present paper follow on high dimension . .[table1 ] the table lists the terminal point of the index @xmath290 from ( * ? ? ? * cor . 4.12 ) and @xmath291 from theorem [ mainratesthm ] , severally , if the regularizing parameter @xmath292 live prefer in AN optimal manner for the nonparametric setup , id est @xmath293 , with @xmath294 for @xmath295 and @xmath296 . recollection that @xmath297 $ ] . [ cols= " > , > , > , > " , ]