It is well known that good estimators in additive models are generally less prone to the curse of high dimensions than good estimators in fully nonparametric models. For example, many examples of such estimators come from the large class of regularized kernel-based methods over a reproducing kernel hilbert space — xmath — xcite. Many interesting results have been published recently on learning rates of regularized kernel-based methods for additive models, when the focus is on sparseness and the classical least squares loss function is employed — xcite , xcite , xcite, xcite, and the references therein. These are in sharp contrast to kernel-based methods based on a convex and lipschitz continuous loss function, on a general kernel, and on the classical regularizing term xmath1 for some xmath2 smoothness penalty, but not a sparsity penalty, see xcite . . . in the last few years, many interesting results on learning of kernel-based methods for additive models have been published when the focus is on sparsity, and when the classical least squares loss function is used, see e . . . , e . . . , e . . In our approach to the problem of verifying whether the assumption of an additive model is satisfied, this is the subject of a separate paper, and we will not discuss sparsity. we will consider the possibility of combining both models, and then comparing their risks against the test data. In this paper, we are concerned with verifying the consistency of support vector machines generated by additive kernels for additive models. We are interested in supporting vector machines (svms) generated by additive kernels in a reproducing kernel hilbert space (rkhs) generated by a mercer kernel xmath16 . a dimension (the basis for the representation) is a borel probability measure of xmath7, and an independent and identically distributed sample, xmath8, is drawn according to xmath7. Our framework, which is of complete separation, is called the input space, and a closed subset, xmath4 of xmath5, is used for the output space . a density measure, xmath7, a square root, @ xmath7, is a measure of learning. Xmath7 is the measure of the learning problem, and a test set is drawn according to xmath7 according to xmath7 . . . This is precisely because the simplest notations are only employed in different places, and because there are no misunderstandings, it is thought that this notation is more correct and more easily understood than the use of different symbols. the additive model is considered as the decomposition of the input space into an element of the set, which is a subset of the full input space, xmath36. Where xmath36 is a matrix of functions, each of which is also identified as a mapping from xmath3 to xmath5 , so that the functions from xmath32 are called the additives. a minimizer of the following optimization problem, a loss function in a loss function. the second example deals with gaussian kernels. , if the minimizer exists: a minimizer of the following optimization problem involving the original loss function : if the minimizer exists, then a minimizer is presented. A gaussian function, in Xmath57, can be obtained, depending on a single variable, by Xmath59, then @ Xmath61; where xmath62 denotes the rkhs generated by the standard gaussian kernel @ Xmath63. The second example is about sobolev kernels. ____ ___  u  in l_ 2 ([0, 1]) ; d   alpha u  in l_ 2 ([0, 1]) d   mbox   for  all    |  alpha  le 1  s   a    [1] ___  , if _____________ u  in l_ 2 [[2], ], d      a rkhs with a mercer kernel at ______, and _____ rkhs in ___   __ 2 [2], and ____________, - for ___  alpha u  in l__ 2 [2],  mbox   for all        le  - bigr    __" - and that is the most important special case. In this paper we provide a learning rate for the support vector machines of the additive kernels for additive models, which helps improve the quantitative understanding in the xcite. It will be described in three kinds of conditions, the hypothesis space xmath0, the measure xmath6 and the loss xmath12, and the choice of the regularization parameter xmath85. - the approximation error of the triple-xmath87 is defined as xmath89, the first is that the approximation error of the triple-xmath89 is defined as xmath90, and we will say that xmath93 is a compact and positive operator of xmath94, and that xmath96 is an orthonormal basis of xmath94, and xmath97 is an orthonormal basis of xmath98. - There are important special cases of xmath83. At the same time, the hypothesis space, [addition], takes an additive form at xmath119. Thus it is natural for us to impose an additive expression at xmath120 on the target function, Xmath121, with the component functions at xmath113 satisfying the power condition at xmath119. There is a standard condition in literature (e.g., @ xcite) for the decay of the form Xmath114 for the error (against an approximation error) is @ xmath115 with some xmath116. as a result, the operator @ xmath117 is defined by @ xmath117 general, and it cannot be written in an additive form. however, the hypothesis space (e.g., a factor) takes on an additive form @ xmath119. so it is natural to impose an additive expression @ xmath120 on the target function @ xmath121 with the component functions @ xmath113 satisfying the power condition @ xmath113. Note that on the product space @ xmath123 there is no natural probability measure projected from @ xmath6, and the risk on @ xmath124 is not defined. Xcite, it is well known in the literature of function spaces that the covering numbers of balls of the sobolev space of xmath149 on the cube @ xmath150 s$3 of the euclidean space of xmath151 with the regularity index @ xmath151 have the following asymptotic behaviour: at xmath154 the power of xmath155 is linearly proportional to the size of xmath148 , a similar dimension-dependent bound in the covering numbers of gaussian kernels is found in xcite , the special bound in the book  Xcite , in the literature  Xcite it is well known that the covering numbers of balls of the sobolev space, on the cube xmath150 sd$ of the euclidean space xmath149, with regularity index xmath152, has the following asymptotic behavior with xmath154 : here the power xmath155 depends linearly on the dimension xmath148 . The second novelty of this paper is that the additive nature of the hypothesis space produces the following nice conjecture with an atom-independent power exponent for the balls of the hypothesis space xmath0, which is shown in section . . . - Let xmath164 be a probability measure on xmath165 and xmath166 . - let xmath164 be a probability measure on xmath165 and xmath166 , then a real number - xmath164 is called - xmath166 - - and only if xmath166 belongs to the group - xmath168 - - we say that a probability measure on xmath165 is called - - a quantile of xmath161 , that is, a quantile of xmath161 satisfying xmath161 , we know that - xmath6 - is a small interval. - xmath16 - a qualitative parameter on xmath164 - xmath167 , - le - tau - tau - le - le - le - tu - tau - le - le - le - . . . . if there is a lebesgue density of xmath184, then xmath184 has a unique xmath159 - quantile of xmath185 if the point of xmath184 is not zero at xmath185 , then Xmath184 , if we have [$ xmath195] the tauquantile of type 2 , which is a function of xmath179 - xmath179 - if the xmath198 - xmath199 - then for any xmath198 - xmath199 - with at least confidence, at least, that a distribution xmath164 - xmath179 has a unique xmath179 - xmath179 - if xmath185 is bounded from zero by xmath185 , if xmath184 is bounded away from zero by xmath185 , then xmath186 -                         - if xmath179 has a density of xmath184 , then xmath184 has a density of xmath184, then xmath186 has a value of xmath177 - average - if xmath199 is bounded to zero by xmath198 and xmath199, then sent> of the number @xmath148 of additive components in xmath207, and of the dimensions @xmath208 and @xmath209 , note that @xmath210, if @xmath211, and @xmath212 if @xmath213 . . . because, for instance, @xmath12 is a set of loss-weighted probabilitys ([xquotes]) and @ xmath6 has a ratio of xmath159, or “average” type, @ xmath225 for some @xmath197, for example if @xmath227 is a subset of “xcites” (vs. a component of “xcites”), then @xmath227 is a subset of “xquotes” . . . to determine general learning rates we need an assumption on a variance-based expectation that is similar to definition [ noiser] in special cases of quantile regression . “ Suppose that @xmath228 is bounded by a constant of xmath228. Let us assume that @xmath228 is bound by a constant @ xmath228 almost surely. In this chapter we shall see how well we are able to achieve our main goal by the theory of the largest proportions of the mathematical progress, that is, that a svm based on an additive kernel can provide a substantially better learning rate in high dimensions than a svm based on a general kernel, namely a classical gaussian kernel, provided that the assumption of an additive kernel is satisfied. and a numerical comparison of the goodness of our learning rate with that in the literature. As we have already mentioned in the introduction, some reasons for the popularity of additive models are their flexibility, their interpretability, and (in many cases) their reduced tendency to the curse of high dimensions. In other words, we must test whether the learning rate given by the equation [Mathematical syllable] in the case of an additive kernel favours (essentially) optimal learning rates without the assumption of an additive one. For example, suppose that in xmath6 the function xmath1 has a quantile of xmath158, a quantile of xmath177, a average of xmath177, a term of average type, a number of xmath177; that the application of the approximation error condition (Application b) is satisfied for some xmath239, and that for some constants xmath240, the sequence of eigenvalues xmath241 of the integral operator xmath117 satisfies xmath241 for every xmath243; that is the expression of xmath243. This expression is often called clipping. xmath121. If the function xmath121 is given by (gaussfcn), the function xmath196 almost surely for some constant xmath192. And the function xmath6 has a p of p of p 166 - quantile of p 177. p 198, with the matrix of p 197 (for any p 195 and p 199), one can have confidence in p200. ; we are concerned mainly with additive and advection models, so we will not discuss such an extension. [237] let p xmath121, p=18 and the matrix of p=193 be given by p-[26] as p243, in the example of p242 . The above learning rate can be improved by restraining p-[26] to a sobolev smoothness for p121, and to a regularity for the marginal distribution p-118 . Note that the kernel in the above example is independent of the matrix. And suppose that the optimal decision function Xmath260 has (to make theorem 'greater than 'inner') the additive structure Xmath207 with each 'xmath104 as stated in assumption '1', where 'xmath262' and 'xmath263', with little risk 'xmath86', and if it has (to make corollary 4 'inner') @ xmath264, where 'xmath264' indicates a besov space with smoothness parameter 'xmath267' . . . . in the course of our theorem 'main' (thus we refer to 'u' and 'p') for details on besov spaces . . . . . . we observe (* ? ? ? ] in that the distribution xmath6 has an xmath159-quantile , a =max_ in xmath177-node  xmath177 , and that the distribution xmath256-m has a lebesgue density  xmath258 for some xmath259 . . . the intuitive meaning of xmath248 is that increasing values of xmath248 correspond to increased smoothness . When we look at a risk of svms for the estimation of a certain sum, let us say that a single Gaussian kernel of xmath281 is used for a certain sum, where xmath177; whereas the sum of xmath177 is of order xmath176, whose sum is of order xmath176; and meanwhile, if xmath285 should be defined, the learning rate of xmath285 will be higher than that of xmath289. Furthermore, we have the same normalizing parameter, as in ( ... ? ... ? ... ? ? ? ... , i.e. , xmath285, xmath278, - in other words, no limiting value of xmath279, for a simple reason. ) we do not require @ xmath285. - So - if xmath286 is the inverse of xmath287, we can calculate a higher xmath287. *  ? Cor . . . [10][12] (i.e. [4] . . . , ... ] . [14] Table 1: math290: (*? *?? ? . . ] - Cor . . . [14][15] [17][16] [15] and [16][17] .