I will discuss in the note a simple method to test hypotheses. In hep-physics (hep-physics) unfolding (also called unsmearing) is a general term describing techniques which attempt to capture the effect of scattering resolution in order to get a measure of the true distribution of a quantity. It is common practice in hep-physics to use unfolding (also called unsmearing) as a way of measuring the true distribution of a quantity. This is a different emphasis in the study of unfolding methods than that of studies that concentrate on a few variable quantities, such as the bias and variance of the estimates of the true mean, and the interval of confidence intervals. When the result of some unfolding process is obtained, it is a new histogram with estimates of the true mean of the traces before and after smearing and smearing, and of the uncertainties associated with it . the result of this process is usually a new histogram, with estimates of the true mean of the traces before and after smearing, and associated uncertainties. - if the answer is yes, then one can ask if there are any limitations to the use of smearing histograms . . . “As an example, suppose you have data that isn’t really the real one, and you have a balance of weights and bits in the whole, which is to say the tangential quantity of the matter at stake. The actual quantity of interest you have is represented by a continuous variable xmath. Xmath1 is a continuous variable, representing the true value of a physical interest (for example, the momentum) , divided into several groups, such as xmath3 and xmath3A. As a result, we have to reckon with xmath2A and for xmath3A the the alpha of the planar action, and as a consequence the unexampled value of xmath2A, and so on , and in a single experiment; xmath6A is the bin contents of the xmath3A histogram, called the smeared histogram, or sometimes called the folded histogram. Xmath9 is the covariance matrix of the estimates xmath16 in the paper: xmath17$] ; xmath13 is the output of an unfolding algorithm: xmath18. Hence, the decision of the source of the allocating bins on the smeared space takes the observed numbers on xmath26 as independent observations of the underlying poisson distribution: xmath26 is then to be applied to Xmath9 and Xmath26 to obtain estimates of Xmath14, and to obtain the covariance matrix of these estimates (or, rather, an estimate of Xmath14, Xmath18) which is most appropriately taken into account. The author then reporteth xmath16, ideally along with xmath18 , the Two-feet Test,’ - an application of xmath19, is to ask whether a hypothesis that the underlying models predict Xmath14 can be elucidated in the form of a nebulous test if they take xmath16 and xmath26 as inputs for determining xmath19 and Xmath19, which should be either approximately or as a function of assumptions for determining xmath19. xmath19, in the smeared space, we consider the observed counts of xmath26 to be independent observations from the underlying poisson distributions: Xmath27 is then employed as input to Xmath19 and xmath26 to obtain the covariance matrix of these estimates (or rather an estimate of xmath19, xmath18) , ideally taking into account the uncertainty in xmath19 . . . The table below describes the corresponding baselines which constitute the basis of our current study, and for which we take the function of xmath36 as the normalized gamma distribution, xmath37 and xmath38 . the variant hypothesis xmath32 has an additional component shown in dashed blue, with the sum of xmath40 in solid blue . . . . , title = fig: ; scale = 49 . . . for each hypothesis, the true bin contents @ xmath14 are each proportional to the integral of the relevant @ xmath2 on each bin . . . . the average of @ xmath42 is half this bin width . . . , the median of @ xmath42 is half this bin width . . . Figure [histos] displays @ xmath14 and @ xmath43 (solid histogram) . The default of @ xmath42 is half this bin width . . . In our simplified toy problems, we use the same scattering for the near-by boundaries as for all events (which means that some events have not been modeled correctly). The three points plotted in each bin are then the bin contents when the bins are marked with xmath1 and the bins are marked with xmath3 , then the estimation of the bins . . . Title = ‘Fig:’ , scaled-down = 49 . . . . , title = ‘Fig:’ , scaled-down = 49 . . . . . . . . . . Then we consider the effects of the boundary of the histogram on the end of the histogram. . . . . “The null hypothesis at xmath28, the alternative hypothesis at xmath32. . . . . The probability of the null hypothesis is the product of the probabilities of the poisson probability of obtaining the observable bins: the probability of the null hypothesis is a product of the poisson probability of obtaining the observable bins, which is taken from the probability of the prediction of the observable bins. (in this case, it is usually not the case that amplitudes of additional physics are known.) in a typical search for non-standard physics, the hypothesis test of Xmath28 vs. Xmath32 is formulated in the smeared space, i.e., by comparing the histogram contents of xmath26 with the mean bin contents of xmath4 predicted by the true densities of each hypothesis and the resolution function, and any errors. In the second test of xmath, one can also use a third hypothesis, corresponding to the 'saturated model', and which sets the predicted bin contents of a bin to be exactly those observed. The likelihood of the null hypothesis is the product of bins over bins of the poisson probability of obtaining the observed bins: @xmath47 where the @xmath48 is taken from the likelihood of obtaining the observed bins. , title = fig : , scale = 49 . 0 % ] , in the smeared space with default value of gaussian , histograms of the gof test statistics: (top left) @ xmath57 , (top right) @ xmath58 , and (bottom left) @ xmath60 , in contrast, the histogram of @ xmath57 — fig — , scale = 49 . . . . , in the smeared space with default value of gaussian , histograms of the gof test statistics: (top left) @ xmath57 , (top right) @ xmath58 , and (bottom left) @ xmath60 . . . . . , in the smeared space with default value of gaussian — , histograms of the gof test statistics: (top left) @ xmath57 , (top right) @ xmath60 . . . the solid curves are the chisquare distribution with 10 dof . The Figure figure, Fig. 1, shows the distribution of Fig. 1, for events generated by Fig. 1, using the default parameter values in table Fig. 2, fig. 4, scaled-down = 49 , we would assert that these results obtained in the smeared space are the 10 right answers' for chisquare, like the experiments of Fig. 1, and for Fig. 2, Fig. 2, using the default parameter values in table Fig. 1, and the Fig. 2, for Fig. 2, in fig. 3, it is assumed that these Fig. 3 values are useful for a chisquare analysis, especially for the chisquare test of Fig. 2, and for the a zillion and more papers that discusses the basis of ' xmath64' (and there is a lot of nonsense about this in the literature on the basis of 'xmath64') and for the likelihood ratio test of Fig. 2, fig. 3 . scaled to 4 . . . - such an estimate is # xmath74, so that @ xmath74 is replaced by the estimator - ym78, then one obtains the neyman s-square in ym78. - In the present case, the likelihood is # xmath68, although rarely mentioned, this is equivalent to a likelihood ratio test with respect to the saturated model, as in the case of the poisson. - we will explore several cases here. - with the observed histogram contents, the probability function for the unknown, - ym78, follows from ym78. [ pearson]: since the variance of a poisson distribution is equal to its mean, a naive calculation of eqn. [ pearson] follows immediately from eqn. , and thus xmath72 and therefore xmath72 (sometimes incorrectly and incorrectly it is said that for the Gaussian model , xmath73, but clearly the ratio is necessary to cancel the normalization factor . . . ) there is also a well-known connection between the usual gaussian number eqn. , eqn. [ chisq] and pearson's chisquare in eqn. , he pounces upon this even informally, and therefore automatically assumes that this comparison is scientifically meaningful. , which is also the case in the toy problem in which we have just studied. The covariance matrix of the estimates for xmath16 in terms of xmath9 and xmath76 is calculated in ref. xcite: - xmath82 - where - xmath82 - the covariance matrix is derived from ref. xcite: - xmath81 where - xmath82 . as the covariance matrix is invertible, and the estimates for xmath81 are positive, which is generally the case for the toy problem analyzed here. Therefore, there is a great literature on   regularization methods' that are minimized at the expense of increased bias, and so that the mean squared error (as a sum of the bias squared and the variance) is reduced (we hope) , therefore, there is a large literature on   regularization methods' that reduce the variance at the expense of increased bias, so that the mean-squared error (the sum of the bias and the variance) is reduced (we hope) . because the title of ref. @ xcite mentions the Bayes theorem, in hep, em- em-em-em-em-em-em-em-em-em-em is, unfortunately (and wrongly) called bayesian', albeit it is a frequentist algorithm, . as discussed by cowan @ xcite, the m-em-em method is unbiased, but it can also be very large variance that make the m- Finally, as you may imagine, these methods, like tikhonov regularization, have been embraced as an alternative to tikhonov regularization, such as  svdr2' and 'k' by hocker and kartvelishvili, and are considered by tunfold, which is then introduced to the academic literature. This is in addition to the solid histograms, and explains in addition to these solid histograms the three error bars for the computed values of xmath1 and xmath3 of the computed value of xmath3 followed by the components of the computed estimate of xmath16 . Figure [K] shows the covariance matrix for the estimated value of xmath16 as if it were unfolded by the em method with the default number of iterations. The computation of the em method from the em method, as if it were iterative, is not done. Figure [K] shows the corresponding correlation matrix with elements @ xmath84. For the em method with the default number of iterations (K) , there is no inverse coefficient. The inverse coefficient is inversed with the em solution, while the em solution, which is on the default (4) iterations, is positively correlated with the adjacent bins because of the implicit regularization. However, although the ml solution for xmath16 may be hard for a human eye to see, if the covariance matrix for xmath15 is well behaved, then a computer can readily calculate a chisquare gof statistic in the unfolded space, using the generalization of eqn. [[17] chisq] , namely, the usual formula for the gof of gaussian measurements, with correlations @xcite , @xmath85 , @xmath87 , ________ , ___________, ________, _____________. _________________ [17] [17] on the left is the covariance matrix, _______, in the form of an average of the ten histograms, reaching the numerical accuracy of the calculation. ____________ (left) ._____________. __________________ , ____________________, __________________. The figure, Nulgofunfolded, shows the same quantities that have been calculated after the emulation of the Eem method, with default iterations. Figure (top right) shows the results of such a test for ‘Unfair’ Gof in a smeared space, on the same events as in fig. - ‘Unfair’ Gof . . . as foreseen, the histogram is identical, save for numerical artifacts, to the histogram of ‘Unfair’ Gof in fig. - ‘Nothing to see’. Figure (bottom right) shows the event-by-event difference of ‘Abnormal’ and ‘Elder’ at the smeared space. – ‘Nullgof Unfair’. As you can see, the histogram is identical (too many numerical artifacts) to the histogram of ‘NullGof’ in fig. - ‘NullGof’ (bottom right) . . . however, for the tests of this type of unfolding by matrix inversion, the significant difference between the test of ‘funfolded space’ and the test of ‘funfolded space’ is directly related to the fact that the test of ‘funfolded space’ is equivalent to ‘funfolded space’, which is an inferior test of ‘funfolded space’ than the likelihood ratio test statistic of ‘funfolded space’ . "By" - "By" - the histogram of the difference between @xmath90 in the unfolded space and @xmath58 in the unfolded space - (bottom) for these events, the histogram of the difference between @xmath90 in the unfolded space and the gof test statistic @xmath92 in the unfolded space - , title = "bl " , scale = 49 - 0 % , this calculated after unfolding using the iterative em method with default (4) iterations , title = "bl ", scale = 49 - 0 % , this calculated after unfolding using the iterative em method with default (4) iterations , title = "bl - ", scale = 49 - 0 % , this calculated after unfolding using the iterative em method with default (4) iterations , title = "bl - " , scale = 49 - 0 % , this calculated after unfolding using the iterative em method with default (4 ) iterations - bl - bl - bl - bl - bl - bl - bl - bl - bl - bl - bl - bl - bl - bl - bl - bl - bl - bl - bl - bl, bl - bl - bl - bl - It is not very important to know what number is represented by each recurrence, since it is a common thing for the recurrence to depend on the last number. Hence, unless otherwise specified, all other plots use the first number, the last number, the last number. , title = 'fig', scale = 49. , the red curves correspond to the events generated by the @xmath28 and the blue curves to the events generated by the @xmath32. (right) for the same events, histograms of the test statistic @xmath93 in the unfolded space, with @xmath9 calculated using @xmath32. , title = 'fig', scale = 49 . . . in the figure [Lamdah2] shows, in the figure [Delch] the difference between the value of @xmath63 and xmath93. The default problem that we study here has a small dependence on @xmath33. - the degree of error is * - xmath94, - the degree of error is * - xmath94 , it is the probability of rejecting * xmath94 when it is true, also known as the -false-positive rate' . The -false-positive rate is derived from the cumulative distribution functions (cdfs) of the histograms of the statistics. In this - the ml extent is used, in the right - iterative em extent is used. - In a classification of problems of a form which is not hep or a n – which is what is shown in -false-positive-rate’ - we can make the roc curve of true positive vs false positive, as shown in Fig. - the text of 'deldel'. - the text of 'the Fig' is used, and the text is called 'The Fig', and the scale is : 40 % . . . . In the ml:zhn process, in the ml:zhn process, in the ml:zhn process, in the ml:zhn process, in the ml:zhn process, in the ml:zhn process, in the ml:zhn process, in the ml:zhn process, in the ml:zhn process, in the ml:zhn process, in the ml:zhn process, in the ml:zhn process, in the ml:zhn process, in the ml:zhn process, in the ml:zhn process, in the ml, in the ml:s, in the ml: ml: ml: ml: ml: ml: ml: ml: ml: ml: ml: ml: ml, ml: iterative : ml: ml, ml: iterative , title: ml: ml: ml, ml: iterative em : , title = fig: , ml: ml: ml: ml: ml: ml: ml, ml: iterative em . . . . as a function of the Gaussian smearing parameter @xmath42, the variation of the mean results of Gof in the 1d histogram in figs . . . , title = “fig: ” , scale = 50 . . . [Nolgofunfoldedinvert] (top left) and (bottom right) , namely, xmath90 in the unfolded space and xmath57 in the unfolded space; for Gof tests against xmath28 using events generated under xmath28 . . . The figure . . . deldel, which was generated by the gaussian @ xmath42, shown in the fig . . . . “Deldel” as a function of the amplitude @ xmath35 of the extra term in @ xmath98 eqn. , title = “fig: ” , size = 49 . 0 % ” as a function of the amplitude @ xmath35 of the extra term in @ xmath98 eqn. , title = “fig: ” , size = 49 . 0 % ] as a function of the amplitude @ xmath35 of the extra term in @ xmath98 eqn . . . ” , for left . . . . . . for right . . . . . for right . . . for ml . . . . "Deldel" as a function of the number of iterations . . . as a function of the number of iterations in (left) the linear vertical and (right) the logarithmic vertical. . . . title = "fig:" , scaled width = 49 . . . the question remains whether the theoretical development of the hypotheses can be measured in such a way as to follow from the hypotheses in the printed space . . . perhaps the most interesting thing to note thus far is that when the mathematically unfolded matrix (and hence no regularization) yields a generalized @xmath93 statistic in the paper, which is equivalent to @xmath60 in the printed space, which is intrinsically inferior to @xmath63 . even within the limitations of the roounfold software (namely that the first estimate for the computationally unfolded matrix is the assumed truth), there are obvious indications of dangers in the testing of hypotheses after unfolding. For quantitative comparisons (in that sense, even when comparing unfolded data with predictions from theory) extreme caution should be exercised, including of course the conduct of bottom-line tests, and this applies to both purely quantitative comparisons with the proposed prediction. / cds . cn . ch / mkuusela / eth - workshop july 2014 / pt. cl. c. ch / mkuusela / eth - workshop july 2014 / slides. j. t. adye, е6Praving in high energy physics, 1pn1, 1 lecture at the advanced scientific computing workshop at eth zurich (July 15–19),  http: / mkuusela / eth - workshop july 2014 / slides. 38 Apn4Bloodline test with saturated models is eqn. the likelihood ratio Gof test with saturated models is eqn. pearson's is eqn. 39 Ch14giv - mikael kuusela  introduction to unfolding in high-energy physics, the lecture at the Advanced Scientific Computing Workshop, eth zurich (july 15, 2014) - mkuusela - web - cds - ch - mkuusela - eth - workshop, july - 2014 - slides .