The invention of the fuzzy min-max neural network (fmm) made the decision to organize hyperboxes according to the degree of belongingness to a particular class, which is known as a membership function. in addition to this elegant suggestion, he also presented the characteristics of a good classifier, among which are nonlinear separation, overlapping classes, and the corresponding proportions. The presented membership function calculates the affinity of the hyperbox so that it increases uniformly as we move away from the hyperbox. This weakness of fmm was the pattern to be represented in overlapping regions, where there is a great need for precise classification. As a result, gfmm [4] offered a general method. It is a convex box, which is represented by both min and max points. Simplified set theory was proposed to be of use in the fields of pattern classification and information processing. with this elegant method, a large number of other characteristics were identified, and the most important feature was the introduction of a tuning parameter, theta (@xmath0), which controls the size of a hyperbox, but its effectiveness was restricted by the overlapping region, so that the accuracy of the system was reduced by larger theta values. Moreover, the more significant contribution to this problem was the modification of the membership function. Another weakness of fmm was the identification of patterns which did not belong to any of the hyperboxes, which was, in turn, less accurate. Therefore, fmm classification results were completely characterized by a membership function. to address this problem, fmm, in chapter 3, introduced this generality. In the analyzed case, dcfmn gave a significant improvement over fmcn. The authors classified the overlap into three categories, namely, the full containment, the partial overlap, and the no overlap, and then a new membership function to accommodate belongingness based on the compensation value. where dcfmn took place only in a few cases, but had some serious drawbacks. The authors defined the overlap in three parts, namely, the full containment, the partial overlap, and the no overlap, and then a new membership function to accommodate the membership by compensating the data value. In short, we explain that mlf achieves a considerable milestone, but the training accuracy is still more important than the training accuracy, as it greatly influences the choice of the algorithm in practical situations. In a dcfmn, they took away the need of overlap categorization, and argued that this membership function was not always preferred, since it is not well suited for very many samples belonging to an overlapped area. authors have eliminated the need for a categorization. Moreover, the sequence of the training exemplars plays an important role. dcfmn introduced two new variables: @xmath1 and @xmath2 . @xmath1 is used to suppress the influence of noise and @xmath2 is used to control the hyperbox size . , both of these variables greatly influence the performance of the model, and of course it is a tedious task to define them. - for the test phase, the overlap regions are first traversed recursively to discover the appropriate subnet to which a test pattern belongs. - in that subnet, a class of hyperboxes whose membership is highest with hyperboxes in the discovered subnet, is selected as a predicted class. - in this recursive procedure, to the maximum depth, or to the overlap, it is filled. Then, if the superposition is increased by a hyperbox size parameter ( @ xmath0), then it is checked by acriding to equation ( ) in equation ; it is checked by acriding to equation ( ) . Then, after each recurrence, xmath0 is updated with the equation ( 3), and - xmath9, where , xmath10 and xmath11 are the dimensions of next level and previous level, respectively, and xmath12 is the value between 0 and 1, and - xmath12 is the value between 0 and 1 , it is possible to reduce the size of hyperbox in the overlapped region by taking into account the relationship between Xmath and Xmath. On the left-hand side are two blocks of hyperboxes – hbs – and ols – hbs represents hyperboxes created at that level, ols is overlapped at that level. Here we introduce a boundary between two hyperboxes, where, according to our experiments, the rate of misclassification is relatively high. To this we introduce a boundary region which exists between any two hyperboxes, where, according to our experiments, misclassification is rather high. in our method the recommendations of mlf are still intact, and we use distances with the data centroids to improve the classification rate in the aforementioned boundary region. Thus, in the same way as mlf has evolved, d – mlf has introduced a new step in the learning process called data centroid (dc) determination, where the dc of all the input patterns belonging to each hyperbox is stored in the dc. dc is computed as follows: dc = xmath20, where dc = xmath21 is the data centroid of the dc of the dc of the dc of the dc of the dc., where dc = xmath22 is the data centroid of the dc of the dc. dc is computed as follows: dc = xmath20, where dc = xmath21, where dc = dc = dc , along with the information of the dc, data centroid dc . We do not alter this model, but instead make it more efficient in deciding which subnets to choose. In the first stage, a training set is to be traversed n times. In the second stage, the data belongs to the overlapped region is traversed in order of the number of overlaps in that region. It is not the original mlf that used a decision to select the subnet, a pattern which is in the subnet’s decision. After recursively traversing the ols, the appropriate subnet is discovered, to which the test pattern belongs. The membership function is explained in the equation (6) and used to compute the membership of the subnets in the selected subnet. This step, however, is not altered, but rather is used to enhance the determination of the subnet’s decision. t nd a membership function described in the equation (6) – the membership of the subnets is computed against the overlapped area. h. centroid = sample; h. centroid = sample; h. centroid = sample; h. centroid = sample; h. centroid = sample; h. centroid = sample; h. centroid = sample; h. centroid = sample; h. centroid = sample; h. centroid = sample ; h . centroid = sample; h . centroid = sample ; h . centroid = sample; h . centroid = sample; h . centroid = sample; h . centroid = sample ; h . centroid = sample ; h . membercount = 1; create an overlap-box as @ xmat , we define - xmath33 and - xmath42 as a combined angle between test pattern and two hyperboxes. xmath33 is a difference between the max and the max points of the sample, and - xmath37 is a modifiable parameter to reduce fuzziness. Thus, focused on the inclusion value, the output of the network is denoted as either the class of maximum - xmath41 among all the hyperboxes, or the minimum of the distances of the highest two hyperboxes - which are already calculated. where -math43 is given by: -math44 where -math44 is the membership of the test sample in -math46 subnet, where -math47 is the edge between subnet -math46 and the corresponding subnet -math47 - xmath47 is a difference between Mean and Min with -math36, and -math37 is a tuning parameter to measure fuzziness . if the test pattern is in the boundary region, euclidean distance [10] between test pattern and the data points of the selected hyperboxes is computed. . . . In this illustration, we describe the effectiveness of the proposed model, pointing out clearly the identification and application of the claimed gulf of confusion. In this case, the hyperbox size parameter (@ xmath46) is fixed at 0 and a boundary parameter (@ xmath38) is fixed at 5 %. Out = d = d - mlf - test ( net, mlf) ; out = d - mlf - test ( net, mlf) - return null. Out = d = membership ( sample, h1 , h2 , dc) ; d = max ( d) ; out = max ( mv) ; d = max ( mv) . Out = max ( mv) ; d = eudistance ( sample, h1 , h2 , dc) - d = eudistance ( sample, h1 , dc) ; d = eudistance ( sample, h1 , dc, h2 ) - d = membership ( sample, @ xmath57) ; = = eudistance ( sample, @ xmath57 ) ; d = eudistance ( sample, @ xmath57 ) - mv = mv, mv = mv ( mv @ xmath57 max ( mv) ) - = = membership ( sample, @ xmath57 ) - , = ; d A variety of experiments were conducted to test the proposed method, which was d-mlf, on the following standard datasets: iris, glass, wine, wisconsin breast cancer (wbc), wisconsin diagnostic breast cancer (wdbc) and ionosphere. These datasets were obtained from the uci repository of machine learning databases . . . and many of the modifications in the state of the art. In this brief, we have introduced a new, region-based and distance-based d-mlf classification method, which will be able to distinguish patterns belonging to that region. It has been eloquently proven that our proposed method surpasses all the previous fmm methods. , d-mlf will serve a lot of applications in security, natural language processing, biomedical reasoning, and so on . . . in this case, we have introduced a new, boundary-region-based d-mlf classification method, d-mlf is effective in reducing the significance of outliers and other errors in the decision-making process . . . In this presentation, we compare our results with the traditional fmm method, which has already been proved to perform better than the previously proposed fmm methods. . . . It was taken from the utmost resolution: a factor and the least precision for the maximum fineness of the rojo-land . . . nandedkar and p. k. biswas, a data-memory fuzzy min-max neural network for pattern classification, ieeee trans . neural netw . 2 , pp. 402414, mar. a . . . nandedkar and p . k biswas, a fuzzy min-max neural network for pattern classification, ieeee trans . neural netw . 1 , pp. 4254, mar. a . . . panella and f. . . mascioli, adaptive density classes, iee trans . neural netw . 2 , pp. 481 485 , mar. w . chandler and h . . chandler, results of an analysis and recognition of vowels by computer, proc. 2060 - 2066, nov. ,