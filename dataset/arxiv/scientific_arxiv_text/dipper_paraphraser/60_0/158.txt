... or another weakness in fmm. This weakness was the upper region of the overlapping region, where the rates of misclassification are particularly high. Splendid number theory was proposed to provide an example for the study of pattern classification and information processing. The method had the fundamental feature of representing not one of the hyperboxes, and therefore it reduced the accuracy. For this purpose, Splendid number theory was applied. Simms also presented a method for clustering, using fmm. Splendid number theory is a set of neural networks whose preponderance is chosen to organize hyperboxes by the degree of their belonging to a particular class, which is known as a membership function. Moreover, in this branch of the network, there is a unique feature, that is, the nonlinearity of the domains, the overlapped classes, and the elements of the scale. In this, too, the tendency to represent a self-identification of this kind is inevitable. Besides this generality, the main contribution of gfmm [4] is that it is adapted to a membership function. simpson presented a clustering approach using fmm [5], but many real life problems require both classification and clustering. fmm (gfmm) presented a fuzzy, swarming neural network (gfmm), which has the effect of dividing the hyperboxes by their level of belonging to a particular class (the form of a membership function), so that the membership value decreases uniformly as one moves away from the hyperbox. In this way the invariability of the number of hyperboxes was considerable, the limitations of the tuning parameter (theta (@xmath In a brief, we explain the problem of overlap in general. The authors gave the overlap a three-dimensional classification, namely, the overlap is in part contained, the overlap is in part absent, and the membership function that accords with the compensation value is also suggested. Authors further analyzed that neatly preserving the overlap automatically results in an insensitivity to the hyperbox size parameter, @xmath0. Moreover, the order of the exemplars also plays a role. fmcn) was familiar in [7] . . . and dcfmn strove to replace fmcn in few cases; but there are some serious drawbacks. dcfmn introduces two new user-controlled variables, @xmath1 and @xmath2 , @xmath1 prevents the influence of the noise, @xmath2 prevents the descent speed of the membership function , these two variables greatly affect the performance of the model, and naturally, defining their values is a difficult task. The multi-level fuzzy min-max neural network (mlf) [8] addresses the problem of overlap in an elegant way. It uses separate levels for overlapping regions, monotonically decreases the size of the hyperbox (at xmath0) . for most cases, mlf achieves 100 % accuracy. The results of the multi-level fuzzy min-max neural network (MLNNN) are given in the section II and III. mlnNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNENNNNNNNNN  NCNNNNNN DE CELLNANN QUARTONS THER SEPARDENTS DE CELL AND RECENTLY DECIDED AT DE CELL NNN INDOING THE COLLECNEN DE CELL NNNNNNNNN DE CRNNLND NUNNNNNNNNNN, DE COLLECNS AND ENGLISH FLOWS DE TURN DE CELL NNN DE CELLUNNDENTS DE CELLNGN THER-QUALL DUBLIN DIRS IND XII , DE QUANTUM NNN DE MR DILING DE CELLULK, DE LARDENDENCELLNNNNNN TO ORDERING DRIVERSITY WITHOUT DELIVERING DRIVERSTN NW THE DRIVE DE FLOW DE CELLULK DE CELLULK, DULLEIN AND FILE WITH DE BOX AND DISCELLULANE DE CLING, EQUIPMENT ONE AND T " Figure 1 shows the structure of d - mlf , each node of s - xmath14 contains two sections, hyperboxes and overlapping sections (ols) ; hbs represents hyperboxes created at that level, ols represents overlaps. in the proposed method, the recommendations of mlf are preserved, and, in addition, distance with the data centroids is used to improve classification in the aforementioned boundary region. Here, d - mlf maintains d - mlf using hbs and ols; here, d - mlf adds a new step, called data centroid (dc), where dc of all input patterns belonging to each hyperbox is kept in hbs. [31] , the max point is 1, and the min point is X, among which overlap is tested. Then each hyperbox is checked with the other hyperboxes to detect overlap with the equation (4). , where xmath15 is the max point, and xmath19 is the min point, among which overlap is tested. In the proposed method, the recommendations of mlf are preserved, and in addition to that we can add distance with the data centroids to improve the classification rate in the new boundary zone. b. if a training set does not include a subset of other subsets, then in the next stage a subset of overlapping classes is inspected in the order of the number of overlaps in the region. Note that the members of overlapping classes are not part of the computation of dc. a membership function, explained in the equation (6), is used in this case to compute the membership of a subset of subsets in the selected subset. b. if a subset of the test set does not need to be a leaf node of the tree, a subset of the subset must not be a leaf node. * net = d-mlf – train (net, @ xmath0) + @ xmath25 = h- centroid + m- centroid = sample; h- centroid + m-centroid = sample; h-centroid + = sample; h-centroid + = sample; h-centroid + = sample; h-centroid + = sample; h-centroid + = sample; h-centroid + = sample; h-centroid + = sample; h-centroid = sample; h-centroid = sample; h-centroid = sample, h-centroid = sample; h-centroid = sample; h-centroid = sample; h-centroid = sample; h-centroid = sample; h-centroid = sample; h-centroid = sample; h-centroid = sample; h-centroid = s; hi-centroid = s; hi-centroid = s; It is a user-controlled variable which is mentioned in the percentage value. if the material exists outside the boundary region, we simply follow a path of mlf and classify the material based on the maximum membership value which has already been computed. if the material is in the boundary region, the euclidean distance between the material and the data centroids of the selected hyperboxes is calculated. if the material is in the boundary region, we take the distance of the Euclidean distance [10] between the material and the data centroids of the selected hyperboxes. - sent> - gamma  gamma At the outset of the diagram, a hyperbox size parameter ( @ xmath46) is fixed at 0 and a boundary parameter ( @ xmath38) is fixed at 5 percent. “For this example, we describe the effectiveness of the proposed method by clearly illustrating the correct identification and correct application of the mentioned area of confusion. [25] d = d = d - test (net, sample) - + d = d - d - d = d = d = d = d = d = eudistance (data, h1 . dc, h2 . dc) d = membership (data, h1 . dc , h2 . dc) d = = membership (data, h1 . dc , h2 . dc) d = eudistance (data, h1 . dc . dc . . . . ). d = = v . . . d = = = mv , v = . . . . . . , e.g., a normalization method d-mlf (D-mlf) was developed for different test datasets, including iris, glass, wine, wisconsin breast cancer (wbc), wisconsin diagnostic breast cancer (wdbc) and ionosphere . a hyperbox size parameter, @xmath0; [0], i.e., 0], is chosen for each experiment, and the training and testing data is random. We took advantage of a new method for the whole of d-mlf, which has been proven to be better than all the previous methods, and in this way d-mlf can be used in many fields, such as security, natural language processing, biomedical reasoning, etc., . we compared our results with mlf, because it has already been proven to be better than the old methods ... d-mlf is a new method for mlf in general, because it has already been proved to be better than the previous methods ... , ... d-mlf will be applied to a lot of fields, such as security, natural language processing, biomedical reasoning, etc. . BARgiela, w. pedrycz, and m. tanaka, a multi-level fuzzy maximum-max neural network classifier, ieee trans. neural netw. 12 , pp. 402414, mar. a. v. nandedkar and p. k. biswas, a tetrahedral non-convergent fuzzy maximum-max neural network classifier, ieee trans. neural netw. 2 , pp. 402414, mar. a. rizzi, m. panella, and f. m . f. mascioli, adaptive resolution. Ieeee trans. neural netw. 2 , pp. 402414, mar. h. zhang, j. liu, d. ma, and z. wang, adaptive, k-level, fuzzy min-max neural network classifier, ieee trans. neural netw. 2 , pp. 481, mar. w. bezdel and h. j. chandler, results of the analysis and recognition of vowels by computer using zero-crossing data, proc. 2060, 2066, nov.