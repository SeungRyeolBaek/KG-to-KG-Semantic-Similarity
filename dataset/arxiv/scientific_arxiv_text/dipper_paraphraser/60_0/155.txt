‘ A network’ is a network of interconnected entities that can be represented mathematically as a graph; there are vertices and edges at the vertices; a network is a system of interconnectedness and flows among many different and numerous domains. It is a network which, with advances in technology, the proliferation of mobile phones and a network of information about human relationships, has become quite large. Many network studies are carried out today, and they are not only much larger than they have ever been, but sometimes in a decentralized form (e.g. the network of blogs or the web itself) . This effect, in some cases, is sufficient to make it difficult to study and even to obtain these data. Therefore, how should one analyze and mine these network data? In our work we focus on a particular aspect: sampling, which is to be conducted with small and discrete numbers of nodes and links from the network. Using this method, we can draw from the network data a few examples. The epidemiology of the network, for example, was examined in depth, and the structural analysis of such samples showed that they could be very useful in inferring network protocol performance. So how should we proceed in analyzing and analyzing this network data? networks are ubiquitous and arise in many different fields, from neurons to proteins to food webs, there is now access to vast networks of interaction and interaction among various entities, and a need to analyse and understand these data. The past few years have seen a great deal of work devoted to the analysis of problems which arise from network sampling biases, and the causes of these biases. We have thus formulated a detailed study of network sampling biases. This study is different from most of the work done in the literature because we consider network sampling bias as an asset to be exploited. a number of our findings are remarkably surprising as they contrast starkly with the conventional wisdom (e ) of network sampling (e ) in many ways. We find that bias towards high-global (a concept borrowed from expander graphs) offers several unique advantages over other biases, such as those which favour high-global nodes. In other analyses, we show that a simple sampling process which picks up nodes with many connections from those already sampled is often a fairly good approximation for sampling at high-global nodes and finds nodes, in fact, particularly well connected (i.e. high-global) nodes far faster than other approaches. Finally, we discuss the advantages of some of our findings in several important applications, such as disease detection and market research. a number of these findings are surprising because they are in stark contrast to the old tradition (e ) in many respects (e ) , as well as to the conventional wisdom (e ) , e ) . In this regard, we conclude that some of our findings can be exploited in a number of important applications, including disease outbreak detection and market research. We conclude that a bias towards the very highest “extending” (a concept derived from the expansion graph) has many unique advantages over other biases, such as those towards the high-order nodes. In other This work is most recently done, in particular by xcite. These methods, often based on modified random walks, have been shown to be effective for various frequency estimation problems (e.g. determining the proportion of pages in a language graph in a web graph) ., therefore, we do not conceive of them and instead concentrate on other more suitable sampling methods (such as those mentioned in xcite). Our work is much more closely related to xcite. , many studies have been carried out about a sampling bias. many studies have examined biases in different sampling methods, which can not be fully captured by the simple attribute frequencies . However, as mentioned above, our work focuses on sampling the structural (and functional) properties of the network itself . , the present work focuses on the imputation of the mean and median of the network itself (e.g., an equation of the proportion of pages of a certain language in a graph in xcite). so, we do not look at these biases and instead concentrate on more suitable sampling methods (such as the ones we have described in imputation) . Nevertheless, as I have already mentioned, the present work focuses on a sample of structural (and functional) properties of the network itself (seeing that many of these properties cannot be accounted for by simple inverse frequencies). - Properties of certain biases and ways in which they can be exploited in network analysis. But as you will see, property-testing in graphs is relatively new in the domain of combinatorics and graph theory, and its primary topic is the study of properties of property in graphs. The various properties of property-testing in graphs are generally less useful for the analysis of - real world networks (e.g., the exact meaning of, for example, @xmath0 - colorability, which is a social network) . Moreover, the theoretical work on property-testing in graphs is extraordinarily surveyed in xcite. For reviews on decentralized search both in complex networks and in p2p systems, one may use the terms xcite and xcite. For examples of networks and sample-testing, see xcite. for decentralized search and sample-testing, see xcite. In the present work, we are using graph search algorithms (e.g. to search on network unstructured) and web crawling (e.g. to search on network with a single click) . thus, some of our results later have implication for these research areas (e.g. see xcite) . . . . . . . All networks are treated as undirected and unweighted. In this work we examine sampling biases in a total of twelve different networks: a power grid (power grid @ xcite) , a Wikipedia voting network (wikivote @ xcite) , a pgp trust network (pgp@ xcite) , a citation network (hepth @ xcite) , a citation network (hepth @ xcite) , a citation network (hepth @ xcite) , a citation network (hepth @ xcite) , an email network ( enron @ xcite) , two p2p file sharing networks ( gnutella04 @ xcite and gnutella31 @ xcite) , two online social networks ( epinions @ xcite and slashdot @ xcite) , two online co-buying networks ( e-shopping @ xcite and slashdot @ xcite) , two online social networks ( e-shopping @ xcite and slashdot @ xcite) and a product co-seller network ( amazon @ xcite ) . If a link is crawled (since a link is crawled 'to access nodes'), if a network is 'online' (because the network 'sees' itself iteratively in the course of the sampling process) . . . This of course correctly characterizes most real networks . . . for instance, neighbors of a web page can be obtained from the links on a visited page, or from the friends list in an online social network . . . in both 'see' and 'see' xmath6 . as an aside, notice from the definition 'defn' link-tracing' that we implicitly assumed that a node can be found by visiting the node during the sampling (i.e. . . , i. , i.e. xmath6 is known) . , this choice obviously affects the properties of the sample being constructed. ffs, proposed in @xcite, is essentially a probabilistic version of bfs , at each iteration of a bfs-like process a neighbor is scoured only by a burning probability of xmath20. In a bfs-like process a neighbor is explored only by some burning probability of xmath16. This requirement is acceptable for certain domains, such as p2p networks and social networks. ffs, proposed in @xcite, is basically a probabilistic version of bfs. ffs, proposed in @xcite, is basically a probabilistic version of bfs. Notice that in order to select the node xmath20 with the highest degree of probability (i.e. number of neighbors), the sec strategy tracks the links from the currently constructed sample xmath4 to each node xmath20 and selects the node xmath20 with the most links from xmath4 . in other words, we use the degree of sampling. Notice that in order to select the node xmath20 with the highest degree (i.e. number of neighbors) the process must know xmath22 for each xmath20. that is, at each iteration, this requirement is acceptable for some domains, such as p2p networks and social networks. Then the next node (often called a “neighbor”), selected for inclusion in the sample is chosen by using the expression: -= , the node which is chosen for inclusion is chosen based on the expression: -= , like the ds strategy, this approach is based on the knowledge of -= . (The standard deviations of the measures of --> size are described in Section --> above . ) for each analysis we obtain 100 samples randomly selected from a random set of seeds, compute our measures of ---- in each sample, and plot the average value as the sample size grows. We therefore divided the measures of ---- into three categories: ---- crowning , ---- reach, and ---- reach , ----, ----. because of the space limitations and the large number of networks examined, we show only two results, which are illustrative of general trends observed in all the results ... but we will discuss the full results later on in a supplementary article. In computer science, the word - better ... is generally taken to mean structural _approach_ (as correctly observed by ahmed et al. ----) . and thus the tests of the system can be taken as the _approach_ of the network . . . that is, if the test is better than the one before, then the test is considered better if it is more representative of the structural properties of the original network . . . And for a given dataset, we would like to calculate the degree distribution similarity (distribution), and distribution, a distance measure. , and this distance measure consists in distribution of the length of the distance from the same sample, by the square root of the diameter of the centre. As we study the totality of the distribution, we look at the proportion of the highest, i.e., higher, nodes accumulated in the sample. Hence, for our purposes we use distribution ... on a distance measure of k. Then we calculate the distribution of the distribution, by subtracting the distance of k from the one. Several purposes he only cares less about matching the overall distribution, but more about accumulating the highest, the most high-degree nodes (i.e. immunization strategies , xcite) , in our cases we use distribution ... (Xs is similar, but of a less minor dimension), for example, we consider that the distribution, which is biased to the high-degree nodes, is best at the integration (which is also a direct result of this bias) ... on the other hand, the strategies of bfs, ffs, rw are better at the integration, but less on the comparison (which is also a direct result of this bias) . a second tendency is a consequence of the size of the network, which, of course, depends upon the quality of the network itself, but the method is a more granular one, and the trend is a little more consistent. Besides, the second tendency is of course to depend on the quality of the networks and the number of hubs present in them (depending on the size of the network), that is, the closer a cluster is to the density of the anchors (see Figure (fig. 11))]) in general, the best performances of the samples of these networks are in the social networks (in contrast to the networks of technology such as the power-grid with many of the most “good” hubs, of low average degree, and long paths) , thus, clustering has been an interest of graphs for some time now. In fact, clustering has been a key feature of many real world networks, such as social networks, which are much more dense than what one would expect at random @xcite. The local clustering coefficient ccloc is defined as @xmath36, where @xmath37 is the degree of node @xmath16, and @xmath38 is the number of links among the neighbors of @xcite. (other strategies also influence the results of this section) however, more investigation is needed to draw firm conclusions on this last point. — it is very well known that for many strategies and systems the estimation of clustering was first higher than actual, and then gradually declined (Fig. 3) — - - . in which we have chosen a new measure of representativeness called 'network reach'. * net reach is an indicator of the extent to which a sample is covered by a network . . . an indicator of the total number of nodes in a cluster - this agrees with intuition. Many real-world networks exhibit what is known as 'network structure'. In the most general sense, a net is a collection of diverse nodes connecting to each other, whereas in the most general sense it is to be reduced to a small "network corner" of the graph. These nodes in a network should have more paths leading to them, and thus be found earlier in a sampling (as opposed to nodes not in a net and in the peripherals of the network) . This will be explained in detail by discussing in detail the two netherworld measures of netherworld reach: netherworld reach and netherworld reach. For more recent studies, netherworld reach has obviously received a lot less attention than degree and netherworld, but it is a vital measure for a number of important applications (see Section 2: netherworld approach) . . . It is to assess whether a sample is spread across the network. As a matter of fact, we define communities merely as the result of a sampling strategy. Since community detection is somewhat an imperfect science (e.g., see xcite), we measure _community reach_ with two different algorithms. The first one is the method proposed by clauset et al. in xcite (described as cnm) and the second method, by raghavan et al. in xcite (described as rak). as a matter of fact, we are slightly less interested in the structural details of communities detected here than in the spatial resolution of them. One may omit to take into account the strength of links drawn from the immediate neighborhood, so that each iteration can be used to select another set of elements from the surrounding neighborhood, which yields a much smaller search space. For this purpose, we are defining community reach simply as the output of a detection strategy. For this, we imply the "bridge" of a sample by measuring the extent to which it is a hop away from the others. The link-trace method has a very well-known sharp approximation of @xmath42. This approximation is proved to be very close, but the sampling of the population is limited to selecting additional components from the existing neighborhood @xmath6 each iteration, which results in a much smaller search space. – on the other hand, the sec method seems to be among the least effective at finding clusters or communities – this is probably because sec prefers to select clusters which are already represented in the sample. The third measure, dq, is remarkable, but the ds strategy, which explicitly selects high-level nodes, is often unable to compete with the xs strategy on all three measures. this is partly due to the overlap in the neighborhoods of well-connected nodes. We find, in short, that nodes which contribute most to the expansion of the sample are unique in that they offer certain and significant advantages over and above those provided by nodes that are simply well-connected and those that are accumulated in the bfs approach. As you can see, link-trace sampling begins with randomly selected seeds. Typically, link-trace sampling is initiated from randomly selected seeds. The figure [Std] shows the standard deviation of each sampling strategy for hub inclusion and network reach as the sample size increases. " Here we will focus on the analytical analysis of these observed connections. ' random walks' (rw),' ( xcite - there is a very large literature on random walks and markov chains (see  xcite for a thorough survey). There is a well-known analysis of the statistical effect of the probability (or fixed probability) of residing at any node at xmath16 during a random walk on a connected, undirected graph, which eventually converges with time to xmath43, where xmath44 is the degree of node at xmath16 @ xcite - so that the experimental effect of rw is only a somewhat greater success than other strategies (e.g. bfs) on measures such as _hub inclusion_ and _discovery_. This phenomenon, as shown in figure 1, is very weak. Here, we turn our attention to the analysis of these observed connections. In study of the problem of searching peer-to-peer networks, adamic et al., according to ds , proposed and analyzed a greedy search method similar to the ds method. This strategy, which we call a degree-based walk, was found to quickly locate the highest-level nodes and cover large sections of free network. In fact, as we see in figure 1, for example, the slap time of the walk (i.e. the expected number of steps to reach a node from any node) was directly related to this stationary probability. Thus, these results seem to be limited and do not predict actual sampling. , we can prove that if the conductance of the communities is sufficiently low, the expansion of the sample is directly affected by the community structure. In our example, the expectation is concentrated on the nodes in section 82. elevated> _community detection is _controllability_ , that is, the fraction of edges emanating from a sample (the lower the number of edges, the stronger the community): @ xmath45, where @ xmath46 is an entry in the adjacency matrix, and @ xmath47 is the total number of edges (non-zero) incident on the node set 82. Note that @ xmath50 and @ xmath51 are both directly related to controllability . When controllability is low, @ xmath51 is smaller than @ xmath52, the total number of edges incident on xmath53 is @ xmath54, and @ xmath50 and @ xmath51 are random variables denoting the inward and outward edges of each node (as opposed to constant values) . . . . . . as we compute an upper bound on xmath63, we assume that there is exactly one node in xmath16's community (as by definition, no nodes in xmath16's community will be in xmath16's community . . .). Using the linearity of expectations, the upper bound on xmath63 is xmath64, where the term xmath65 is the expected number of nodes in xmath16's community that are both allied to xmath16 and already in xmath6 . . . by the linearity of expectations, the lower bound on xmath63 is xmath67, where the term xmath68 is the expected number of nodes in xmath16's community that are both allied to xmath16's community and already in xmath6 . . . . . solution for xmath51, if xmath60 , then xmath69 . . . theorem [Thm: xsbias] shows analytically the link between expansion and the structure of the community - a connection which till now has only been empirically proved ... Let @xmath73 be a function that returns the expected degree of a given node in a given random network (see @xcite for more information on the expected degree sequence). Then, let @xmath75 be a function that returns the expected degree of a given node in a given random network (see @xcite for more information on expected degree sequences) . combining the results of this analysis from @xcite (the sec: biases, existing) is a theoretical basis for observed performance of the sec strategy on measures like _hub-inclusion_. in a recent paper, christakis and fowler investigated the effectiveness of predicting the presence of an outbreak in a social network. We present here a preliminary analysis of the results of this analysis, in which we will give an account of the results, which are a general description of the present results, and in the course of this work we will discuss three issues: 1) prevention of an outbreak of diseases in a social network, 2) references to red lines, (3) in turn to red lines, (4) with the following explanation. In a recent study christakis and fowler investigated the detection of an outbreak in the social network. For a recent study christakis and fowler investigated the detection of an outbreak in the social network. He replied, “We are rarely able to access our friend and acquaintances, as we are usually limited by space and comfort. The result of this sampling is tremendous: it gathers information that is time-consuming, expensive, and often impossible for large networks. This makes things worse when one realizes that most existing network-based methods for identifying patients and detecting diseases have no idea of the full details of the network structure (e.g. @xcite) . christakis and fowler @xcite have a sampling method called acquaintance sampling (acq) which is based on the so-called friendship paradox , so-called by christakis and fowler. . . . acq is based on the so-called friendship paradox @xcite . . . and it requires less information than ds and xs, which are more commonplace. This means that christakis and fowler @xcite have an immense advantage over christakis and fowler. (And, on the other hand, it also takes less information than ds and xs, which are the best performers.) . Now, in the following sections, we have shown, empirically and analytically, that the sec method performs exceedingly well in gathering hubs. This superiority is due to the fact that sec is far more efficient at locating the well-connected peers in a network. Figure 1 (Seed as a table) shows the sample size needed to gather the top-ranking well-ranking individuals. . . . if the characteristics of nodes are not known in advance, it is difficult to collect a sample. Furthermore, it has the added benefit of identifying members of diverse groups without having any prior knowledge of the characteristics of the nodes or the social structures of the community. As a result, this strategy is likely to be very useful. . . . if the characteristics of nodes are not known in advance, it can be very difficult to collect a sample from many different groups. This is a new marketing technique. In marketing surveys, one seeks to collect stratified samples that collectively represent the diversity of the population. The xs strategy, which achieved the best network reach, is the most suitable method. It covers the network substantially better than other approaches. As we have seen in our test for network reach in section [fig], rep. reach, the xs strategy yields the best _discovery quotient_ and covers the network with great accuracy. In this way, it represents an attractive strategy for the xs strategy. It is also useful to understand how graphs should be explored, crawled and searched. As shown in figure [fig], rep. reach, the most commonly used method for exploring networks, bfs, ranks low in terms of _discovery quotient. It is also indicated in figure (fig] The high-order nodes tend to be in newer, different parts of the network that have not yet been sampled. In addition, we have shown that sampling a great many connections from those that have already been sampled is a reasonable approximation to sampling a large number of nodes. In future, we intend to investigate how to make the simplest, most efficient sampling strategy more attractive for a wider variety of applications ...