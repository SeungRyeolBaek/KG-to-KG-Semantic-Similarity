‘In high energy physics (hep), unfolding (also called unsmearing) is a general term describing methods that attempt to take advantage of the error in smearing resolution to get a true measurement of the true distribution of a quantity. The result of an unfolding procedure is then a new histogram, with estimates of the true mean bin contents before the unfolding and the encoding, and some associated uncertainties. Here, in order to encourage more interest in this topic, we will describe what one of us (rc) calls a bottom-line test of an unfolding method. In this note, we have introduced a few variations on a play-acting example that illustrates some of the characteristic features of unfolding problems in hep. We purposely present a certain method to explain this: if the open-ended spectrum and the observed uncertainties are to be of use in determining which of two models is the more favourably favored by the data (and by how much) then the answer will be materially the same as that obtained by smearing the two models and comparing them directly with the data without unfolding them. This is a different emphasis in evaluating unfolding methods from that which is applied to studies of intermediate quantities, such as the bias and variance of the estimates of the true mean contents, and the frequentist coverage of the confidence intervals. Xcite: it omits the background contribution, which he calls xmath0, as he stands at the beginning of the principle. Xmath2 is a continuous variable, representing the true value of some physical object (for example, momentum) . . . this is distributed according to the strategy xmath2. xmath5 is the determination function of the detector: the conditional k to observe xmath3 in the presence of the true value of xmath1 (and that it was observed somewhere) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . , if the value of xmath1 is xmath1 . . . . . . . . Then, according to the function of resolution, we suppose that the number of bins in each bin is unknown (and in particular that the densities in each bin are known, in Xmath2] in each bin) is therefore unknown either roughly or as a function of assumptions about the true bins. (@ xmath21 is often suggested, while @ xmath24 is usually omitted from the system of equations.) In the smeared space, we take the observed counts at xmath26 to be independent observations from the underlying poisson distribution: @ xmath26 the unfolding problem is then to use both @ xmath17 and / xmath17 as inputs to obtain estimates @ xmath16 of xmath14, and to obtain the covariance matrix @ xmath15 of these estimates (or rather an estimate of xmath17, - xmath18) , ideally taking into account the uncertainty in xmath17. (If only a histogram of @ xmath17 with > error bars 'is displayed, then only the diagonal elements of @ xmath17 are communicated, degrading the information.) the "bottom line" test of an application of unfolding is then whether hypothesis tests of underlying models that predict xmath14 can be meaningful if they take as input 'xmath16 and 'xmath18' . "The corresponding subtotal "xmath" is a scaled square. The figures Fig. 1-2 are the baselines of the present study, for which we take xmath36 to be a normalized gamma distribution, xmath37 and xmath38. , title = "fig: ", scaled squared ... ", scaled squared ... " for each hypothesis, the true bin contents @ xmath14 are each proportioned to the integral of the relevant @ xmath2 on each bin. , Fig. 1-2 is represented by @ xmath39, shown in red . . . . . , we have assumed the smearing of @ xmath3 to be a normalized gamma distribution, @ xmath37 and @ xmath38 . . . the constant xmath35 governs the level of departures. " Fig. ' ' scaled width = 49. " The data point : in mc simulation a set of true points is chosen randomly and then smeared to be set at xmath45 . The three points plotted in each bin are then the bin contents at xmath1 and xmath3 , then the estimate for the bin contents . . . title = fig : , scaled width = 49 . . . The ends of the histogram are an important part of a real problem. . . . in our simplified, toy problems we use the same smearing for events at the ends of the histogram as for all events (and therefore we do not simulate properly some events) . For example, in a typical search for non-standard physics, the hypothesis test of xmath28 vs xmath32 is formulated in the smeared space, i.e., by comparing the histogram contents xmath26 with the mean bin contents xmath43 predicted by the true densities xmath2 under each hypothesis, and the resolution function and any efficiency loss. The likelihood of the null hypothesis is the product of the probability of the poisson probability of obtaining the observed bins, where the probability of the null hypothesis is the product of the probability of the estimated bin contents : @ xmath47 where the @ xmath48 are taken from the null hypothesis prediction. The likelihood of other hypotheses, such as  xmath49, are constructed similarly. The negative log- likelihood ratio is a goodness-of-fit statistic, which is asymptotically distributed as a chisquare distribution, if xmath28 is true. Similarly, the likelihood for other hypotheses, such as  xmath49, are made up. Thus,  xmath51 is the upper bound on  xmath52 for any hypothesis, provided that it is observed. (bottom right) histogram of the event-by-event difference in the two test statistics at xmath58 and xmath57. The title is “fig:” and the scale is 49 %. In contrast, the histogram of xmath58 (figure [non-observed] [bottom left] has significant differences from the theoretical curve . . . The solid curves are the chisquare distribution with 10 % of freedom. (bottom right) histogram of the event-by-event difference in the two test statistics at xmath58 and xmath57. The solid curves are the chisquare distribution with 10 % of freedom. . . . . . In the smeared space with the default value of gaussian @ xmath42, histograms of the test statistics: (top left) @ xmath57 , (top right) @ xmath58 , and (bottom left) @ xmath60 . . . * The same point is remarked on in the figure: fig. 266 for events generated by fig. 266 for events generated by fig. 266 (see fig. 2). The proportion of the difference in events produced by fig. 266 for events generated by fig. 266 (see fig. 224), . * . . scaled width = 49. 0 %] , given a particular data set, such histograms can be used to calculate fig. 266 values for each hypothesis, simply by integrating the appropriate tail of the histogram beyond the observed value of the relevant likelihood ratio . . . (there is a great deal of literature on the subject of 'xmath64' in frequentist statistics, and here we assume that they can be useful and have the interest in comparing them . . . . (there is a vast literature about the foundations of 'xmath64', but we assume in this note that it is useful and that we have looked at the different ways of calculating them. When you open histograms, and then you compare the unfolded histograms with the predictions for the saturated model, even informally, you implicitly assume that it is scientifically meaningful. So let's say if you use the estimated xmath69, you get xmath71, and therefore xmath72 (there is a well-known connection between the usual Gaussian ' xmath59' and pearson's 'chi square' in eqn. chi square) and a probability of xmath70 (for the saturated model '-saturated' it is equivalent to a likelihood ratio test, as in the poisson case ) if one folds histograms and compares the unfolded histograms @ xmath16 with (never smeared) model predictions @ xmath75, then even informally if the comparison is not wrong, then the result is implicitly given as the 'right answer' in the smeared space - we will see a few tests here - given the observed histogram contents @ xmath26, the likelihood function for the unknown associativity @ xmath76 follows from eqn . . . . and leads to the maximum likelihood . And the covariance of the estimates at xmath16, as defined in ref. xcite, is obtained by the covariance method, and in ref. xcite: xmath81, where xmath82 is a negation. Because the title of ref. xcite mentions the Bayes theorem, in hep it is unfortunately (and is incorrectly) called Bayesian, even though it is a fully frequentist method , s17 . the covariance matrix of the estimates of @ xmath14 is in accordance with the integer Potential, i.e., in the case of the toy problem, as discussed by cowan  xcite, the ml estimates are unbiased, but this unbiasedness can lead to a great variety of variance, so that the mean-squared error (i.e., the sum of the bias and the variance) is reduced. Thus, a wide literature on regularization is already being written on this subject. as noted by the covariance of the estimates @ xmath14, which is sometimes called "em" in hep, it is known as "em" in hep (and studied for example by bohm and zech @ xcite): this method is simply to stop the iterative em method from approaching the ml estimates. The points on each box represent the order of the experiments in which the examples are calculated, i.e. the sample of which is the starting point for the iterative em method; so we have not studied convergence starting from a uniform distribution. The methods of these different methods are discussed in kuusela. xcite . In addition to the solid histograms mentioned above, a figure is inserted with error bars in each box, calculated from a particular set of simulated data corresponding to one experiment. Figure [matrices invert] (left) shows the covariance matrix for the estimates of xmath16 which were obtained from the same simulated data set, which had been inverted by matrix inversion (eqn. nurmuinv) in order to obtain the ml estimates. Figure [matrices invert] (right] shows the correlation matrix with elements @ xmath84. ml estimates are negatively correlated, while the ml estimates are positively correlated because of implicit regularization. This and other methods of hep in hep are also advocated by the hocker and the Kartvelishvili in the cvd. . The connection of these methods with the professional literature is explained by Kuusela . It is a generalization of chisquare chisquare of the sum of correlations at the angles, called chisquare. (Figure converge) a simulated set of estimations. (The corresponding matrices invert) (right) the correlation matrix corresponding to xmath16, with elements xmath84. ( xmath84) , title = fig : , scaled-down = 0 , on the left the covariance matrix corresponding to xmath16, converges to the ml solution of xmath15, by means of the generalization of eqn. chisq. chisq, which is the usual formula for chissquares for gaussian measurements, with correlations @ xcite , @ xmath85 if a transform is performed by matrix inversion (when equal to the ml solution), then substituting @ xmath86 from eqn. eqn. covmu, and @ xmath88 from eqn. In the depiction given, the histogram is identical (without a number of numbers) with the histogram of 'xmath90' in fig. ' 'nothing' (bottom left) . . . a relation of xmath90 and pearson @ xmath59 in the depiction of 'xmath59'. . . . in the depiction of 'nothing', the distinction in the reconstructed fig. ' nothing'. This is without any decomposition of the equations and decompositions of the em-p-inversions. The X-math90 histogram is identical (except for the numerical errors) to the x-math60 of 'nothing' (bottom left) . . . it is remarkable that even though the oblique-t-change matrix would seem not to lose any information, in practice the way the information is used (relatively linearizing the problem by expressing the result via a covariance matrix) in a way that does not result in the bottom-line test of gof, as it does not involve any regularization, or approximate-t-change . ‘Sent’ for the events generated by ‘sent’. (Top right) for the events, histogram of the difference between ‘sent’ in the unfolded space and ‘sent’ in the unfolded space . . . (bottom) for the events, histogram of the difference between ‘sent’ in the unfolded space and the ‘gof test statistic ‘sent’ in the unfolded space . . . ‘bottom’ for the events, histogram of the difference between ‘sent’ in the unfolded space and ‘sent’ in the unfolded space . . . ‘ The plodding method is ml on the left, and iteratively iteratively iteratively. The red curves correspond to events generated under xmath28, while the blue curves correspond to events generated under xmath32. 0 % ] , histogram of the test statistic @ xmath93 in the plodding space, for events generated under xmath28 (in blue) and xmath32 (in red) , with @ xmath9 calculated using @ xmath32 . , title = fig: , a measure of a size , a measure of a value ... , this is an illustration of the histograms of the tests of xmath93, for events generated under xmath28 (in blue) and under xmath32 (in red) , corresponding to xmath93 . , title = “fig:” , scale = 49 % , we can examine the effect of the differences in fig. , in [deldel] by using the language of neyman-pearson hypothesis, where one rejects @ xmath94 when it is true, also called the 'true positive rate' , the quantity @ xmath96 is the power of the test, also called the 'true positive rate' . In the left histogram, ml unfolding is used, in the right iterative unfolding is used. , title = "fig" , scaled width = 49 . in the left histogram, ml unfolding is used, and in the right histogram, iterative unfolding is used. in emphases, the histogram is ml unfolding, and in the right histogram, iterative unfolding . ‘ i f – t. . , title = fig – t. , scale – t. . Sadly, the general conclusion to be drawn from this observation cannot be drawn, since, as has been said above, the ems’ vs. i f – t. ’ – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r – r The figure, deldelsigma, shows the variation of the 1d histogram in fig. deldel with the gaussian parameter of @xmath42, in 1d histograms in figs. (Top left) and bottom left , namely, math90 in the unfolded space, and math57 in the smeared space; for math28 experiments, using math28 events. math28 [Second Line], title = "fig : ", scaled width = 49 . . .  % ] in smearing (downright) . 0 % ] in smearing (upper-left) . . . the horizontal axes are the same as those in a fig . . . The result of the bottom-up test of fig . . . . [deldel] as a function of the amplitude of xmath35 of the extra term in xmath98 in eqn. . . . , title = ‘fig’ , scaled width = 49 . . . 0 %] as a function of the amplitude of xmath35 of the extra term in xmath98 in eqn. . . . [deldel] as a function of the amplitude of xmath35 of the extra term in xmath98 in eqn. . . . . . . . [dxmm] , for (left) Xmath9 derived from xmath28 and (right) derived from xmath32; for ml unfolding . . . . Title = “fig: “, scale = 49 . asymmetrical, that is, we see a generalised theory in the smear of the paper, and that a generically similar estimate for smearing the paper is the assumed truth. This explains the differences in size of the smear in the smeared space, but also the potential danger of the smear of the paper in the cases of an unfolding hypothesis. Aside from the limits of the Riounfold software (especially that the first estimate for iterating is the presumed truth), one can draw a conclusion of the potential usefulness of the "bottom-line" hypothesis. for quantitative comparisons (that is, the presumed use of the unfolded result in analyzing the predictions in the future from theory) one must exercise extreme caution, even when we are dealing with the bottom-line of the data in a form that is independent of expectation. This rule applies to both linear and logarithmic tests, as well as multiple tests of the same hypothesis. This rule applies to both inverse and linear tests of the hypothesis, as well as to several tests of the same hypothesis. perhaps the most interesting thing to note thus far is that unfolding by matrix inversion (and hence no regularization) yields, in the implementation tested, a generalized _xmath93_ statistic that is identical to _xmath60 in the smeared space, which is intrinsically inferior to _xmath63 . - dte - dte - adye , - the implementation of the envelope-shape technique, - cv - dte - ch - mkuusela - eth zurich, 15-July-2014 - (from the 21st to 22nd january 2011 ) - https - cds - cern - ch - mkuusela - eth zurich, 15-31 - 3 - cv - cv - cv - web - cern - ch - mkuusela - eth zurich, 15-29 June--2014 - - http - cv - web - cv - ch - mkuusela - eth zurich, july-15 - csv - 36319 ( 2015) and 2015. louis lyons, introduction to the eth: introduction, (nonsense) in the Proceedings of the physico-spatial Conference on the Problems of Discovery and Appraisal, (cern, geneva-switzerland, 17–20 january 2011) + https://cds. cern . ch / mkuusela . ch / mkuusela / eth-conference july-2014 / p. p. p. rc.