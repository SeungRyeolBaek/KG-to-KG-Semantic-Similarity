for semiparametric models, these are called additive models, and the most important kinds of additive models are those that are applied to two-way or three-way estimations and classifications. Therefore, we shall describe the case of the regularized kernel based methods for additive models, based on a general convex and lipschitz continuous loss function, and on a general kernel, and on the classical regularizing term xmath1 for some xmath2 that is smoothness and not sparsity, see e.g. xcite , xcite , xcite, xcite, xcite and all the references. The most important reasons for the success of additive models are their increased flexibility compared to linear or generalized linear models and their greater interpretability compared to fully nonparametric models. We know well that the good estimators in additive models are generally less prone to the curse of high dimensions than the good estimators in fully nonparametric models. Thus, we shall now consider the case of regularized kernel based on a general convex and lipschitz continuous loss function, on a general kernel, and on the classical term xmath1 for some xmath2: a smoothness penalty but not a sparsity penalty, see xcite , cite , cite , cite , cite , cite , cite , cite , cite and the references therein . By the pinball loss function the bastard-correction is guaranteed, and it is also called the check function in literature, see e.g. a xcite and a xcite for parametric quantile regression, and a xcite, a xcite, and a xcite for kernel-based quantile regression . . . In this paper we will look at the proportions of these algorithms. Let us recall the foundation of the paper with a complete separable metric space, xmath3 as the input space, and a closed subset of xmath4 of xmath5 as the output space. A rate of learning will be attained for these models. support vector machines (svms) are used here for support vector regularization, in a reproducing kernel-hilbert space (rkhs) , produced by a mercer kernel - xmath16 , a loss function @ xmath17 introduced to cope even with long-term distributions as @ xmath17 . , we mention here that there is a formal problem here: that is, in the previous formula, each quantity is an element of the quotient [xmath35], which is a subset of the full quotient of xmath36, where xmath36 is a quotient [xmath36], whereas in the definition of sample [xmath36], each quantity is an element of the full quotient of xmath36, where xmath37 is a quotient [xmath37] for xmath36, where xmath39 is a series of functions [xmath30], each of which is also a quotient of xmath37 for @xmath36 from xmath3 to xmath5 and so on, thus the functions from @xmath32 take the additive form @xmath33. sent> reducer of the following optimization problem, in which the loss function is derived from a loss function (a loss function) @xmath12, for the purpose of a set of function functions @xmath35, corresponding to the form (quote) of xmath43, quoting from xmath41 , the norm of @xmath41 is satisfied. . . . let’s say that there’s a gaussian function defined at xmath57 on xmath58 , where xmath61 denotes the rkhs of the gaussian rbf kernel that’s included in the standard gaussian kernel of xmath63 . . . if there’s a mercer kernel defined at xmath68, then let’s say that @ xmath67 is a mercer kernel and definition of rkhs on xmath73 . . . the second example is a sobolev kernel. The figures in this paper are in the form of individual asymptotic variations of the excess risk - xmath82, and are described in the form of xmath83 with xmath84. , the time of acrobats [8] are treated as important special cases, in which, in particular, we introduce some learning rates for the support vector machines generated by the additive kernels, for the additive model. They are presented under three kinds of conditions, namely the hypothesis space - xmath19, the measurement - xmath6 the loss - xmath12 and the choice of the regularization parameter - xmath85 . Moreover, to find the approximation error, we consider the minimization of the risk xmath89, we define the integral operator xmath91 associated with the kernel xmath41 , we say that xmath96 is an orthonormal basis of xmath94 and xmath97 is an orthonormal basis of xmath98. , then we can define the power xmath101 of xmath93 by xmath101 . This is a solution to the objection error of the triple xmath87, as Xmath100, this is a solution to Xmath101. This is called Xmath101 . , [12] xmath110 is a function of the form xmath110 with some xmath111. Therefore, it is natural to infer an additive expression for the target function xmath121 from the component functions xmath113 that meets the power condition xmath110. a standard condition in literature (e.g., @ xcite) to achieve decays of the form xmath114 for the approximation error ( [20] is @ xmath115 with some @ xmath114 . . . note that the natural probability measure is not proportionate to the marginal distribution @ xmath122 projected onto @ xmath27, and so the existing methods in literature (e.g., @ xcite) cannot be applied directly. this is the first novelty of this paper. The above natural assumption leads to technical difficulties in estimating the approximation error: the function @ xmath113 has no direct relation to the marginal distribution @ xmath122 projected on @ xmath27, hence existing methods in literature (e.g., @ xcite) can not be applied directly. Here the operator @ xmath117 is defined by @ xmath117, it cannot be written in an additive form , but it is the expected value of the component function @ xmath113 that is in the parameter mix. There is a second novelty of this paper, namely, that the additive nature of the hypotheses gives a nice bound with a dimension-dependent power exponent for the cover of the balls of the hypotheses of xmath0, to be proved in Section . . . the second novelty is that the binding for cover numbers in the hypotheses stated in theorem . . . a special bound in theorem . . . , under assumption . . . , for some . . . . . . . . for some . . . . . . for every . . . . . . for every . . . . . , the . . . cover number of the balls of the euclidean space . . . in the cube . . . . . . . , euclidean space . . . . - xmath146 , he has . . . . . The cover number of . . . balls of the euclidean space . . . ['" The Cover Number is a special Point. The Power . . . . . If xmath164 is a probability measure on xmath165, then a real number is called xmath164 - quantile of xmath164 , if and only if xmath167 belongs to the group xmath168 , then a real number is called xmath164 - quantile of xmath164 , then a probability measure on xmath165 is called xmath167 - quantile of xmath165 , if there is a xmath164 - quantile of xmath170 , if there is a xmath171 - quantile of xmath171 , then we have a xmath174 - quantile of xmath166 , if and only if xmath168 belongs to the xmath168 - I will prove that the distribution with the magnitude of xmath164 is of xmath159-quantile of xmath177 - of xmath177 - of average type @ xmath179 , where we are able to find @ xmath186    , and that for the distribution with the magnitude of xmath179 it is of xmath159-quantile of xmath177 - of average type @ xmath199 - so, if xmath199 is bounded on @ xmath185 , it is of xmath186 that is, if xmath184 is bounded on @ xmath185 - that is, in [the tauquantile of type 2 ] the corresponding equation (the tauquantile of type 2 ) - so, if xmath164 has a density @ xmath184, then @ xmath184 has a density @ xmath184 and a type @ xmath195 if @ xmath184 is bounded by zero on @ xmath185, as we can use @ xmath186      ] tauquantile of type . . . so, this assumption can be applied to many distributions in the statistical world, such as gaussian, student s, xmath186, and logistic, gamma and log-normal (with @ xmath189), uniform and beta distributions (with @ xmath190) . After defining the number of additive components in Xmath207, and the dimensions of Xmath208 and Xmath209, we will cite the following in detail: The number of additive components in Xmath207, and the dimensions of Xmath208 and Xmath209. Furthermore, note that if Xmath213 is equal to xmath214, the learning rate, independent of the dimension xmath216, is near xmath217 for large values of xmath177, and the learning rate is near xmath218 or better if Xmath211 is equal to xmath213. , if the triple xmath223 is satisfied, the exponent xmath224 can be larger. Suppose, for example, that xmath12 is a loss of pinball (a loss of pinball), and xmath6 is a quantile of xmath177 - average, xmath226 as defined in xcite, then xmath227 . , if the triple xmath223 is satisfied by some conditions, the exponent xmath224 can be greater . At this point the approximation error condition (aproximation mcl) is satisfied for some constants @ xmath241, and for some constants @ xmath240, the series of eigenvalues @ xmath241 is satisfied for all @ xmath241 in some cases, and it is satisfied by the integral operator @ xmath117 for all @ xmath243, that is, for all @ xmath243 the approximation error condition (aproximation mcl) is satisfied for some @ xmath240, and that the value of the integral operator @ xmath117 is satisfied for every @ xmath243, the goodness of our learning rate and the quality of our results are compared with the literature. The main goal of this paper is to prove the main goal of the svm and the main rate (or higher) of the theory, i.e., that an svm with an additive kernel can give a substantially better learning rate in high dimensions than an svm with a general kernel, i.e., the classical gaussian rbf kernel, provided that the assumption of an additive kernel is satisfied. For example, if we assume that xmath6 has a xmath159 - quantile of xmath177 - average type @ xmath177, the approximation error condition (approximation) is satisfied for some xmath240, and that the sequence of eigenvalues @ xmath240 of the integral operator @ xmath117 satisfies - we are interested in additive models and we will not discuss the extension of the above. Note that the kernel in the above example is independent of any specific function xmath121. For example, you can use a gaussian kernel xmath121 depending on the sample size xmath203, and a regularity condition for the marginal distribution xmath250. We can, for example, use a gaussian kernel xmath121 depending on the sample size xmath203, and with xcite, we can obtain the approximation error condition (aproxerrorb) for some xmath200. Then, taking xmath197 as xmath145 and xmath199, we can hold with certainty at least xmath200. We can, however, use a gaussian kernel for xmath121, for example, if we assume a constant xmath192, and if xmath6 has a xmath159-contrast of xmath177-average type @ xmath177 for some xmath199, then we can for example assume a gaussian kernel for xmath295, if the sample size is >200, then we will take the approximation error condition ( xapproximation error) for some xmath295 . Note that the kernel in the above example is independent of the value of a gaussian kernel. [ And so on: suppose that the optimal decision function, in Xmath260, has (to make the theorem [maintainable] with Xmath261 and Xmath263 (it is also) the additive structure, as defined in assumption[1][2][3], where @ xmath264 and Xmath265 are numerical values, where @ xmath266 denotes a besov space with a smoothness parameter - at xmath267 . The logical meaning of the word xmath248 is that the increasing values of xmath248 correspond to the increased smoothness. furthermore, if the optimal decision function of xmath260 has (to make theorem [5] applicable to @ xcite), the additive structure @ xmath207 with each @ xmath104 as described in the assumption - 'besov', where @ xmath265 and __math266 = a besov space with smoothness parameter @ xmath267 , we refer to (*? ? ? ] and p. 44) for details on besov spaces. Let us use (see table) the values of @ xmath284 from [embellished] [embellished] [embellished] [embellished] [[20]] that are negative, except for @ xmath284 and @ xmath287 , where [20]_________ is some determinate positive constant independent of @ xmath279 , then we have the same order of regularizing parameters as in (, , , , , cor ... ] [22] , and we have the same sequence of rationalizing parameters as in (, , , , , , , , , , , , , , , , , ,    , , , , ,  , ,  , , ,  , , , ,  , , , ,  , , , , , " (cf. Cor. ). [Cols. ] - cf. cf. - [Clarity of Cf.] [cf.] - (cf.) , from the formulae (mainratesthm) [7] [8] , if the regularizing parameter - xmath290 is chosen in an optimal manner for the nonparametric, i.e. - xmath293, with xmath294 for xmath295 and xmath296, recall that -