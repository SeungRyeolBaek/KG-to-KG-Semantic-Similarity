The purpose of the process of selection is to choose a model which best approximates the observed data and captures the underlying regularities. The object of the process of selection is to choose a model which best approximates the observed data and captures its underlying regularities. For the purpose of the procedure of re-selection, such as cross-validation and bootstrapping, the generalization error of the model is estimated by Monte Carlo simulation. In contrast to re-selection, the process of selection of the model using aic and bic does not require validation for the estimation of the error, and is computationally efficient. In these procedures an information criterion is defined, which makes the generalisation error in the data equivalence estimates by penalizing the error of the model on observed data. The structure of these processes is thus greatly influenced by information criteria. In particular, a fairness-of-fit or agood-fit means how well a model captures the regularity in the data. The good-fit describes how well a model captures the regularity of the data. In general, the greater the complexity, the more weakly it is generalized, and the lower its generalization. In the example of the cross-validation and bootstrapping, the generalization error of the model is estimated by Monte Carlo simulation. a large number of information criteria have been proposed, each of which has a different objective, and which has different theoretical properties. We study a kernel-based information criterion for ridge-regression. The kernel-based information criterion is a criteria for selecting the most suitable subspace in the domain between the data set and the unknown data. The methods of this type, called kernel-based method, are mainly used for applying a matrix of the most generalized, the original kernel method. The literature on kernel-based methods has, however, mainly been concerned with the selection of the kernel parameters and their changes, and there is little work on kernel-based methods. This process, which is called kernel-based, converts the conventional, already proven, selection of models into stronger and more corresponding kernel methods. The study of ridge-based models (krr) is based on the adjustment of the ridge parameters to find the most resolving subspace with respect to the data, and the unknown data. In the classical method of selection, the performance of the selection criteria is evaluated in theory by providing a consistency proof in a condition where the sample size tends to infinity, and in practice through simulated experiments with finite samples. rosipal et al. xcite developed covariance information criterion (cic) for model selection in a kernel-regression model. in this research, kobayashi and komaki investigated the kernel-based regularization criterion (kric) for kernel-regression logistic regression and support vector machines (svm). rosipal et al. The term “kic” is a construction of the method in which we study a novel variable-wise variance, and we derive a complexity measure from the combination of kernels identified on the model parameters. However, the results differ because the complexity measures capture the interdependency of the points of the model rather than the parameters of the model. The work is arranged as follows: in the first section, krr, we explain the type of kernel-based information criterion (kic) . The second section, krr, is an overview of the kernel-ridge regression. In this section, kic is discussed in detail. The method for calculating kernel-based information criterion is a novel one: kernel-based information criterion (kic) . However, the methods differ: kic utilizes a covariance-based complexity measure. In a linear regression, we have, , xmath7, where xmath7 is the observation vector (response variable) of size xmath8, xmath9 is the full rank data matrix of the data. In this study, we define a novel variable-wise variance, and a complexity measure is derived from the additive combination of kernels of the parameter of interest. In a paper we have written a description of the kernel ridge. This problem is not posed in the first place, so it needs a regularization, like tikhanov regularization (ridge regression), and the coefficients minimize the following optimization problem: xmath20. xmath21 is the regularization parameter. The estimated coefficients in ridge regression are: xmath23 in ridge regression (krr), the data matrix xmath9 is non-linearly transformed in rkhs by a feature map of xmath24. The estimated coefficients for xmath22 are: xmath23 in krr, the data matrix xmath9 is non-linearly transformed in rkhs by the feature map xmath24. the arithmetic of xmath28, which is obtained by Xmath28, is not possible (the kernel trick helps you avoid inventing a definition of xmath28 which could be numerically impossible if calculated in rkhs, if he knew what it was), and so a ridge estimator is used (e.g. xcite) that does not include xmath24. The estimation of the ridge estimator by xmath25 is: @xmath25, where xmath27 is the kernel matrix . Since a normal distribution of xmaths is a normal distribution of xmaths, the complexity of a covariance matrix, xmaths, is the shannon entropy, xcite, where xmaths, xcite, are the marginal and joint entropy, and xmaths, the diagonal element of xmaths, if and only if the covariates are independent. To overcome these drawbacks, bozodgan and haughton invented an icomp data criterion for computing the covariance of maximum covariance, which is an upper bound on the complexity of equation: xmaths, xmath47 this complexity is proportional to the estimated arithmetic ( @ xmath48) and geometric mean ( @ xmath49) of the eigenvalues of the covariance matrix. This complexity measure changes with orthonormal transformations because it is dependent on the coordinates of the random variables . zhang . . . , the kernel form of this complexity measure, which is computed on the kernel basis of the ridge estimator: @ xmath51 the complexity measure in the gaussian process (gpr; @ xcite) is defined as @ xmath52, the concept of joint entropy @ xmath42 (as shown in the equation eq . ) . . . . in other words, the concept of the size of the model is hidden because the definition of a kernel is not found. Then, let xmath55 be the parameter vector of the ridge-regression: xmath56, where xmath57 and xmath58 are given, and xmath59 is the solution of krr, by xmath60. in the case of xmath72 the parameter xmath28 is given by xmath72, where xmath73 is the gram matrix of xmath71. in this case, the parameter xmath28 is given by xmath72, where xmath72 is a function in xmath70, the rhs defined by xmath71. the quantity xmath61 =  sigma  2  operator name  tr  [k (k +  alpha i)   - 2] ] . and if xmath72 is given by xmath73, then xmath69 is equal to xmath75 in the equation [eq : g] equals xmath74. . . . and if . . . . . . if . . . . . . . , then . . . (defined as follows) , assume . . . and . . . are random vectors with feature maps . . . . if . . . , then . . . , then . . . . . . Then, based on the hsic on the hsic of the yin - and hsic on the yin - covariance, we can calculate the independence between the two  . . . since ,  xmath104 is a symmetric positive semi-definite matrix,     - , and the trace of the hs norm - We compared kic with loocv @ xcite, kernel-based icomp @ xcite, and the maximum log of marginal likelihood in gpr (as referred to as gpr) @ xcite in order to find the optimal ridge regressors. We compared kic with loocv @ xcite and the penalized log of marginal likelihood in gpr (in abbreviated gpr) @ xcite in order to compare the optimal models. . . . we denoted these information criteria as follows: @ xmath114 ,  end  aligned   ] @ xmath115 . . . in the case of kic_1 we denoted the solution of a quadratic optimization problem, @ xmath119, where @ xmath120 . . . . . because the complexity term depends on @ xmath16, the @ xmath111 for kic_1 is: @ xmath117 = 0 . . . . . the minimum kic is the best model . . . Hence we consider the kernel-based closed form of loocv for linear regression introduced by xcite. For a given training set of xmath129, and xmath130, a multivariate gaussian distribution is set up on any of the functions, at xmath127, where xmath35 is the kernel. the log of marginal likelihood is denoted as: @xmath133, where xmath133 denotes the fit of the model, @ xmath134 denotes the complexity, and @ xmath136 is the normalization constant. We consider the kernel-based closed form of a regression method (as proposed by @xcite) for a given training set, @xmath130, such that , @xmath130, where xmath35 is the kernel. minimizing the log of the marginal likelihood (gpr) is a kernel-based regression method. Having a parity of results and a quick recovery, the closed form formula is developed for the estimation of risk under special conditions. And so, when the ridge parameter @xmath21 increases, the complexity of the model decreases, and the goodness of the model is adversely affected. The effect of @xmath142 on complexity, kic and kic values is measured by comparing the kic value and the mean square error, @xmath152, for different kic values. In the kic value and the mean square error, @xmath151, the results are shown in Figure 2. The influence of sample size on the effect of @xmath1 on complexity, kic values and kic values is measured by comparing the sample sizes, yoming, and yamming, for a total of four experiments: @xmath147, yoming, yoming, yoming , yoming, yoming, yoming , yoming, yoming, and yoming. As the ridge parameter @xmath145 increases, the complexity decreases, and the goodness of fit increases. The kic balances between these two terms, which provides a criterion for the selection of a good generalization and good fit to the data. (discussed in accordance with the second example). The four tests averaging 100 times were conducted in the same order, each of which was done in a hundred different ways: (@xmath148) , (@ xmath149) , – if the particle size is a high math – a small math – a little smaller than a math 162–2500 sample – as expected, the mse of all the methods is larger when nsr is high, – a little less for the two-test data (100 samples) – the best results are obtained by kic – the kic method has a smaller math – a smaller math – kernel, – to – – - - in the second case – kic, – – – the kic method is comparable to kic, but with a – standard deviation close to zero. – in the fourth experiment, – we considered four experiments of the same size, math 160 – and nsr – - at xmath164 – and nsr – at xmath164 – we looked at the frequency of tuning and selecting the parameters in comparison with loocv – the results are shown in Figure (loocv) and in Figure (loocv) – the frequency of tuning or selecting the parameters is indicated in Figure (loocv) , and in Figure (loocv) – the frequency of selecting the parameters is shown in Figure (loocv) for loocv, and in Figure (kic) for kic – the frequency of selecting the parameters is shown in Figure (loocv) and in Figure (loocv) – The abalone, in the exoskeleton, was to estimate the age of abalones. The results are shown in Figures 1 and 2. In each trial, 100 random samples were randomly chosen for the training, and the remaining 4077 samples for the test. kin family and puma family are realistic simulations of a robot arm, for which combinations of attributes are taken into account, such as whether the movement of the arm is nonlinear (n) or fairly linear (f), and whether the noise level in the data is low or high (h) . The kin family includes: kin-8fm, kin-8fh, kin-8fh, kin-8nm, and kin-8nh , respectively. , and kin-family contains: kin-8fm, kin-8fm, kin- 8nm, kin-8nh , kin-8nh, kin-8nh, and kin- 8nh . In the kin-family , kin- 8fm, kin-8fm, kin- 8nh, and kin-8nh . we compared kic_1 with kic_2 with loocv, icomp, and gpr on the three datasets . The result of kic is equal to icomp and better than gpr for puma , puma , puma , puma , puma , puma , puma , puma , puma , puma , puma , puma , puma , puma , puma , puma , puma , puma , puma , puma, puma, puma, and kic for puma , puma , puma, and kic, but not as good as kic, kic, and kic are superior to kic, kic for puma , puma , puma , puma, and kic for puma , puma , puma , puma , puma , puma , puma , puma , puma , puma , puma , puma , puma , puma , puma , kin-kic, puma , kin-kic , kic is comparable to kic and kic is equal to kic, the median of mse for kic and gpr is equal to kic, and kic is more significant (smaller interquartile in the box) . We based our experiments on artificial data and real benchmark data, the abalon family, the kin family, and the puma family. In this work kic efficiently balances the goodness of fit and complexity of the model, is stable against noise (although for higher noise we have a larger confidence interval as expected), and sample size, is well-calibrated in selecting the ridge and kernel parameters, and produces significantly smaller or comparable values for the competing methods, while producing more regressors. The effect of different kernels was also investigated, since the definition of the proper kernel plays an important role in kernel methods. kic has superior results with different kernels, and for the proper kernel you obtain smaller mse.