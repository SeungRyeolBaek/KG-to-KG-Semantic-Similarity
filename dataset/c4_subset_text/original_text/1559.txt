But it still makes some mistakes that a human never would.
Microsoft scientists have released a paper claiming that their visual recognition software is outperforming humans , a long-sought after feat. The benchmark for humans in image classification is a 5.1 percent error rate. Microsoft claims their error rate is 4.94 percent, beating out Google's 6.66 percent.
In the ImageNet Competition, teams are given a dataset of 150,000 images and asked to identify 1,000 objects within those images. The 1,000 objects may appear in different pictures, meaning a computer may need to identify something like a power drill both with a person holding it and sitting on a workbench by itself. The provided images are often of a medium resolution, so it's going on compressed datasets.
This recent success a demonstration of Microsoft's deep learning capabilitiesâ€”though as the company points out, some of its misses were pretty glaring, making "mistakes in cases that are not difficult for humans, especially for those requiring context understanding or high-level knowledge." The researchers also acknowledge that the demonstration related to the ImageNet dataset its algorithms were given